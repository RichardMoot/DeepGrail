{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM part-of-speech tagging and supertagging for the French Treebank: \n",
    "\n",
    "This notebook trains a part-of-speech tagger and supertagger for the French Treebank using a vanilla bi-direction LSTM network.\n",
    "\n",
    "Run the following cell to load the Keras packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Bidirectional, Dense, Input, Dropout, LSTM, Activation, TimeDistributed, BatchNormalization, concatenate, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.constraints import max_norm\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from grail_data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the TLGbank file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences with verified parses\n",
    "# number of sentences, train: 9449, test: 3150, dev: 3150\n",
    "words, Y1, Y2, S, vocabulary, vnorm, partsofspeech1, partsofspeech2, superset, maxLen = read_maxentdata('parsed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Longest sentence   :  140\n",
      "Number of POS tags :  32\n",
      "Number of supertags:  891\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print()\n",
    "print(\"Longest sentence   : \", maxLen)\n",
    "print(\"Number of POS tags : \", numClasses)\n",
    "print(\"Number of supertags: \", numSuperClasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the input into train/dev/test\n",
    "\n",
    "Split the full training set into 60% train, 20% dev and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create auxiliary mappings\n",
    "\n",
    "Create mappings from supertags and the two sets of part-of-speech tags to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_to_index = load_obj('super_to_index')\n",
    "index_to_super = load_obj('index_to_super')\n",
    "pos1_to_index = load_obj('pos1_to_index')\n",
    "index_to_pos1 = load_obj('index_to_pos1')\n",
    "pos2_to_index = load_obj('pos2_to_index')\n",
    "index_to_pos2 = load_obj('index_to_pos2')\n",
    "p1_to_integer = load_obj('p1_to_integer')\n",
    "integer_to_p1 = load_obj('integer_to_p1')\n",
    "p2_to_integer = load_obj('p2_to_integer')\n",
    "integer_to_p2 = load_obj('integer_to_p2')\n",
    "p3_to_integer = load_obj('p3_to_integer')\n",
    "integer_to_p3 = load_obj('integer_to_p3')\n",
    "p4_to_integer = load_obj('p4_to_integer')\n",
    "integer_to_p4 = load_obj('integer_to_p4')\n",
    "s1_to_integer = load_obj('s1_to_integer')\n",
    "integer_to_s1 = load_obj('integer_to_s1')\n",
    "s2_to_integer = load_obj('s2_to_integer')\n",
    "integer_to_s2 = load_obj('integer_to_s2')\n",
    "s3_to_integer = load_obj('s3_to_integer')\n",
    "integer_to_s3 = load_obj('integer_to_s3')\n",
    "s4_to_integer = load_obj('s4_to_integer')\n",
    "integer_to_s4 = load_obj('integer_to_s4')\n",
    "s5_to_integer = load_obj('s5_to_integer')\n",
    "integer_to_s5 = load_obj('integer_to_s5')\n",
    "s6_to_integer = load_obj('s6_to_integer')\n",
    "integer_to_s6 = load_obj('integer_to_s6')\n",
    "s7_to_integer = load_obj('s7_to_integer')\n",
    "integer_to_s7 = load_obj('integer_to_s7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numSuperClasses = len(index_to_super) + 1\n",
    "\n",
    "Y = lists_to_indices(S, super_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 221.  599.  264.  890.   11.  597.  319.   38.  264.  890.   11.  597.\n",
      "  653.  597.  315.   20.  585.  756.   11.  597.  174.   11.  597.   57.\n",
      "    1.  597.  653.  597.  319.  162.   11.  597.   57.    1.  597.  319.\n",
      "  174.   11.  597.  319.  315.  330.  832.  221.  609.  426.  387.  597.\n",
      "  429.   11.  597.  319.  653.  597.  174.  221.  653.  597.  315.  162.\n",
      "   11.  597.  319.  653.  597.  629.   11.  597.  724.  361.   11.  597.\n",
      "   57.  597.  174.  221.  315.  447.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.]\n",
      "(4320, 140)\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 140)\n"
     ]
    }
   ],
   "source": [
    "Yin = Y[:2795]\n",
    "print(np.shape(Yin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            list = []\n",
    "            line = line.strip().split()\n",
    "            for i in line:\n",
    "                list.append(i)\n",
    "            out.append(list)\n",
    "    return np.asarray(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeftList  = read_data('brackets_left.txt')\n",
    "RightList = read_data('brackets_right.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_to_indices(X, max_len):\n",
    "\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "    X_indices = np.zeros((m,max_len,1))\n",
    "\n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split it into words. You should get a list of words.\n",
    "        list = X[i]\n",
    "\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in list:\n",
    "            try:\n",
    "                X_indices[i, j, 0] = float(w)\n",
    "            except:\n",
    "                print(\"Not a float/integer: \", w)\n",
    "                X_indices[i, j, 0] = 0  # unknown\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "            \n",
    "    return X_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "Left = l_to_indices(LeftList, maxLen)\n",
    "Right = l_to_indices(RightList, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  2.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  2.  0.  1.  2.\n",
      "  1.  1.  1.  4.  0.  1.  2.  1.  1.  0.  0.  1.  2.  0.  1.  2.  1.  0.\n",
      "  1.  1.  1.  0.  2.  0.  1.  3.  1.  0.  1.  0.  1.  2.  4.  0.  1.  0.\n",
      "  1.  0.  1.  0.  2.  0.  1.  2.  0.  1.  0.  1.  2.  0.  2.  0.  2.  0.\n",
      "  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "(2795, 140)\n"
     ]
    }
   ],
   "source": [
    "print(Left[0])\n",
    "print(np.shape(Left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_indices = Input(shape = (maxLen,), dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = Embedding(numSuperClasses,32,trainable=True,mask_zero=True)(sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = Bidirectional(LSTM(128, return_sequences=True))(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = TimeDistributed(Dense(32,kernel_constraint=max_norm(5.)))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "L =  TimeDistributed(Dense(1,kernel_constraint=max_norm(5.)))(X)\n",
    "outl = Activation('relu')(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R =  TimeDistributed(Dense(1,kernel_constraint=max_norm(5.)))(X)\n",
    "outr = Activation('relu')(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 140)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 140, 32)      28512       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 140, 256)     164864      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 140, 32)      8224        bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 140, 1)       33          time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 140, 1)       33          time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 140, 1)       0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 140, 1)       0           time_distributed_14[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 201,666\n",
      "Trainable params: 201,666\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(sentence_indices, [outl, outr])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['mae','mae']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2236 samples, validate on 559 samples\n",
      "Epoch 1/30\n",
      "2236/2236 [==============================] - 20s 9ms/step - loss: 1.5729 - activation_10_loss: 0.6433 - activation_11_loss: 0.9296 - val_loss: 1.4241 - val_activation_10_loss: 0.5025 - val_activation_11_loss: 0.9216\n",
      "Epoch 2/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 1.3815 - activation_10_loss: 0.4519 - activation_11_loss: 0.9296 - val_loss: 1.2768 - val_activation_10_loss: 0.3556 - val_activation_11_loss: 0.9212\n",
      "Epoch 3/30\n",
      "2236/2236 [==============================] - 20s 9ms/step - loss: 1.1107 - activation_10_loss: 0.3206 - activation_11_loss: 0.7900 - val_loss: 0.9676 - val_activation_10_loss: 0.2846 - val_activation_11_loss: 0.6831\n",
      "Epoch 4/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.8602 - activation_10_loss: 0.2738 - activation_11_loss: 0.5865 - val_loss: 0.7500 - val_activation_10_loss: 0.2458 - val_activation_11_loss: 0.5042\n",
      "Epoch 5/30\n",
      "2236/2236 [==============================] - 20s 9ms/step - loss: 0.7228 - activation_10_loss: 0.2574 - activation_11_loss: 0.4655 - val_loss: 0.6954 - val_activation_10_loss: 0.2379 - val_activation_11_loss: 0.4575\n",
      "Epoch 6/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.6774 - activation_10_loss: 0.2436 - activation_11_loss: 0.4338 - val_loss: 0.6650 - val_activation_10_loss: 0.2294 - val_activation_11_loss: 0.4356\n",
      "Epoch 7/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.6535 - activation_10_loss: 0.2339 - activation_11_loss: 0.4196 - val_loss: 0.6584 - val_activation_10_loss: 0.2349 - val_activation_11_loss: 0.4235\n",
      "Epoch 8/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.6367 - activation_10_loss: 0.2272 - activation_11_loss: 0.4095 - val_loss: 0.6369 - val_activation_10_loss: 0.2221 - val_activation_11_loss: 0.4148\n",
      "Epoch 9/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.6241 - activation_10_loss: 0.2219 - activation_11_loss: 0.4022 - val_loss: 0.6218 - val_activation_10_loss: 0.2102 - val_activation_11_loss: 0.4115\n",
      "Epoch 10/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.6116 - activation_10_loss: 0.2174 - activation_11_loss: 0.3942 - val_loss: 0.6219 - val_activation_10_loss: 0.2137 - val_activation_11_loss: 0.4082\n",
      "Epoch 11/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.6014 - activation_10_loss: 0.2136 - activation_11_loss: 0.3878 - val_loss: 0.6078 - val_activation_10_loss: 0.2088 - val_activation_11_loss: 0.3991\n",
      "Epoch 12/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.5930 - activation_10_loss: 0.2100 - activation_11_loss: 0.3830 - val_loss: 0.6125 - val_activation_10_loss: 0.2174 - val_activation_11_loss: 0.3951\n",
      "Epoch 13/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5858 - activation_10_loss: 0.2073 - activation_11_loss: 0.3786 - val_loss: 0.6053 - val_activation_10_loss: 0.2188 - val_activation_11_loss: 0.3865\n",
      "Epoch 14/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5784 - activation_10_loss: 0.2047 - activation_11_loss: 0.3736 - val_loss: 0.5861 - val_activation_10_loss: 0.2003 - val_activation_11_loss: 0.3858\n",
      "Epoch 15/30\n",
      "2236/2236 [==============================] - 19s 9ms/step - loss: 0.5708 - activation_10_loss: 0.2011 - activation_11_loss: 0.3697 - val_loss: 0.5812 - val_activation_10_loss: 0.2018 - val_activation_11_loss: 0.3794\n",
      "Epoch 16/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.5633 - activation_10_loss: 0.1984 - activation_11_loss: 0.3649 - val_loss: 0.5750 - val_activation_10_loss: 0.1953 - val_activation_11_loss: 0.3797\n",
      "Epoch 17/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.5584 - activation_10_loss: 0.1965 - activation_11_loss: 0.3619 - val_loss: 0.5803 - val_activation_10_loss: 0.1924 - val_activation_11_loss: 0.3879\n",
      "Epoch 18/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5539 - activation_10_loss: 0.1953 - activation_11_loss: 0.3587 - val_loss: 0.5681 - val_activation_10_loss: 0.1935 - val_activation_11_loss: 0.3746\n",
      "Epoch 19/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.5486 - activation_10_loss: 0.1922 - activation_11_loss: 0.3565 - val_loss: 0.5657 - val_activation_10_loss: 0.1942 - val_activation_11_loss: 0.3715\n",
      "Epoch 20/30\n",
      "2236/2236 [==============================] - 19s 9ms/step - loss: 0.5451 - activation_10_loss: 0.1910 - activation_11_loss: 0.3541 - val_loss: 0.5646 - val_activation_10_loss: 0.1901 - val_activation_11_loss: 0.3745\n",
      "Epoch 21/30\n",
      "2236/2236 [==============================] - 22s 10ms/step - loss: 0.5407 - activation_10_loss: 0.1891 - activation_11_loss: 0.3516 - val_loss: 0.5569 - val_activation_10_loss: 0.1862 - val_activation_11_loss: 0.3707\n",
      "Epoch 22/30\n",
      "2236/2236 [==============================] - 20s 9ms/step - loss: 0.5397 - activation_10_loss: 0.1886 - activation_11_loss: 0.3511 - val_loss: 0.5531 - val_activation_10_loss: 0.1820 - val_activation_11_loss: 0.3711\n",
      "Epoch 23/30\n",
      "2236/2236 [==============================] - 20s 9ms/step - loss: 0.5360 - activation_10_loss: 0.1870 - activation_11_loss: 0.3489 - val_loss: 0.5613 - val_activation_10_loss: 0.1886 - val_activation_11_loss: 0.3727\n",
      "Epoch 24/30\n",
      "2236/2236 [==============================] - 19s 9ms/step - loss: 0.5325 - activation_10_loss: 0.1864 - activation_11_loss: 0.3462 - val_loss: 0.5557 - val_activation_10_loss: 0.1876 - val_activation_11_loss: 0.3681\n",
      "Epoch 25/30\n",
      "2236/2236 [==============================] - 19s 9ms/step - loss: 0.5288 - activation_10_loss: 0.1847 - activation_11_loss: 0.3440 - val_loss: 0.5567 - val_activation_10_loss: 0.1866 - val_activation_11_loss: 0.3702\n",
      "Epoch 26/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5270 - activation_10_loss: 0.1851 - activation_11_loss: 0.3419 - val_loss: 0.5571 - val_activation_10_loss: 0.1830 - val_activation_11_loss: 0.3741\n",
      "Epoch 27/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5230 - activation_10_loss: 0.1824 - activation_11_loss: 0.3406 - val_loss: 0.5476 - val_activation_10_loss: 0.1853 - val_activation_11_loss: 0.3623\n",
      "Epoch 28/30\n",
      "2236/2236 [==============================] - 18s 8ms/step - loss: 0.5208 - activation_10_loss: 0.1815 - activation_11_loss: 0.3393 - val_loss: 0.5545 - val_activation_10_loss: 0.1867 - val_activation_11_loss: 0.3679\n",
      "Epoch 29/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5185 - activation_10_loss: 0.1816 - activation_11_loss: 0.3369 - val_loss: 0.5554 - val_activation_10_loss: 0.1906 - val_activation_11_loss: 0.3648\n",
      "Epoch 30/30\n",
      "2236/2236 [==============================] - 19s 8ms/step - loss: 0.5151 - activation_10_loss: 0.1805 - activation_11_loss: 0.3346 - val_loss: 0.5461 - val_activation_10_loss: 0.1845 - val_activation_11_loss: 0.3616\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(Yin, [Left,Right], epochs=30, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lpred, Rpred = model.predict(Y[4000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '21', 'septembre', '2000', ':', 'les', 'rÃ©vÃ©lations', 'posthumes', 'de', 'Jean-Claude', 'MÃ©ry', '(', 'cassette', 'MÃ©ry', ')', 'sont', 'publiÃ©es', 'par', 'le', 'journal', 'Le', 'Monde', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.51808262]\n",
      " [ 0.        ]\n",
      " [ 1.06027424]\n",
      " [ 0.        ]\n",
      " [ 2.71995115]\n",
      " [ 0.        ]\n",
      " [ 2.04945779]\n",
      " [ 0.        ]\n",
      " [ 1.07729733]\n",
      " [ 1.14353406]\n",
      " [ 0.44215032]\n",
      " [ 1.10520411]\n",
      " [ 1.05459976]\n",
      " [ 0.        ]\n",
      " [ 2.13619494]\n",
      " [ 0.        ]\n",
      " [ 1.00193095]\n",
      " [ 0.99981958]\n",
      " [ 1.06076026]\n",
      " [ 1.04573667]\n",
      " [ 1.08062315]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Lpred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 1.01456225]\n",
      " [ 0.        ]\n",
      " [ 2.68298173]\n",
      " [ 0.        ]\n",
      " [ 0.93558681]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 1.7421037 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 6.3043251 ]\n",
      " [ 0.        ]\n",
      " [ 1.07220888]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 9.12233067]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Rpred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
