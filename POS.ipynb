{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Bidirectional, Dense, Input, Dropout, LSTM, Activation, TimeDistributed, BatchNormalization, concatenate, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.constraints import max_norm\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from grail_data_utils import *\n",
    "\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "pos1_to_index = load_obj('pos1_to_index')\n",
    "index_to_pos1 = load_obj('index_to_pos1')\n",
    "pos2_to_index = load_obj('pos2_to_index')\n",
    "index_to_pos2 = load_obj('index_to_pos2')\n",
    "p1_to_integer = load_obj('p1_to_integer')\n",
    "integer_to_p1 = load_obj('integer_to_p1')\n",
    "p2_to_integer = load_obj('p2_to_integer')\n",
    "integer_to_p2 = load_obj('integer_to_p2')\n",
    "p3_to_integer = load_obj('p3_to_integer')\n",
    "integer_to_p3 = load_obj('integer_to_p3')\n",
    "p4_to_integer = load_obj('p4_to_integer')\n",
    "integer_to_p4 = load_obj('integer_to_p4')\n",
    "s1_to_integer = load_obj('s1_to_integer')\n",
    "integer_to_s1 = load_obj('integer_to_s1')\n",
    "s2_to_integer = load_obj('s2_to_integer')\n",
    "integer_to_s2 = load_obj('integer_to_s2')\n",
    "s3_to_integer = load_obj('s3_to_integer')\n",
    "integer_to_s3 = load_obj('integer_to_s3')\n",
    "s4_to_integer = load_obj('s4_to_integer')\n",
    "integer_to_s4 = load_obj('integer_to_s4')\n",
    "s5_to_integer = load_obj('s5_to_integer')\n",
    "integer_to_s5 = load_obj('integer_to_s5')\n",
    "s6_to_integer = load_obj('s6_to_integer')\n",
    "integer_to_s6 = load_obj('integer_to_s6')\n",
    "s7_to_integer = load_obj('s7_to_integer')\n",
    "integer_to_s7 = load_obj('integer_to_s7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correct\n",
    "numClasses = len(index_to_pos1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistake \n",
    "numClasses = len(index_to_pos2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRO': 1, 'CS': 2, 'ADVWH': 3, 'DET': 4, 'PROREL': 5, 'ADJWH': 6, 'PROWH': 7, 'VPP': 8, 'VIMP': 9, 'VS': 10, 'P+PRO': 11, 'P': 12, 'VPR': 13, 'ADV': 14, 'CLO': 15, 'NPP': 16, 'ADJ': 17, 'CLR': 18, 'ET': 19, 'I': 20, 'NC': 21, 'CLS': 22, 'VINF': 23, 'P+D': 24, 'DETWH': 25, 'PONCT': 26, 'CC': 27, 'V': 28, 'PREF': 29}\n"
     ]
    }
   ],
   "source": [
    "print(pos1_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = 0\n",
    "        maxlen = 0\n",
    "        text = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            length = len(line)\n",
    "            if (length > maxlen):\n",
    "                maxlen = length\n",
    "            text[lines] = line\n",
    "            lines = lines + 1\n",
    "    return text, lines, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_vocab(text):\n",
    "    vocab = set()\n",
    "    for (k,v) in text.items():\n",
    "        for i in range(len(v)):\n",
    "            word = v[i]\n",
    "            if word not in vocab:\n",
    "                vocab.add(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, numLines, maxline = read_text_file('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Le', 'crédit', 'foncier', 'va', 'prendre', 'une', 'participation', 'de', '20', '%', 'dans', 'le', 'capital', 'de', \"l'\", 'immobilière', 'constructions', 'de', 'Paris', '(', 'ICP', ')', ',', 'qui', 'détient', \"d'\", 'importantes', 'participations', 'dans', 'des', 'sociétés', 'immobilières', \"d'\", 'investissement', '(', 'SII', ')', 'cotées', ',', 'comme', 'Sefimeg', ',', 'Cofimeg', ',', 'et', 'dans', 'plusieurs', 'sicomi', ',', '\"', 'dans', 'lesquelles', 'le', 'crédit', 'foncier', 'est', 'déjà', 'présent', '\"', 'précise', 'le', 'communiqué', '.']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(numLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxLen = 266\n",
    "\n",
    "vocab = text_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ce', 'convertibles', 'formées', 'deux', 'cosmétiques', 'Toledo', 'travail', 'dès', 'les', 'Paris', 'plus', 'Europe', 'communiqué', 'il', 'tchécoslovaque', 'associations', 'processus', 'où', 'participations', 'malgré', 'sont', 'européenne', 'devant', 'animaux', 'incapables', 'tests', '1989', 'lesquelles', 'ouest-allemands', 'réunion', 'Vaclav', 'assistance', 'octobre', 'atteignait', 'Biscaye', 'avait', 'transférable', 'effet', 'se', 'chiffre', '-', 'mixtes', 'aux', 'Luft', 'discours', 'simple', 'compte', 'reculé', 'détérioration', '12', 'date', 'CEE', 'industrie', '...', '10', 'importante', 'milliards', 'va', 'intervienne', 'hauteur', 'cotées', 'autoriser', 'un', 'financière', 'En', 'commerce', 'selon', 'douze', 'investissement', 'trouve', 'constructions', 'par', 'production', 'actuellement', 'ensemble', 'échanges', 'ICP', 'moyennes', 'SII', 'réalisé', 'autonomie', '10,7', \"n'\", 'envisagent', 'Mr', 'baisse', 'vingt-cinq', 'parfums', 'majoritaires', 'alors', 'est-allemand', 'approuvée', '1988', 'déjà', 'Ils', 'donné', 'ou', '1991', 'passé', 'présent', 'Euro', 'avoir', 'centenaire', 'ses', 'français', ',', '15', 'leurs', 'provenance', 'supermarchés', 'Cette', 'vice-', 'Rubio', 'la', 'sociétés', 'soviétique', 'mutuelles', 'assemblée', 'britanniques', 'capital', 'allemands', 'sein', '20', 'son', 'crise', 'Sefimeg', 'contre', 'sur', 'et', '8,9', 'mettre', 'crédit', \"d'\", 'orientale', 'groupes', 'soumettre', 'proposé', 'unanimement', 'majorité', 'taux', 'plusieurs', 'monétaires', 'démantèlement', 'négociations', 'détient', 'novembre', 'mercredi', \"l'\", 'nette', 'appelé', 'pays', 'des', 'décembre', 'annoncé', '-ils', 'Peta', 'au', 'plein', 'dans', 'Cacharel', 'eux-mêmes', 'répliqué', 'pas', 'qui', 'statistiques', '8', 'autres', '6,9', 'groupe', 'Phas', 'Blanzy', 'fusion', 'majoritaire', 'conseil', 'diminution', 'affaires', 'Prague', 'Réunis', 'veille', 'Italie', 'décès', \"L'\", 'économiques', '0,8', 'Etats', 'mette', 'y', 'partenaires', 'précédente', 'sicomi', 'Dans', 'tôt', 'président', 'tumultueuses', 'sa', 'premier', 'conseillers', '17', 'RFA', 'numéro', ';', 'mondial', 'profit', 'semaine', 'derniers', 'économique', 'avec', 'pharmacies', 'chômage', '.', 'paraît', 'participants', 'foncier', 'établissements', 'ministre', 'industriels', 'quelque', 'été', 'privée', 'chambres', 'nécessité', 'janvier', 'Etats-membres', 'manifestations', \"qu'\", 'prendre', 'plénière', 'boycottage', 'comme', 'forment', 'demandent', 'Laroche', 'membres', 'immobilières', 'moins', '16', 'intervenu', 'La', 'ces', 'Cofimeg', 'italiens', 'également', 'définitif', 'Biotherm', '24,45', 'en', 'association', 'mutuelle', ')', 'Monde', 'unité', 'tenant', 'marques', 'abandon', 'que', 'etc', 'permanente', 'Oréal', 'pourraient', 'fin', 'affirmé', 'devises', 'Guy', 'stat', 'finances', '(', 'Depuis', '150', 'montrés', 'Hormis', 'ont', 'américaine', 'militants', 'quasi', 'rouble', 'grave', 'délégation', 'entreprises', 'eu', 'accord', 'situation', 'Selon', 'COMECON', 'seulement', 'Conduits', 'voudraient', 'une', 'précise', 'mois', 'faveur', 'Mme', 'était', 'mardi', 'communauté', '50', 'lors', 'CAEM', 'succession', '\"', 'Klaus', 'Mariano', 'pour', 'est', 'le', 'première', 'cesser', 'notamment', 'trois', 'participation', 'francs', 'décidé', 'migrations', 'Quant', 'doit', 'Aussi', 'cinq', 'entité', 'petites', 'Bilbao', 'de', 'RDA', 'arbitrage', 'prononcé', 'protection', 'France', 'toutefois', 'étaient', 'autorités', 'examinée', 'organiser', 'mais', 'jours', 'sièges', 'capitaux', 'proposition', 'gouverneur', \"s'\", 'convenus', 'charge', 'Espagne', 'produits', 'immobilière', 'on', 'testés', 'ministres', 'ouverte', 'vente', 'être', 'Plusieurs', 'Sofia', 'responsables', '%', 'banque', 'à', 'monnaies', 'Mans', 'Danemark', 'marché', 'Pedro', '9', 'du', 'Vichy', 'comptabiliser', 'urgent', 'importantes', 'combinats', 'Contre', 'point', 'a', '5', 'Le', 'ans', 'usines', 'Christa', 'semaines'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = indexify(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_prefvec(word, alen, afset, af_to_int):\n",
    "    if len(word) >= alen:\n",
    "        pref = word[:alen]\n",
    "        if pref in afset:\n",
    "            int = af_to_int[pref]\n",
    "        else:\n",
    "            int = af_to_int['*UNK*']\n",
    "    else:\n",
    "        int = af_to_int['*OOR*']\n",
    "    return to_categorical(int, len(afset)+1)\n",
    "\n",
    "\n",
    "def word_to_sufvec(word, alen, afset, af_to_int):\n",
    "    if len(word) >= alen:\n",
    "        pref = word[-alen:]\n",
    "        if pref in afset:\n",
    "            int = af_to_int[pref]\n",
    "        else:\n",
    "            int = af_to_int['*UNK*']\n",
    "    else:\n",
    "        int = af_to_int['*OOR*']\n",
    "    return to_categorical(int, len(afset)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix1 = p1_to_integer.keys()\n",
    "prefix2 = p2_to_integer.keys()\n",
    "prefix3 = p3_to_integer.keys()\n",
    "prefix4 = p4_to_integer.keys()\n",
    "\n",
    "suffix1 = s1_to_integer.keys()\n",
    "suffix2 = s2_to_integer.keys()\n",
    "suffix3 = s3_to_integer.keys()\n",
    "suffix4 = s4_to_integer.keys()\n",
    "suffix5 = s5_to_integer.keys()\n",
    "suffix6 = s6_to_integer.keys()\n",
    "suffix7 = s7_to_integer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_prefix_vector(word):\n",
    "    p1 = word_to_prefvec(word, 1, prefix1, p1_to_integer)\n",
    "    p2 = word_to_prefvec(word, 2, prefix2, p2_to_integer)\n",
    "    p3 = word_to_prefvec(word, 3, prefix3, p3_to_integer)\n",
    "    p4 = word_to_prefvec(word, 4, prefix4, p4_to_integer)\n",
    "    return np.concatenate((p1,p2,p3,p4))\n",
    "\n",
    "def word_to_suffix_vector(word):\n",
    "    s1 = word_to_sufvec(word, 1, suffix1, s1_to_integer)\n",
    "    s2 = word_to_sufvec(word, 2, suffix2, s2_to_integer)\n",
    "    s3 = word_to_sufvec(word, 3, suffix3, s3_to_integer)\n",
    "    s4 = word_to_sufvec(word, 4, suffix4, s4_to_integer)\n",
    "    s5 = word_to_sufvec(word, 5, suffix5, s5_to_integer)\n",
    "    s6 = word_to_sufvec(word, 6, suffix6, s6_to_integer)\n",
    "    s7 = word_to_sufvec(word, 7, suffix7, s7_to_integer)\n",
    "    return np.concatenate((s1,s2,s3,s4,s5,s6,s7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affixes(vocab):\n",
    "    \n",
    "    word_to_suffix = {}\n",
    "    word_to_prefix = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        w = word.lower()\n",
    "        w = re.sub(r'[0-8]', '9', w)\n",
    "        pvec = word_to_prefix_vector(w)\n",
    "        svec = word_to_suffix_vector(w)\n",
    "        word_to_prefix[word] = pvec\n",
    "        word_to_suffix[word] = svec\n",
    "        \n",
    "    return word_to_prefix, word_to_suffix\n",
    "\n",
    "word_to_prefix, word_to_suffix = compute_affixes(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format('../wang2vec/frwiki_cwindow50_10.bin', binary=True)\n",
    "veclength = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknowns:  63\n",
      "In vocabulary:  312\n"
     ]
    }
   ],
   "source": [
    "word_to_vec_map = {}\n",
    "unknowns = set()\n",
    "invoc = 0\n",
    "\n",
    "for w in vocab:\n",
    "    wn = normalize_word(w)\n",
    "    wr = remove_prefix(wn, \"-t-\")\n",
    "    wr = remove_prefix(wr, \"-\")\n",
    "    try:\n",
    "        vec = wv[wr]\n",
    "        invoc = invoc + 1\n",
    "    except:\n",
    "        unknowns.add(w)\n",
    "        vec = np.zeros(veclength)\n",
    "    word_to_vec_map[w] = vec\n",
    "\n",
    "print('Unknowns: ', len(unknowns))\n",
    "print('In vocabulary: ', invoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pref = np.zeros((numLines, 266, 5788))\n",
    "X_suff = np.zeros((numLines, 266, 14983))\n",
    "X_word_emb = np.zeros((numLines, 266, 50))\n",
    "X_indices = np.zeros((numLines,266))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(numLines):\n",
    "    line = text[i]\n",
    "    for j in range(len(line)):\n",
    "        word = line[j]\n",
    "        X_pref[i,j,:] = word_to_prefix[word]\n",
    "        X_suff[i,j,:] = word_to_suffix[word]\n",
    "        X_word_emb[i,j,:] = word_to_vec_map[word]\n",
    "        X_indices[i,j] = word_to_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained fastText vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 2           # adding 1 for 'unknown'and 1 to fit Keras embedding\n",
    "    emb_dim = word_to_vec_map[\"est\"].shape[0]    # get dimensionality of word vectors\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=False,mask_zero=True)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def POS_model(input_shape, word_to_vec_map, word_to_prefix, word_to_suffix, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the graph for the part-of-speech tagger model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    prefix_emb = pretrained_embedding_layer(word_to_prefix, word_to_index)\n",
    "    suffix_emb = pretrained_embedding_layer(word_to_suffix, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    pref = prefix_emb(sentence_indices)\n",
    "    suff = suffix_emb(sentence_indices)\n",
    "    P = Dense(32,kernel_constraint=max_norm(5.))(pref)\n",
    "    S = Dense(32,kernel_constraint=max_norm(5.))(suff)\n",
    "    merged = concatenate([embeddings,P,S])\n",
    "    X = Dropout(0.5)(merged)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = Bidirectional(LSTM(128, recurrent_dropout=0.2, kernel_constraint=max_norm(5.), return_sequences=True))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    Y = TimeDistributed(Dropout(0.2))(X)\n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    Y = TimeDistributed(Dense(numClasses, activation='softmax'))(Y)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=Y)\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 266)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 266, 5788)    2182076     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 266, 14983)   5648591     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 266, 50)      18850       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 266, 32)      185248      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 266, 32)      479488      embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 266, 114)     0           embedding_13[0][0]               \n",
      "                                                                 dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 266, 114)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 266, 256)     248832      dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 266, 256)     1024        bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 266, 256)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 266, 32)      8224        time_distributed_8[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 8,772,333\n",
      "Trainable params: 922,304\n",
      "Non-trainable params: 7,850,029\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = POS_model((maxLen,), word_to_vec_map, word_to_prefix, word_to_suffix, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model = load_model('best_pos1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 266)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 266, 5788)    175387976   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 266, 14983)   454014866   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 266, 50)      1515100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 266, 32)      185248      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 266, 32)      479488      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 266, 114)     0           embedding_1[0][0]                \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 266, 114)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 266, 256)     248832      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 266, 256)     1024        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 266, 256)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 266, 32)      8224        time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 631,840,758\n",
      "Trainable params: 922,304\n",
      "Non-trainable params: 630,918,454\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = trained_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights2 = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30302, 5788)\n",
      "(377, 5788)\n",
      "(30302, 14983)\n",
      "(377, 14983)\n",
      "(30302, 50)\n",
      "(377, 50)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(weights[0]))\n",
    "print(np.shape(weights2[0]))\n",
    "\n",
    "print(np.shape(weights[1]))\n",
    "print(np.shape(weights2[1]))\n",
    "\n",
    "print(np.shape(weights[2]))\n",
    "print(np.shape(weights2[2]))\n",
    "\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,len(weights)):\n",
    "    weights2[i] = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'PRO', 2: 'CS', 3: 'ADVWH', 4: 'DET', 5: 'PROREL', 6: 'ADJWH', 7: 'PROWH', 8: 'VPP', 9: 'VIMP', 10: 'VS', 11: 'P+PRO', 12: 'P', 13: 'VPR', 14: 'ADV', 15: 'CLO', 16: 'NPP', 17: 'ADJ', 18: 'CLR', 19: 'ET', 20: 'I', 21: 'NC', 22: 'CLS', 23: 'VINF', 24: 'P+D', 25: 'DETWH', 26: 'PONCT', 27: 'CC', 28: 'V', 29: 'PREF'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le|DET crédit|NC foncier|ADJ va|V prendre|VINF une|DET participation|NC de|P 20|DET %|NC dans|P le|DET capital|NC de|P l'|DET immobilière|NPP constructions|NPP de|P Paris|NPP (|PONCT ICP|NPP )|PONCT ,|PONCT qui|PROREL détient|V d'|DET importantes|ADJ participations|NC dans|P des|DET sociétés|NC immobilières|ADJ d'|P investissement|NC (|PONCT SII|NPP )|PONCT cotées|VPP ,|PONCT comme|ADV Sefimeg|NPP ,|PONCT Cofimeg|NPP ,|PONCT et|CC dans|P plusieurs|DET sicomi|NC ,|PONCT \"|PONCT dans|P lesquelles|PROREL le|DET crédit|NC foncier|ADJ est|V déjà|ADV présent|ADJ \"|PONCT précise|V le|DET communiqué|NC .|PONCT \n",
      "Dans|P le|DET capital|NC d'|P ICP|NC ,|PONCT où|PROREL le|DET groupe|NC centenaire|ADJ Blanzy|NPP est|V majoritaire|ADJ ,|PONCT on|CLS trouve|V également|ADV les|DET mutuelles|NC du|P+D Mans|NPP ,|PONCT à|P hauteur|NC de|P 20|DET %|NC .|PONCT \n",
      "Le|DET vice-|PREF premier|ADJ ministre|NC est-allemand|ADJ en|P charge|NC des|P+D affaires|NC économiques|ADJ ,|PONCT Mme|NC Christa|NPP Luft|NPP ,|PONCT s'|CLR est|V prononcé|VPP mercredi|NC 17|ADJ janvier|NC en|P faveur|NC du|P+D démantèlement|NC des|P+D combinats|NC ,|PONCT lors|ADV d'|P un|DET discours|NC prononcé|VPP devant|P l'|DET assemblée|NC permanente|ADJ des|P+D chambres|NC de|P commerce|NC et|CC d'|P industrie|NC de|P RFA|NPP .|PONCT \n",
      "Le|DET ministre|NC n'|ADV a|V pas|ADV donné|VPP de|DET date|NC précise|ADJ pour|P le|DET démantèlement|NC des|P+D quelque|ADJ 150|ADJ groupes|NC industriels|ADJ responsables|ADJ de|P la|DET majorité|NC de|P la|DET production|NC en|P RDA|NPP ,|PONCT mais|CC a|V affirmé|VPP qu'|CS il|CLS était|V urgent|ADJ d'|P autoriser|VINF l'|DET autonomie|NC des|P+D petites|ADJ et|CC moyennes|ADJ entreprises|NC .|PONCT \n",
      "La|DET veille|NC ,|PONCT Mme|NC Luft|NPP avait|V déjà|ADV annoncé|VPP que|CS des|DET capitaux|NC ouest-allemands|ADJ pourraient|V être|VINF majoritaires|ADJ dans|P le|DET capital|NC de|P sociétés|NC mixtes|ADJ formées|VPP par|P des|DET partenaires|NC des|P+D deux|ADJ Etats|NC allemands|ADJ .|PONCT \n",
      "Selon|P les|DET statistiques|NC d'|P Euro|NPP -|PONCT stat|NPP ,|PONCT le|DET taux|NC de|P chômage|NC atteignait|V 8,9|DET %|NC en|P novembre|NC 1989|ADJ dans|P la|DET communauté|NPP européenne|ADJ contre|P 9|DET %|NC en|P octobre|NC .|PONCT \n",
      "Hormis|P le|DET Danemark|NPP et|CC la|DET RFA|NPP -|PONCT où|PROREL la|DET situation|NC \"|PONCT doit|V être|VINF examinée|VPP en|P tenant|VPR compte|NC des|P+D migrations|NC en|P provenance|NC de|P la|DET RDA|NPP et|CC d'|DET autres|ADJ pays|NC d'|P Europe|NPP orientale|ADJ \"|PONCT -|PONCT il|CLS n'|ADV y|CLO a|V pas|ADV eu|VPP de|DET détérioration|NC du|P+D marché|NC du|P+D travail|NC .|PONCT \n",
      "En|P douze|DET mois|NC ,|PONCT le|DET taux|NC de|P chômage|NC dans|P l'|DET ensemble|NC de|P la|DET CEE|NPP a|V reculé|VPP de|P 0,8|DET point|NC .|PONCT \n",
      "La|DET diminution|NC paraît|V ,|PONCT toutefois|ADV ,|PONCT moins|ADV nette|ADJ en|P France|NPP et|CC en|P Italie|NPP .|PONCT \n",
      "Quant|ADV au|P+D Danemark|NPP ,|PONCT son|DET taux|NC de|P chômage|NC est|V passé|VPP de|P 6,9|DET %|NC à|P 8|DET %|NC et|CC de|P 10|DET %|NC à|P 10,7|DET %|NC pour|P les|DET moins|ADV de|P vingt-cinq|DET ans|NC alors|ADV que|CS ,|PONCT pour|P ces|DET derniers|NC ,|PONCT \"|PONCT la|DET baisse|NC a|V été|VPP importante|ADJ \"|PONCT dans|P les|DET autres|ADJ Etats-membres|NC .|PONCT \n",
      "Plusieurs|DET associations|NC pour|P la|DET protection|NC des|P+D animaux|NC ont|V appelé|VPP mercredi|NC 17|ADJ janvier|NC au|P+D boycottage|NC des|P+D produits|NC de|P L'|DET Oréal|NPP .|PONCT \n",
      "Conduits|VPP par|P l'|DET association|NC américaine|ADJ Peta|NPP ,|PONCT des|DET militants|NC allemands|ADJ ,|PONCT italiens|ADJ ,|PONCT français|ADJ et|CC britanniques|ADJ voudraient|V en|P effet|NC que|CS le|DET numéro|NC un|ADJ mondial|ADJ des|P+D cosmétiques|NC mette|VS fin|NC aux|P+D tests|NC sur|P les|DET animaux|NC .|PONCT .|PONCT \n",
      "Ils|CLS demandent|V aux|P+D pharmacies|NC et|CC aux|P+D supermarchés|NC de|P cesser|VINF la|DET vente|NC des|P+D produits|NC de|P ce|DET groupe|NC et|CC envisagent|V d'|P organiser|VINF des|DET manifestations|NC devant|P les|DET sièges|NC des|P+D usines|NC .|PONCT \n",
      "L'|DET Oréal|NPP ,|PONCT qui|PROREL a|V réalisé|VPP un|DET chiffre|NC d'|P affaires|NC de|P 24,45|DET milliards|NC de|P francs|NC en|P 1988|PRO avec|P notamment|ADV les|DET marques|NC Vichy|NPP ,|PONCT Biotherm|NPP ,|PONCT Phas|NPP ,|PONCT les|DET parfums|NC Guy|NPP Laroche|NPP ,|PONCT Cacharel|NPP ,|PONCT etc|ADV .|PONCT ;|PONCT a|V répliqué|VPP dans|P un|DET communiqué|NC que|CS 5|DET %|NC seulement|ADV de|P ses|DET produits|NC étaient|V testés|VPP sur|P les|DET animaux|NC ...|PONCT Contre|P 50|DET %|NC ,|PONCT il|CLS y|CLO a|V cinq|DET ans|NC .|PONCT \n",
      "Réunis|VPP pour|P trois|DET jours|NC à|P Prague|NPP ,|PONCT les|DET ministres|NC des|P+D finances|NC des|P+D pays|NC membres|NC du|P+D COMECON|NPP (|PONCT ou|CC CAEM|NPP ,|PONCT conseil|NPP d'|P assistance|NC économique|ADJ mutuelle|ADJ )|PONCT sont|V convenus|VPP ,|PONCT le|DET mardi|NC 16|ADJ janvier|NC ,|PONCT de|P la|DET nécessité|NC de|P comptabiliser|VINF au|P+D plus|ADV tôt|ADV leurs|DET échanges|NC en|P devises|NC convertibles|ADJ .|PONCT \n",
      "La|DET semaine|NC précédente|ADJ ,|PONCT lors|ADV de|P la|DET réunion|NC plénière|ADJ du|P+D COMECON|NPP à|P Sofia|NPP ,|PONCT la|DET délégation|NC soviétique|ADJ avait|V proposé|VPP que|CS l'|DET abandon|NC du|P+D rouble|NC transférable|ADJ (|PONCT simple|ADJ unité|NC de|P compte|NC )|PONCT au|P+D profit|NC des|P+D monnaies|NC convertibles|ADJ ,|PONCT intervienne|ADJ dès|P 1991|PRO .|PONCT \n",
      "Cette|DET proposition|NC paraît|V avoir|VINF ,|PONCT selon|P le|DET ministre|NC des|P+D finances|NC tchécoslovaque|ADJ ,|PONCT Mr|NC Vaclav|NPP Klaus|NPP ,|PONCT \"|PONCT été|VPP approuvée|VPP quasi|ADV unanimement|ADV \"|PONCT par|P les|DET participants|NC à|P la|DET réunion|NC de|P Prague|NPP .|PONCT \n",
      "Le|DET gouverneur|NC de|P la|DET banque|NC d'|P Espagne|NPP ,|PONCT Mr|NC Mariano|NPP Rubio|NPP ,|PONCT est|V intervenu|VPP le|DET mercredi|NC 17|ADJ janvier|NC pour|P mettre|VINF fin|NC à|P la|DET grave|ADJ crise|NC ouverte|VPP au|P+D sein|NC de|P la|DET banque|NC de|P Bilbao|NPP et|CC de|P la|DET banque|NC de|P Biscaye|NPP actuellement|ADV en|P plein|ADJ processus|NC de|P fusion|NC .|PONCT \n",
      "Depuis|P le|DET décès|NC ,|PONCT le|DET 12|ADJ décembre|NC 1989|ADJ ,|PONCT du|P+D président|NC de|P la|DET banque|NC de|P Biscaye|NPP ,|PONCT Pedro|NPP Toledo|NPP (|PONCT Le|DET Monde|NPP du|P+D 15|ADJ décembre|NC )|PONCT ,|PONCT les|DET conseillers|NC des|P+D deux|ADJ établissements|NC qui|PROREL forment|V ensemble|ADV la|DET première|ADJ entité|NC financière|ADJ privée|ADJ du|P+D pays|NC s'|CLR étaient|V en|P effet|NC montrés|VPP incapables|ADJ ,|PONCT malgré|P cinq|DET semaines|NC de|P tumultueuses|ADJ négociations|NC ,|PONCT de|P se|CLR mettre|VINF d'|P accord|NC sur|P sa|DET succession|NC .|PONCT \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_indices)-1):\n",
    "    for j in range(len(X_indices[i]-1)):\n",
    "        if X_indices[i][j] != 0:\n",
    "            num = np.argmax(predictions[i][j])\n",
    "            wi = int(X_indices[i][j])\n",
    "            print(index_to_word[wi], end='')\n",
    "            print('|', end='')\n",
    "            print(index_to_pos1[num], end=' ')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
