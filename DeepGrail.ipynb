{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM part-of-speech tagging for the French Treebank: \n",
    "\n",
    "This notebook trains a part-of-speech tagger for the French Treebank using a vanilla bi-direction LSTM network.\n",
    "\n",
    "Run the following cell to load the Keras packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Bidirectional, Dense, Input, Dropout, LSTM, Activation, TimeDistributed, BatchNormalization, concatenate, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from grail_data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the TLGbank file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# very small initial part of corpus (only file aa1)\n",
    "# X, Y1, Y2, Z, vocabulary, vnorm, partsofspeech1, partsofspeech2, superset, maxLen = read_maxentdata('aa1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small initial part of corpus (files aa1, aa2, ab2 and ae1)\n",
    "# number of sentences, train: 1195, test: 398, dev: 399  \n",
    "# X, Y1, Y2, Z, vocabulary, vnorm, partsofspeech1, partsofspeech2, superset, maxLen = read_maxentdata('aa1_ae1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# entire corpus\n",
    "# number of sentences, train: 9449, test: 3150, dev: 3150\n",
    "X, Y1, Y2, Z, vocabulary, vnorm, partsofspeech1, partsofspeech2, superset, maxLen = read_maxentdata('m2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Longest sentence   :  266\n",
      "Number of POS tags :  32\n",
      "Number of supertags:  891\n"
     ]
    }
   ],
   "source": [
    "numClasses = len(partsofspeech2)+1\n",
    "numSuperClasses = len(superset)+1\n",
    "\n",
    "print()\n",
    "print(\"Longest sentence   : \", maxLen)\n",
    "print(\"Number of POS tags : \", numClasses)\n",
    "print(\"Number of supertags: \", numSuperClasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the input into train/dev/test\n",
    "\n",
    "Split the full training set into 60% train, 20% dev and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (9449,)\n",
      "Test:   (3150,)\n",
      "Dev:    (3150,)\n"
     ]
    }
   ],
   "source": [
    "# split the training data into the standard 60% train, 20% dev, 20% test \n",
    "X_train, X_testdev, Y_train, Y_testdev = train_test_split(X, Y2, test_size=0.4)\n",
    "X_test, X_dev, Y_test, Y_dev = train_test_split(X_testdev, Y_testdev, test_size=0.5)\n",
    "print(\"Train: \", X_train.shape)\n",
    "print(\"Test:  \", X_test.shape)\n",
    "print(\"Dev:   \", X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create auxiliary mappings\n",
    "\n",
    "Create mappings from supertags and the two sets of part-of-speech tags to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NAM': 1, 'VER:infi': 2, 'PRP': 3, 'DET:POS': 4, 'ADJ': 5, 'PRP:det': 6, 'VER:cond': 7, 'PRO:POS': 8, 'PUN:cit': 9, 'VER:pper': 10, 'VER:simp': 11, 'NOM': 12, 'DET:ART': 13, 'PRO:IND': 14, 'VER:futu': 15, 'ABR': 16, 'VER:ppre': 17, 'ADV': 18, 'PRO:REL': 19, 'VER:impe': 20, 'VER:pres': 21, 'SYM': 22, 'PRO:PER': 23, 'VER:impf': 24, 'VER:subp': 25, 'PRO:DEM': 26, 'PUN': 27, 'PRO': 28, 'INT': 29, 'KON': 30, 'NUM': 31}\n"
     ]
    }
   ],
   "source": [
    "# create mapping for the two POS tagset and for the supertags\n",
    "\n",
    "super_to_index, index_to_super = indexify(superset)\n",
    "pos1_to_index, index_to_pos1 = indexify(partsofspeech1)\n",
    "pos2_to_index, index_to_pos2 = indexify(partsofspeech2)\n",
    "print(pos2_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain the word vector information\n",
    "\n",
    "We are using a shell call to the compiled fastText code to produce a file _vectors.txt_ with the relevant vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1. Feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "french_suffixes = read_suffixes('suffixes.txt')\n",
    "print(len(french_suffixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_vector(\"seraient\", french_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually designed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_features(\"ABCD\"))\n",
    "print(word_features(\"Abcd\"))\n",
    "print(word_features(\"1234\"))\n",
    "print(word_features(\"*%\"))\n",
    "print(word_features(\"Ab-cd\"))\n",
    "print(word_features(\"-t-il\"))\n",
    "print(word_features(\"Contre\"))\n",
    "print(word_features(\"dans\"))\n",
    "print(word_features(\"anti-\"))\n",
    "print(word_features(\"et\"))\n",
    "print(word_features(\"ou\"))\n",
    "print(word_features(\"-t-il\"))\n",
    "print(word_features(\"-il\"))\n",
    "print(word_features(\"-tu\"))\n",
    "print(word_features(\"eussent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sending the vocabulary through the fasttext executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the vocabulary to an output file, then pass it to the fastText executable to produce the relevant word embeddings for our text. Since the fastText model is over 5 Gb, the shell call can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", 'w') as vocab_file:\n",
    "    for w in vnorm:\n",
    "        print(w, file=vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell call to `fasttext` for my Macbook Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!/Users/moot/Software/fastText-master/fasttext print-word-vectors /Users/moot/Corpus/wiki.fr/wiki.fr.bin < vocab.txt > vectors.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell call to `fasttest` for my Macbook Pro, with `wiki.fr.bin` on external drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!/Users/moot/Software/fastText-master/fasttext print-word-vectors /Volumes/LaCie/Corpus/fastText/wiki/wiki.fr.bin < vocab.txt > vectors.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all vector information\n",
    "\n",
    "Combined all information from fasttext, suffixes, manually selected features and the features for digits (which are not in fasttext) to produce combined feature vectors for all words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_vecs('vectors.txt', vnorm, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save auxiliary mapping to files\n",
    "\n",
    "Use pickle to save the auxiliary dictionaries to files. This avoids having to generate them from scratch when using the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(word_to_index, 'word_to_index')\n",
    "save_obj(index_to_word, 'index_to_word')\n",
    "save_obj(word_to_vec_map, 'word_to_vec_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Alternative word embeddings using CWindow\n",
    "\n",
    "Instead of the fastText embeddings, we can use the `wang2vec` embeddings which should be more appropriate for\n",
    "syntactic applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format('../wang2vec/frwiki_cwindow50_10.bin', binary=True)\n",
    "veclength = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute `word_to_vec_map` for all words in the vocabulary using only the `cwindow` embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'délestées', '67.240', 'Obadia', '104', '576', 'TEM', '190', 'hirudines', 'bls', 'militaro-industriel', 'Bénichou', 'ANGIOX', 'SAMT', 'Châtelet', '8,30', 'nord-américaine', 'vidusienne', 'CISL', '16', 'Shearson', 'Rowntree', 'Conforama', 'Saint-Etienne', 'Jean-Benoit', 'APCCA', 'opportunément', 'Matuikhin', 'secondariser', 'Limelight', '5,5', 'SMC', 'est-européennes', '40,5', 'Hughes', 'CTCE', 'Mentzelopoulos', 'Week-End', 'CVS', '22.627', 'Lanquetin', '1845', '3000', 'Abou-Dhabi', 'Extrême-Orient', 'FNTP', 'Gbm.H', '1735', 'hebdomadairement', 'story', 'repousserons', 'ADGAS', 'Sopharga', 'Xiaoping', 'Ebbene', 'Apep', 'KB', '5,8', 'non-dits', 'Costa-Rica', 'francilien', 'Gensis', '72.355', '19.000', 'Tardivon', 'Marone', '204', 'americana', 'CE1', 'Dassault-aviation', 'MSF', 'COJO', 'contre-exemple', 'Lagourgue', 'Harbour', 'Hany', 'sur-le-champ', 'Silco', 'DM', 'ASNL', 'Gèdre', '315.000', '47,5', 'Boutros-Ghali', 'Besançon', 'Jean-Claude', 'LEBAUBE', '11,7', 'Ascom', '13.719', 'Karachi', '167', 'appuierons', 'amicalement', '8,96', 'vice-procureure', 'Thieblin', '189', '385000', 'Velasquez', '1998', '1940', 'XIV', '175', 'Bydgoszcz', 'vingt-cinq', '6.000', '5,22', 'Kreditinstitut', 'injustement', 'A26', 'Scaime', 'télé-poche', 'Delebarre', 'casualwear', '32', 'Sandoglass', 'Altus', 'trachytique', '18,50', '35,7', 'Falcon-900', 'agro-industrielle', 'Lentement', '7,40', '4,55', '557,4', '88.560', 'Eridania-Béghin-Say', 'Meisonnier', 'Arte', 'Exchange', 'CBI', 'tranquillement', 'Modanais', 'CSEE', '1,58', 'entrainer', 'SNI-PEGC', '2e', 'Isle-sur-le-Doubs', 'Eaux-Bonnes', 'Peletons', 'hispano-Suiza', '/', '2.275', 'Oxbridge', 'réembauches', '28,85', 'Bridel', 'Charléty', 'Météo-France', 'Beazer', 'Aubert', 'Abou', 'SP', 'AMB', 'Gunthert', 'EUROCRATES', 'Ghosn', 'Exupéry', '1.600', 'ultrafrais', 'germano-britannique', '1943', 'SGAB', 'procéduraux', 'Moubarak', 'hôtellerie-restauration', '2,11', 'Pierre-Alain', 'Torte', 'Kélian', '228', 'SNAM', 'U-2', '29,5', 'Stordeur', 'INRETS', 'Smer', 'Veracruz', '4,99', 'Sutherland', 'surperformé', 'infléchissements', 'aincu', 'Amelot', 'éminemment', 'Chichester', '3,6', 'Fauchers', 'Bourdet', 'hébétement', 'Desktop', 'solidairement', 'GLAM', 'cinquante-sept', 'Bari', '106', '5,06', '43.900', 'manifestement', 'Saint-Victor', 'UBF', '130000', 'Abdesslam', 'classless', 'Seine-et-Marne', 'dissuadées', '25000', '7720', 'Electronics', 'Nijni-Novgorod', '3.515', 'Socpresse', 'Rendez-vous', 'Albin', 'FS', 'Aga', '66,72', 'S.', 'Gaulandeau', 'Desset', 'UGB', 'Somatem', 'BUENOS-AIRES', '10,8', 'Simeoni', '680.000', 'Kapferer', 'selette', '2.300', 'Visiblement', 'East', 'commuait', 'traitre', 'Monclain', 'Azun', 'vaguement', '40,13', '44,772', 'Allianz', 'Mazères', 'Mc', '196,66', 'X', 'Nicolaï', 'Grass', '6,7', 'mesures-miroirs', 'Edinter', 'Greenwich', '1991-1992', 'IPSN', '546-571', 'périodiquement', 'Braniff', 'appercevoir', '163', 'Seydoux', 'uninominal', '28,7', 'exclusivement', 'dix-sept', 'Hechter', 'Korando', 'Faugère', 'Régnier', 'Fiorentina', '1912', 'Sat', \"Who's\", 'situ', '19,6', 'Scrivener', 'Au-dessus', '0,8', 'damoclès', '12.102', 'Desjeux', '35,5', 'Springer', 'FNAC', '1989-1995', 'Marceau', 'BERD', 'Keller', 'Laeken', '8,16', 'grièvement', 'Saviem', 'positivement', 'clopidogel', 'Sharyo', 'Mesguish', '21,7', 'grand-chose', 'Pereire', 'Trente-cinq', 'bâtiments-relais', 'welfare', '1981-1982', 'Bouches-du-Rhône', 'Coframines', 'rebattent', 'Bosphore', 'Brenen', 'AGEFI', 'Exor', '17.500', 'Barbât', 'TF1', 'Stoclet', '1910', 'Attali', 'antiaméricain', 'Contractuellement', 'autogonflables', 'anser-anser', 'Renan', 'Mother', '...', 'Orzo', 'voierie', 'Foreign', 'Knightley', 'CHS', 'cinquante-six', 'Anchor', 'und', 'inévitablement', '144,8', 'Corp.', '671', '59', '190.500', 'Officiellement', 'ci', '60,60', 'Levrault', '227.000', 'Sainsaulieu', 'Little', '107,3', 'grata', 'Khrissate', 'pro-', 'Slobodan', 'Val-de-Marne', '102.433', '248000', 'CREDOC', '41.000', 'Compu.Add', 'Pohlmann', 'Debatisse', 'soixante-dix', '1983-1988', '5,39', 'Marchiani', 'rédigeai', 'trinquera', 'malheureusement', 'Aspè', 'après-référendum', 'Wang', 'IP', 'Autrement', 'GFD', 'Bund', 'Paradoxalement', 'Brochand', '172,8', 'AFB', 'vingt-sept', 'Cabaud', 'croissanterie', '3,56', 'Gyver', 'ZDS', '665', 'yankees', '191', '14,9', 'calmement', 'Sécu', 'Arbenz', '51,4', 'sous-estimant', '120', 'trente-six', 'Zeitung', 'cinquante-quatre', 'Electrolux', 'étroitement', 'Halfon', 'négativement', 'GNL', 'old', 'VGE', 'Clichois', '128.000', '252.000', 'SEMPAP', 'Manifestement', '1964', 'Inc.', 'Marriott', '22,3', 'progresseraient', '406,5', 'quarante-six', '171.000', 'Sp', 'ANDA', '140.000', '84.000', '874', '1.288.000', 'Île-de-France', 'effarouchera', 'Testut', 'théoriquement', 'FMI', '3,40', 'Suchard', '3048', '550', 'Védrine', 'cèderait', 'Creeks', 'Corp', 'Verneau', 'eux-mêmes', 'Rey', 'Haute-Savoie', 'Swatchmobile', 'Strong', 'suffisamment', 'économiserons', 'IPAA', 'complétement', 'non-respect', 'UIC', 'CSCE', 'États-Unis', 'veux', '25/01/06', 'Joffre', 'inquiéterais', 'montai', '796', 'sous-traitance', 'macro-', 'merchant', 'Saint-Sylvestre', 'Boulogne-Billancourt', 'Giuly', 'activement', 'autrement', 'Tourmalet', 'Vallerysthal', 'houilliers', '1965-1966-1967', 'saint-sylvestre', 'Scheveningen', 'Buenos-Aires', '1986', 'Shinbun', '666', '22', 'Dewatre', 'anecdotiquement', 'Marie-Jo', 'Swatch', 'sous-traitant', 'Deutsches', 'reprogramment', 'CBC', 'pragoises', 'sous-direction', '2.224,11', 'avant-projet', 'Bruxelles-Lambert', 'Moellemann', 'vidéosphère', 'Enron', 'vertement', '108.000', 'gonfleraient', 'Farnoux', '353', '4,50', 'Drug', 'Fortech', 'Backes', 'Dutournier', 'stand-by', '61', 'plates-formes', 'Alitalia', 'quo', 'Pairetto', 'TSW', 'washingtonien', 'dangereusement', '250.000', '9,75', 'Bogart', 'TV6', 'assurance-vie', 'Mahé', 'custom', 'A-26', 'bibliothèque-médiathèque', '2664', 'Repubblica', 'Murmann', 'Youpi', '2,50', 'Aix-les-Bains', 'Morgan-Stanley', 'grippa', 'Brégou', 'OFCE-CEPII', 'aromathérapie', 'cultivons', 'Just', 'Guardian', 'là-dessus', 'rugira', 'Tatra', 'jésuitière', 'Urwald', 'Krasucki', 'Moret', 'CRS', 'Scalbert', 'Tluszczowego', '295,70', '223450', 'Payment', 'Nivel', 'anglo-germano-française', 'Munchener', 'Andre', '2,72', 'Gutmann', '700.000', 'brutalement', 'Confartigianato', '26,5', '25,6', 'Corrand', 'UDF-CDS', 'Belhassine', 'Laffore', '62,7', 'Johnson', '410', '27,908', 'hôtel-restaurant', '11,3', 'temporairement', 'CGM', 'Channel', 'Saint-Brieuc', 'Clémenceau', 'Peylevade', '8,7', 'sous-emploi', '79', '437', 'ballotins', 'Wouts', 'Grand-Prairie', 'Neues', 'libre-service', \"chefs-d'oeuvre\", 'Babusiaux', 'Nouvelle-Zélande', 'semi-conducteurs', 'Kassbohrer', 'quatre-vingt-huit', 'Buchanan', '3,4250', '1875', 'Meurthe-et-Mosellan', '2077', 'virtuellement', '21,5', 'DOM-TOM', 'aérobic', 'OPR', 'Takeshita', 'crève-ballons', 'pipelets', '23,9', 'Saab', 'Ogilvy', 'Waigel', 'Nord-Pas-de-Calais', 'feuilletez', 'Strauss-Kahn', 'sous-équipées', 'Eltsine', 'Fréjus', 'Brown', 'CEA', 'immédiatement', 'Thames', 'sous-chef', 'Maurienne', 'Brillet', '40,8', 'Hetzel', '1996', 'Failure', '76', 'gentiment', 'montbéliardais', 'CCAS', 'DDOM', 'Sonauto', '131', 'Pellerin', 'ostensiblement', '21', '800.000', 'MMB', 'Egor', 'Finchley', 'Debauche', 'Nicholls', 'remboursèrent', 'Jean-Michel', 'VNF', 'C.', 'médicalement', 'Northrop', 'collectivement', 'jésuitard', 'hypothécables', '43,1', '1973', 'chronotachygraphe', 'automne-hiver', '47', 'Sofil', 'vidéocommunication', 'C', 'repète', 'SBF', 'recapitalisation', '20,05', 'Technip', 'Brunhes', 'moi-même', 'Palomar', 'VEB', 'Dietlin', 'Jamont', 'assurance-dommages', '9,50', 'Pittsburgh', '14,6', '22000', 'Paribas', 'Galliffet', 'ACP', 'PRI', '29.100', 'baisseraient', 'caféiers', 'Chalande', 'forge-matriçage', 'banks', 'Tupolev-134', '92.1389', 'Aquamot', 'prince-de-Galles', '131.38', '1866', 'UTA', 'Taxdisk', '12.500', 'Chalmin', 'Chaffoteaux', 'Aminu', 'barbarisants', 'alu', 'tiers-monde', 'substantiellement', 'valseurs', 'Off-Shore', '1.939,64', 'Pierson-Bragard', 'Pierluigi', 'Châlons-sur-Marne', '39,6', 'Messier-Bugatti', '1767', '44,5', 'Multivalores', 'Ostdeutscher', 'Klerk', 'FITCA', 'Manhattan', '2,3', 'indéfiniment', '16,6', 'Afflelou', 'Creusot', 'président-secrétaire', 'extensification', 'Après-midi', 'SAN-FRANCISCO', 'VOLTA-REDONDA', '2,2', 'poinçonneurs', 'TAT', 'mille-feuille', '0,7', 'Port-Salut', 'Cadam', '3,85', 'Power', '1929', 'dégarnissent', 'Barbaroux', '116', 'Meana', 'catimini', 'F.-X.', 'Magny-Cours', '50.000', 'Agassi', 'Ruymbeke', 'asphyxiait', 'Penderecki', 'Euroc', 'Oparine', 'emmenthal', 'joliment', 'Krezdorn', '68000', 'UGICT-CGT', 'SGS', 'SZPT', '36e', 'Brossault', 'Metalbox', 'Gaudillère', 'Canaries', 'Gilliam', 'Richier', 'emblée', 'Pragnères', 'AMX-30', 'Kopper', 'Nobel', 'obérés', 'Veil', 'Khiva', 'Charpin', 'Vogel', '1,74', '220', 'Faure', 'vulgairement', '355', 'Lalique', 'INPI', 'finanza', 'Santilli', '45,4', 'SNTPCT', 'cinquante-trois', 'tartarea', 'Eno', 'Birkin', 'Procopio', 'assurance-vieillesse', '24.000', 'Lugano', '57', '14,5', 'Engoncées', 'Zurich', 'élaborerait', 'heureusement', 'crevait', 'Huchon', 'Mi-avril', 'approximativement', 'rétorqueront', '1979', 'Humboldt', 'Guillemette', '2-F4', '15000', '1.440', 'Dnes', 'SADE', 'Poliet', '1.857,78', 'Ultrafertile', 'Economist', 'toyotisme', '225.000', 'Zielona-Gora', 'Coal', 'cogénération', 'Catteau', 'Jean-Charles', 'sud-coréenne', 'Timisoara', 'Hoeffel', '16,37', 'SPEP', 'enjôlements', 'AEG', 'Cedel', 'Sakik', 'Den', '1.000', 'hula-hoop', '2003', '1914-1915', '15,9', 'over', '67', '8,99', 'Immédiatement', 'time', 'Bunuel', 'Sao-Paulo', 'purement', 'Statistical', 'barilsjour', '71,3', '49,3', '3,1', 'laconiquement', 'CNAF', 'tissue', 'Galzin', 'Economiquement', 'ville-campagne', 'Cottave', 'Bundesbank', 'relativement', 'entièrement', '32,2', 'Volontairement', 'Ouest-France', 'Ki-moon', '168', 'Funkoverload', 'CFP', 'Seine-Saint-Denis', 'Burgelin', 'Communication-développement', 'BBC', '78,5', '570', 'EDF', 'Imbot', 'Strehler', 'Over', 'Desktop-IV', 'Scandédition', 'ultraminces', 'OIC', 'Moggi', '4,25', 'juridiquement', 'Weiss', 'RMO', 'Lausanne', \"'\", 'posteriori', 'DRAM', 'dix-huit', 'Army', 'Besta', 'Leblé', 'Zeller', 'Chaath', 'contrats-formation', 'j', 'Herzog', 'CIF', 'CFIUS', '471', '4,41', 'Uni-Régions', 'Jean-Pierre', 'Peng', 'dix-neuf', '138000', 'Canadian', 'GDF', 'Nec', '12.150', 'XVI', 'Lonsdale', 'Parnassus', 'bivalirudine', '13,5', 'Atlantique-nord', '9,8', 'IIIème', 'rendez-vous', 'Sendowski', 'Free', 'Ifop', 'Eaux-Chaudes', 'IUT', 'Scoupe', '134', 'Aix-en-Provence', '35.000', '8,74', 'Gaïdar', '270,20', 'Mutual', 'Tel-Aviv', 'Looses', '0,4', 'Kozyreff', 'peronnellement', '5,34', 'venture', '1,1', 'CISI', 'RFA', 'Certainement', 'Ceausescu', 'officiellement', 'Teillol', 'privatisables', '1787', 'Arc-et-Senans', 'dezaïgn', 'contrepartiste', 'Saint-Just-en-chaussée', 'ECG', '2.500', '5,9', '3.379,50', 'Roddick', 'Kontavill', '18,3', 'ATR', 'Périgord', 'Raffarin', '466', '7,3', 'face-à-face', 'Orsini', '1974-1975', '1,70', 'Collins', 'surengagés', '85', '721,8', 'Telettra', 'Arbour', '1,37', 'Chegrouche', '380000', 'YLE', 'Euroclear', 'HBO', 'Peabody', 'PIBOR', 'désaisi', 'Ducros', 'Jean-Sébastien', 'franco-italienne', 'Nord-Est', 'Nijdam', 'Meurice', '1933', '1.1', '6.800', 'Schaaff', 'Sapeur-pompier', 'au-delà', 'Entre-temps', 'entrepreunariat', 'CIC', '17,1', '35,4', 'DEFICIT', \"côte-d'ivoire\", 'traversâmes', 'vendeurs-acquéreurs-Etat', 'Gandois', 'Glashuetter', 'Swaleh', '205', 'tourette', 'Bielhe', 'Herta', 'Bandoung', '3', '0,1', 'connaitre', 'spécialement', 'sous-traitants', 'euro-obligataire', 'essentiellement', 'Fourcanade', 'Polygraph', 'UDF-PSD', '1,7', 'malle-poste', 'Devanlay', 'CEEP', 'redécollage', 'logiquement', 'Safrane', 'OCDE', '0,15', 'par-delà', '32,6', 'inappropriée', 'Saemes', '145.000', 'Lanièce', 'Grande-Bretagne', 'Hollywood', 'SNPB', '336', '29.751', '2,45', 'Brighton', 'RVI', 'Seurat', '11.000', 'après-mars', 'Jawa', 'life', '131.500', 'effectivement', 'DGI', 'grandement', 'Andersen', '226', 'AG', '19,7', 'URSS', 'Option-finance', 'Tchécoslovaquie', 'Smirnoff', '16757', '50-50', 'Lacroix', '300', 'Nord-Europe', 'Beatty', '3,4', 'Stigler', 'saisie-arrêt', 'Jean-Cyrille', 'Waïkiki', 'Metropolitan', '168,5', '5,28', 'eurocrates', 'épongés', 'Etats-nations', 'concrètement', 'mogul', '16.754', 'Tecphy', '132.000', 'Industrial', '22,5', 'Leibinger', \"Côte-d'\", 'BRS', 'Eridania', 'Blérancourt', '1981-1984', 'DEC', '1880', 'indûment', 'littéralement', 'bobologie', 'quarante-cinq', 'Fath', 'cul-de-sac', 'conjoncturiste', 'Majdanek', 'Gestner', 'IX', '75008', 'Visegrad', '10000', 'Chronopost', '49', '2011', 'acquitteront', 'Vauvilliers', 'confortablement', 'Guinée-Equatoriale', 'Distrigaz', \"citizen's\", '1988', 'Fabiani', 'descendîmes', 'chronotachygraphes', 'Yergin', 'Schumacher', '1925', '6,50', '16e', '599', 'Ufima', '96', 'Kaufhof', 'Bâtibail', 'quadruplera', 'BV', 'Agency', '87', 'Définitivement', 'UPA', 'Barucci', 'SERGUEI', 'perseverare', '272.000', 'Lefebvre', 'Zapata', '1.566', '314', 'Port-Louis', 'Koweit', '9,45', 'Donnell', '2000-5', 'coca-cola', 'interminablement', 'Talleyrand', 'Goeudevert', '1120', '0-6', 'Bongrain', 'Hypotheken', 'coiffera', 'Mlada', 'chevaucheront', 'Mercosur', '150.000', '5,38', 'Domo', 'Serratosa', 'Trente-deux', 'Régulièrement', 'Himalaya', 'Schlesinger', 'Dernièrement', '1948', 'ci-contre', 'Hezbollah', 'mégavilles', 'emploi-solidarité', '630', 'ASAT', 'Marx', 'oeils', 'notamment', 'Sabeg', 'Vanden', 'appuis-tête', '32,5', 'songeâmes', 'Clermont-Ferrand', '13.000', '956', 'disruption', 'Virgin', 'New-York', '122.2', 'Thomson-CEA', '2002-2004', '340', 'Herald', 'Fragonard', 'Réveilhac', 'ICP', '38,9', '4,19', 'franco-américain', '110', '182.000', 'Jensen', 'viscéralement', 'SOFRES', '465', 'Deux-Sèvres', '1460', 'Aritmos', 'violemment', 'Wednesday', 'Brongniart', 'CEGF', 'GPL', 'BEP', 'Paris-Bâle', 'high-tech', 'secrétaire-général', 'sous-marins', 'Smith', '3,35', '79,2', 'Bollaert', 'GRANDE-BRETAGNE', 'Jibril', 'Vernholes', 'SDIS', '8,5', 'Vogelweith', '1,86', 'soixante-huitard', 'Languedoc-Roussillon', 'perestroika', 'oscuro', 'Macmillan', 'Assad', 'Rivoli', '1985-1988', '60', '2005-2006', '34,6', 'Renon', '3,7', 'm', '70', '1.200', 'Auque', 'it', '38,5', '28000', '4,45', 'travel', 'repointent', 'physiquement', 'AFR', 'intra-', 'Dorsner', 'M', '67025', 'délégatif', '232,59', '17.905', 'Deloitte', 'HP', 'Braga', '79,5', 'Lienemann', 'Marca', 'sournoisement', '0,69', 'celui-ci', 'Tintin', 'Gergorin', 'Verilog', '39', 'Chartered', 'Cemex', 'suspendrait', 'homéostasique', 'Lyon-Libération', 'Rivkin', '112.000', '770', 'Transdev', 'Hleb', 'Austria', 'hold-up', '245.000', 'Mangin', 'Tout-Paris', 'Integrated', 'Orly-Paris', 'CMB', 'TSE', 'Köhler', '13,2', 'LK', 'Juve', 'Nordmann', 'Vuong', 'Zorro', 'OIRT', '9,52', 'Rockefeller', '293.000', 'redistributifs', 'quelques-uns', 'From', 'particulièrement', 'Wacziarg', 'cléricafard', 'DDOS', '25.000', 'Sunrise', 'France-Maîtrise', '68,3', 'Britsh', 'Costdisk', 'Filippot', 'Rochot', 'Ltd', '340.000', '19,4', 'FR3', '9.500', '2.200', 'Kohl', 'AMR', 'CIP', '505', 'compte-rendus', 'Palladio', '217,5', 'Aumonier', 'prioritairement', 'dernièrement', 'Cosby', 'Tamatave', '1942', 'Bergé', 'debt', '64.566', '0,18', '12,7', 'society', 'Delaporte', '3.040', 'liguait', 'Tietmeyer', '1732', 'Drome', 'maine-anjou', 'tout-tourisme', '56000', 'Diderot', 'Orlybus', 'fabricants-grossistes', 'Montek', 'Wadi', '215,10', 'Eurosport', 'Volkswagen', 'Delacre', '19000', 'huasipongo', 'Bouissou', 'Tissot', 'sureffectifs', 'First', 'Popi', '717,6', '10.500', 'agiotateurs', ';', 'Peiffer', 'BD', 'FFD', '37', '9,2', 'italo-suisse', '3900', 'réindustrialiser', 'Lanini', 'Brothers', '1,4', '1802', '-ci', '75', 'Monts-Maudits', 'Valbuena', '10,7', 'ARHV', 'Pierre-Luc', 'Langoni', 'Times', 'sapeurs-pompiers', 'individuellement', 'AGRR', 'mi-journée', 'Croix-Rouge', 'Eins', 'Pèna-Blanca', '5.000', 'Silla', '1949', '598', 'vigoureusement', 'Roden', 'Zenit', 'BTF', 'Probablement', 'Pony', 'rarement', 'biocarburants', 'Marie-Marvingt', 'Klippan', 'gravement', 'Nederkoorn', '9,28', '86', 'Projects', 'ultramarines', 'Fujitsu', '21-22', 'Clionnaise', '14.500', 'bonus-malus', 'AGF', 'quart-monde', 'Mézenc', 'modestement', 'UDC', 'CTCOE', 'doucement', 'Vingt-six', 'Unilever', 'bethoncourtoise', 'vidéocommunications', 'North-Field', 'OP', 'fr', '2012', '1493', 'GTI', 'nunuche', '1.550', 'radio-télévision', 'Workers', '23.700', 'Mayer', 'San-Joseph', '637', '1948-1953', 'Berliner', 'Miami', '12,6', 'centre-gauche', '1969', 'sportswear', '184', 'Usiminas', 'Balandier', '717,5', 'SACER', 'Crabioules', 'Ernewein', 'Kross', 'Graficas', 'indo-pakistanais', '7,8', '591', '32,9', '1954', 'Andiamo', 'initialement', 'impécunieuse', 'Thomson-Brandt', 'Certano', 'Grand-Charmont', '1,25', 'Sesa', '30.000', 'Interbank', '415', 'Kutuzov', 'eurowarrants', 'Volski', '1921', 'Au-delà', 'quarante-deux', 'câblo-opérateurs', 'school', 'Schneider', 'Economic', '11,6', '1871', 'Chugbo', 'hors-médias', 'October', '31', 'AGA', 'Caillaou', 'Bodel', 'Manufacturers', 'Deutschland', 'Bankers', '1995', '99,8', '108.500', '2023', 'VVER-440', 'Queens', 'pangermanique', 'Cousances', '162', '33,7', 'bénévolement', 'respectivement', '5', 'ultérieurement', 'Peut-être', 'Lajoinie', \"Moody's\", 'devils', '775', 'Laffont-Leenhard', '154.000', 'Castille', '0,21', 'CNT', 'soudainement', 'hutchinzia', 'Lockheed', 'Nestlé', '58', 'rééchelonnés', 'drois', 'Arun', 'chambériens', '\"', 'Sampermans', 'Politics', '21,40', 'Bagnères-de-Luchon', '16,353', 'ptt', '5,75', 'robusta', 'Editor', 'navals', 'Shanghai', '13,3', 'SFIO', '594,135', 'anglo-américanisme', 'FISSIER', 'porte-parole', 'Lesur', 'Robert-Schuman', 'Asie-Pacifique', 'sacro-sainte', 'City', 'CAUE', 'Walibi', 'Démilac', 'arrière-pays', '9,10', '39,7', 'Baker', 'Var-Matin', 'PLS', '13,55', '1988-1991', '1.800', 'Jacky', 'certainement', 'Aulagnon', '350.000', '1,17', 'Delannay', 'brièvement', 'IRC', 'Glinn', 'voitures-ventouses', '46,5', 'Dupuy', 'attrape-tout', '6,57', 'Sopad', 'Doutriaux', 'Jean-Luc', 'SEITA', 'CPI', '20,5', 'Coumélie', 'SOFIPA', '14,7', '62,5', 'Wirtschaft', 'largement', 'surqualifiés', 'Nederlanden', '1,15', 'Ileau', '78,6', 'percutanée', '51', 'Ostrava', 'accroitre', 'obstinément', 'D66', 'Coste-cerdan', 'quittâmes', 'Commergnat', 'Suwaidiyah', '1857', 'Grindelwald', 'Nazarbaïev', 'scoubidous', 'Taine', 'Taipeh', 'Hénin', '115.000', 'pétrochimiques', 'Mirage-2000', 'ZDF', '1er', 'Ménez', 'Kléber', 'OPV', 'lestement', 'Rivasseau', 'MFI', 'Mezzogiorno', 'FMC', '113', 'EMBLÉE', 'Foremost', 'L.', 'under-class', 'Juergen', 'R.-G.', 'moitié-moitié', 'Ericsson', '184,1', '232', '9,83', 'Chalvon', 'Sochaux-Montbéliard', 'off-shore', 'Edzard', 'Logata', 'KS', '13,6', 'IIIe', 'Shore', 'Escande', 'corp.', 'Nobrega', 'OTAN', 'Commerzbank', 'Mobis', '277.000', 'Barisienne', 'hyperinflation', '66,4', 'Montreux-Vieux', '47,6', 'Royaume-Uni', 'Comtet', 'grand-messe', 'Bergons', 'Quilès', 'ABN', 'pâtiraient', 'Doctored', 'Larosière', '3,90', 'Trasgo', 'Phillips', 'Insensiblement', 'concassant', 'Saint-Sauveur', 'sous-sol', 'Co.', 'Mozart', 'sourdaient', 'sud-ouest', '2006-08-07', 'Sligos', 'next', '336e', '2266106333', 'renchérissait', '43,9', 'Overseas', 'Alliot-Marie', 'Milred', 'OAT', '3,50', 'maritalement', 'New-Hampshire', 'Séguin', '188', 'Rubicon', 'Legris', 'Ltd.', 'DATAR', 'A.', '100,68', '34,06', 'Soixante-quinze', 'Michel-Edouard', 'dégraissages', '49,7', 'December', 'France-Télécom', '4.500', '2.914571.14.3', '1400', 'pratiquement', 'sûrement', 'PAYS-BAS', 'Wagram', '1920', 'HSBC', 'essaie', 'tout-publicité', 'Münster', 'Viannay', '56400', 'Interland', 'Anciennement', '23,5', 'anti-', 'Brugières', 'Vergèze', 'communément', 'CSMF', 'médiatiquement', '3,37', 'Abdessalam', '0,57', 'Sagem', 'Sontag', '52.000', 'Kuweit', '24,2', 'Usinor-Sacilor', 'ruralité', 'FFP', 'Monory', '31,6', 'franco-britannique', 'rétrocommissions', 'Rambaut', 'Perthois', '190.000', 'XIIe', 'soi-même', 'Marrel', 'RPR', 'ISSC', 'CNJA', 'GUB', '800', 'contre-publicité', '1.700', 'Satam', '24', 'Sheik', 'Hata', 'Celsius', 'Pierrejean', '1963-1967', 'recède', '220.000', 'rebaisser', 'Ménétrez', '2.765', 'Lafont', 'Veszprem', 'Honda', 'parity', 'Milosevic', 'Torras', 'cuisinistes', 'ÉTATS-UNIS', 'VERNHOLES', 'Marie-Claude', 'avantageusement', 'eaux-Dumez', 'ensuivraient', 'conspirationniste', 'Vedial', 'Mariet', 'Indochine-France', 'Singapore', 'empilaient', 'récessionniste', 'Pierre-I', 'épouvantait', 'Galland', 'Goldenberg', 'Nérac', 'Rémy-Cointreau', '1944', 'passionnément', 'Héricourt', 'IG', 'Offenstadt', 'BP', 'trente-huit', 'Courcelles-lès-Montbéliard', 'Grosrichard', 'agréablement', '283', 'Papon', 'Pierlot', 'anticiperaient', 'Iraq', 'Samsung', 'Naville', 'Scuderia', '46', 'Cetelem', 'Enfants-magazine', 'Mont-Perdu', 'Barrère-Maurisson', 'exposition-vente', '26,1', 'tee-shirts', '18,8', 'soixante-six', 'CGE', 'Wahl', 'GAO', '110.000', 'Repsol', 'libre-échange', '155', 'prêt-relais', 'Landeau', '2.600', '1924', '82', 'Schuller-Maréchal', '134.804', 'herbagers', 'Gattegno', 'Ratier-Figeac', 'député-maire', 'statu', 'Paris-Montsouris', 'TF', 'E', 'Lebanese', 'Zorn', '13,8', '1985', 'Lanka', 'Gysi', '1.326', '171,5', '9,442', 'Rhazza', 'MR', 'Floch-Prigent', 'uniquement', 'Guigou', 'Fanchon', '1843', '2-1', 'spoliatoire', '2.700.000', '76.000', '1666', 'Brust', 'ITCF', 'Nyrup', 'considérablement', 'Drian', \"main-d'oeuvre\", 'Effectivement', 'minimum-vieillesse', 'Vitez', 'Hajj', 'concèderont', 'significativement', 'PSA-Peugeot-Citroën', 'légèrement', 'OPA', 'clouter', 'latifundisme', 'Mexès', 'Nissan', 'Swatch-mobile', 'n88-70', '273', 'Friesland', 'stent', 'BHF', 'Brüggemann', 'Kravtchouk', 'Fauvet', 'capital-confiance', 'Drei', 'Sud-Est', 'chèques-restaurant', 'Elfi', 'Midy', 'emprunt-phare', '625', 'SNVC', 'CNTS', 'Sprewell', '56.000', ':', 'Mismanagement', '14,8', '37,5', 'préfèreront', 'sous-évalué', '435,7', 'apparaitre', 'Jonh', 'Petris', 'Mauroy', '74', 'massivement', '2221100824', 'voulez', 'minifundisme', 'VIAL', 'Guyau', 'CEA-Industrie', 'moyen-courrier', 'Brungard', 'VW', 'XVII', 'COLLECTIVEMENT', 'CNRS', 'ISMEA', 'Essen', '55000', 'Pierre-Etienne', 'self-made-men', 'Télécâble', 'Wehbé', 'assurance-chômage', '362', 'Gross', '14.000', 'capo', '88', 'céréaliculture', '1830', 'Bouréquat', 'Keynes', 'cadeaux-souvenirs', 'BGAG', 'EDS', 'Triboulottes', 'Hilmar', 'évènements', '246', '98,6', 'Djeddah', \"M'Vila\", 'Toulouse-Paris', 'Confederation', 'CS', 'ITV', 'Letourneur', 'Mazda', 'Cornéa', '1500', 'R.19', '°C', 'Saint-Hubert', 'Jean-Jacques', 'Exor-Perrier', 'Martignac', 'Mobil', '26,4', 'loi-programme', 'eurofrancs', 'latifundio', 'scared', 'Masson', 'V', '3.168,83', 'Mc.', '1,44', 'Baring', 'IIb/IIIa', 'Sécuripost', 'séparément', 'Antenne2-FR3', 'D', 'commodément', '8e', 'semi-', '3,3', '330', 'avant-première', 'Epicure', 'machines-outils', 'Mehta', 'N.-D.', '176', 'Mesrahi', 'Jean-Louis', 'Up', 'Sportkanal', 'verticalement', 'Saint-Dié', '24,9', 'épargne-logement', 'protègé', 'Télérama', 'enseignants-chercheurs', 'Bulls', 'Centro', 'Pelège', 'redise', 'vainement', 'quant', 'Japan', 'DDOF', 'débureaucratiser', '4,21', 'illicitement', 'PII', 'Globalement', 'artificiellement', 'Cerro', 'Nord-matin', '19.200', 'EDA', 'Groupama', '30,4', 'Lazio', 'Laisné', 'Hermier', 'Estom', '550,3', 'EMNID', 'qua', 'monoproductions', 'Jean-François', 'Cournot', '3750', 'Obama', '1741', 'Biba', 'ITC', 'industrialisons', '9,7', '255', 'Centre-Ouest', 'chauffeur-routier', 'étranglez', 'Floch', 'légumiers', 'Valenciana', 'New', 'Szczecin', '8,6', 'malthusienne', '3.500', 'Cousteau', '389', 'Collor', 'Pointe-des-Galets', 'Donnelley', 'Perraudin', '1586', 'autoentretenue', 'Zelnik', '2.720.000', 'Blatter', '8.000', 'Laigneau', 'Quantas', 'Fisher', 'Karlsruhe', 'Quid', '1976', 'Edbro', 'Jean-Maurice', 'habilement', 'RTL', 'Keating', 'profondément', 'Kuala-Lumpur', 'Toubon', '60.680', 'indirectement', 'Simmermann', 'Sephia', 'Valat', 'auto-', 'Bundesrat', '12-13', '335,6', 'paient', 'ci-dessous', '2005', '°c', '2', 'Moravie', 'Haute-Loire', 'Annan', 'Hill', 'Pollena', 'Electric', 'revoyaient', '1923', 'Aix-la-Chapelle', 'cinéma-télévision', '672', 'conformément', 'Law', 'Giancardo', '90.870', 'CD-ROM', 'Matouk', 'socio-culturel', 'SNECMA', 'Lehman', 'parcourons', 'CEA-Industries', 'pollueur-payeur', 'COBAS', '2-7113-0389-6', 'Beach', 'Rizzo', 'Nécessairement', 'Rhône-Poulenc', 'memory', 'Monde-Economie', 'diner', '850', 'Descamps', 'Independant', 'bleu-verte', 'Ramond', 'Naf-Naf', 'SME', '7.879', '7.958', 'Claude-Annick', '3.035', '2,1', 'privatisable', 'sus-décalage', 'Pennacchioni', '61,7', '85,6', 'damn', 'Giscard', 'Maxéville', 'AC', '101,5', 'traditionnellement', 'Group', 'quarante-trois', 'Feb', 'aisément', '109,70', 'Fiszer', 'Pronasol', 'garde-fous', 'experts-comptables', 'Hedin', 'convenablement', 'anglo-néerlandais', '0,3', 'parfaitement', '49,32', '75.900', 'Fains-Véel', 'Lévi', 'raffermissait', 'H/C/562', '560', 'néo-', '77', 'Nasri', '1.883,64', 'ouistiti-sexe', '277', 'flattai', 'INSEE', '314000', 'Mercadé', 'Rétrospectivement', 'Loo', '42,95', 'contre-courant', 'BMW', 'University', 'Alphandery', 'structurellement', 'FNB', 'G7', 'trop-plein', 'Brittan', 'FSM', 'Korkos', 'Taïpeh', 'conjoncturistes', 'Méziré', 'Carnaud', 'thatchérien', 'Recapitalisation', 'statistiquement', 'Energie-Sud', '95,4', 'Welle', 'B.', '235', 'Abidjan', 'Pocket', 'billettiste', 'Marchelli', 'West', 'GLNF', 'Compania', 'Crouzel', 'Smouha', 'Devaquet', '5,52', 'UNIM', 'Louron', '103,28', 'Yohann', 'longs-métrages', 'paieront', 'grands-parents', 'Ash-Sham', 'Independent', 'Eurofed', 'Ghigonis', 'mollétiste', 'Pierre-François', 'Investments', '8.500', 'Merlin-Gérin', 'Eastern', 'Villepin', 'magyare', '65.340', 'éléments-clés', 'est-à-dire', '959,7', 'Port-Harcourt', 'hip-hop', 'nommément', 'hyper-', '199.920', 'logements-passerelles', 'Eurotunnel', '148', 'Nogent-sur-Marne', 'très-considérable', 'Ruset', 'Lagaffe', '1,5', '388', 'Damoclès', 'Hyundai', 'Jean-Noël', 'traumatisent', 'DGCCRF', 'Welt', 'voulûmes', 'désinvestissements', 'Reboul', 'Ameen', 'Niedenhoff', 'Debray', '250', '135', '18', 'ACEA', '21.000', '1ère', 'Dan-Air', 'joyeusement', 'Sajust', 'Shteynberg', 'Los', 'réajustant', 'île-Longue', 'Marie-Noëlle', '1747', 'apparemment', 'Frydman', 'assurément', 'Child', 'Plessey', 'apple-pie', 'Babangida', '123,75', 'Gear', 'Act', 'CPR', '5,53', '1852', 'Cadbury', 'Jacques-Bailleurs', 'Szentes', 'Algaro', 'anglo-saxonne', 'Enagas', 'Geneval', 'anonymement', 'Montedison', 'CSF', 'Gaîté-lyrique', 'Modrow', '5.760', 'Zenith', 'take', 'méditera', 'interférométrique', 'Frico', 'Dumez', 'dession', 'brocca', 'house', 'AM', 'RM', '1640', 'sévèrement', '2.660', 'Rupo', '533', '1946', '413,5', 'Schumpeter', 'modérément', 'Morin', 'BT', 'Nocivelli', 'Gavalda', '289', 'Ciba-Geigy', 'SDP', 'maitrisée', 'CIM', 'Gitaï', 'AVU', 'Duménil-Leblé', '1920-1921', 'Séraphine', 'Espingo', 'États-UNIS', 'Alsac', 'honnêtement', '29,7', 'SNPMI', 'Harley-Davidson', '606', '169', 'FIFA', '18-24', 'résolument', 'Thapar', 'décloisonnent', 'CGC', 'Maus', '36.000', 'Régniere', 'pélagiques', 'Life', 'Maghreb', 'Zouhri', '108', 'esquivaient', 'Aachener', '7,6', 'PAP', '1,3885', '400', '27', '1764', 'Portici', 'tripalium', 'généralement', 'gratuitement', 'UAW', 'terratenientes', 'barisienne', 'quatre-vingt-quinze', 'Libyan', 'SAE-Fougerolle', 'fimes', 'systèmatiquement', '5-6', 'OFP', 'rocardien', 'Joxe', 'contenterions', 'Esper', 'Rockwell', 'Guyomard', 'Pei', 'Bradstreet', 'vingt-quatre', 'Heyris', '488.000', 'Nordstern', 'Bar-le-Duc', 'désinflationnistes', 'G8', 'Elf-Atochem', 'Agnelli', '1900', 'contractualisés', 'Yacht-club', 'Iavlinski', 'Vis-à-vis', 'télépéage', '110,44', 'Kantor', 'Sorbonne', 'J.-F.-Kennedy', 'reconductions', 'cf.', '2.303', 'IW', '73,9', '6.500', '198', 'laissâmes', '1893', '1,10', 'JAC', 'Horst-Udo', '3,3650', 'circonspectes', 'évanouissaient', 'raccordables', 'Great', 'Arcadi', '64', 'Hara-kiri', 'Schroders', 'euro-emprunts', 'indifféremment', '2500', 'soixante-neuf', 'Philips', 'XIXe', 'Moscow', 'graduellement', '2,08', 'Cassani', 'Arouja', 'Jacquin', 'Steinkühler', 'banlieusardisation', 'Maxévillois', '2.572.968.000', '2.032.100', \"pin's\", 'Cacheux', 'prétendument', '1,6', '4,8', '1785', 'CIRI', 'how', 'Viêtnam', 'Arlington', 'volta-redonda', 'partiellement', 'démutualisation', '1450', '48e', '271', '1.831', 'surteint', 'Gallimard', '36,30', '2.211', '1,762', '674', 'TBB', '332', '0,9', 'attentivement', '200000', 'recyclables', 'Company', '9,94', 'Mulhouse-Montbéliard', 'Bertolt', 'Baloutchistan', 'CUT', 'Bourg-Saint-Maurice', 'Syrota', '117.600', '504', '142', 'israélo-arabe', 'Kwan', 'budgéter', 'Milken', 'GSA', '3413,24', '132.100', 'Gillet', 'way', '260', 'Raissa', '3-0', 'techniquement', 'Cinzano', '39.155', '73.000', 'sportivo-', 'pressurées', '5.756,14', 'plus-value', 'Clergerie', 'leitender', 'taiwanais', 'Jaeggi', 'politico-financiers', 'Lehalle', 'Seiko', 'Cogéma', 'Pardini', 'Cerus', \"O'Kane\", '310', 'storming', 'SOLEUVRE', 'D.', 'farouchement', 'Twingo', 'Egypte', 'Geneimwirtschaft', 'Givray', 'globalement', 'SAP', 'taiwanaise', 'RBMK', 'Agreement', 'toute-puissance', '3,886', 'Corresp', 'stricto', 'G.P.S', 'Bourgoin-Jallieu', 'Pinault-Printemps', 'Carnival', 'OPZZ', 'MM.', '2,4', 'sur-investissement', 'Outre-Rhin', 'lave-linge', '(...)', '0,2', 'sous-traitée', 'Houcine', '4,4', 'Successivement', '4,900', 'Mezrahi', 'DASA', 'from', 'Casadevall', 'PSA-Citroën', 'Châtenois-les-Forges', 'Servan', 'Fitzgerald', 'Tanger', 'Bf.G', '0,06', 'mi-', '10,2', '14,3', '5,12', 'Dominati', 'Conformément', '2.982', 'I.', 'PVD', 'équilibrera', 'Béthune', 'Singh', 'turn-over', 'A-6', '985,4', 'Saint-Exupéry', 'Lafon', 'pré-', '3,4070', '1,75', 'Duisbourg', '1961', '97.000', '8,20', 'rogatoire', 'Piraben', 'ICL', 'Huxley', '80', 'Messenger', 'N1', 'coroné', 'PvdA', 'Moyen-Orient', 'SMIC', 'Naples', 'longeons', 'Fottorino', 'new-yorkais', 'Yoshitomi', 'Transatlantic', '3,80', 'photographer', 'Aeritalia', 'Teulade', 'del', 'Martin-Marietta', 'ras-le-bol', 'Zealand', 'franco-allemand', 'CG2A', '9.000', 'Thatcher', 'avoisineraient', 'Am', 'Pinault', 'maxima', 'Azeglio', 'quarante-quatre', 'franc-maçon', 'Vargas', 'après-', 'Katowice', 'BANUS', 'sciemment', 'Electroménager', 'usuellement', '53,5', '47.800', 'reparait', 'CECA', 'MPG', 'Niel', 'Sainte-Honorine', 'Wonder', 'Chambéry', '768.1', 'UER', '1980', '.', 'O.S.', 'Midwest', '266', '87.125', 'Dauzier', 'IV', 'Grand-Duché', 'Ponviane', 'Durement', 'Polska', 'PSA', 'Notamment', 'Pernod-Ricard', '92.538', '712,20', 'SPD', 'SGS-Thomson', 'sous-épargne', 'Juppé', 'terriblement', 'NAP', 'Jean-Marc', 'Huy', '10.000', 'Upomami', 'Grum', 'sud-africaine', '13,4', 'Monnet', 'chrétien-démocrate', 'Hannover', '703.000', '2.400', 'Jaans', 'congé-maladie', 'Poitou-Charentes', '221', '94', '9,5', 'Matthey', 'Grabis', 'Hoover', 'Tretorn', 'pub-info', 'par-dessus', 'chiffre-clé', 'Washington-New-York', '75ème', 'Eyadema', '160', 'Liberto', 'Hayek', '1645', 'Tardieu', '0,80', 'Weinberg', 'Falcon-50', 'San-Antonio', 'Fernsehfunk', 'markka', 'Soros', 'fahr.', '24,5', '0,96', '30,30', 'Grandier', '15,7', 'Turner', 'SPC', '3,9', 'GP', 'fréquemment', 'Rockseller', 'visitable', 'M3', '5,3', 'OPFI', 'Sutton', '2020', 'SAINT-DENIS', 'FNSEA-CNJA', 'Libron', 'Triange', 'Scali', 'Barbieux', 'Médiaspouvoirs', '66,7', 'sobrement', 'XIX', 'Orléans', 'cahotantes', 'basisme', 'Zuccarelli', 'Reebok', 'débarquons', 'Ferman', 'Baudy', \"Lloyd's\", 'simultanément', 'franco-italien', 'Pompidou', '0,6', 'Telescope', 'Fès', 'Talancé', '70,9', 'UEM', 'Partex', '3,13', 'essuie-tout', 'déjeunâmes', 'Carambar', '10,3', 'reconciliation', 'Mourmelon', 'Sviluppo', '1,80', 'MNR', 'Telephone', '42,1', 'Trèves', 'UNIFA', 'hi-fi', 'Tupolev', 'DSF', '192,10', '717', 'Mermaz', 'Dassault-Electronique', '3.400', 'Isoète', 'vététistes', '143', 'Brutalement', '32.900', '969', 'HLM', 'Pharmaceuticals', '2,25', 'JDD', 'Lamartine', 'Riberolles', 'valons', 'Platinum', '1833', 'Dominguez', 'surréaction', '3,2', 'Lebègue', '4.200', 'Sapeurs-pompiers', 'Bouxières-aux-Dames', 'Etonic', 'Kosovo', 'OJD', 'intouchées', 'abusivement', 'Carnegie', '68.000', 'Dupart', 'maître-nageur', 'Publicis', 'sous-performé', 'Time', 'STC', '-6', 'Sinn', 'random', 'objectivement', 'avivés', 'mulard', 'withdraws', 'CNN', 'Skov', '0', '10', 'regagnez', '7,50', '17,4', '42.000', 'Guestier', 'triréacteur', 'Epéda-Bertrand-Faure', 'implacablement', 'Toluca', 'Publisher', 'Kauffmann', 'Haulage', '1904', 'monumens', '2.184,79', '76,4', '28,6', 'définitivement', 'Fronta', 'maritimité', 'antibloquants', '1573', 'Duport', '168,3', 'cinquante-cinq', 'Flandres', 'caméramen', 'Cogema', 'préventivement', 'FNMA', '2268052621', 'cours-plancher', '5.500', 'Nikkei', 'étalon-boeuf', 'Zygomusic', 'VP-Schickedanz', 'elle-même', '1903', 'Kombou', 'court-circuite', 'sit-in', 'Puy-Guillaume', 'Sopalin', 'Pérol', 'CDU', 'RCCL', '62.000', 'avant-veille', '5,4', 'probablement', 'Waldeck-Rousseau', 'Vignemale', 'VMTV', 'Balmary', 'vivement', 'Ricard', '2,90', 'carrément', '750.000', '1993-1997', 'MULLER', 'Dufournet', 'Pandraud', 'Elmalek', '1936', 'Navarre', 'Lazard', 'Ritz', 'courageusement', '5629148', '17.000', 'Akishima', 'GEB', '2.934.000', 'Blancard', 'ré-inhumé', 'Kinston', 'KLM', 'Heseltine', '1834', 'Soisson', 'Baudesson', 'casse-tête', 'décemment', '11e', 'secteurs-clés', 'trois-quarts', 'CERC', 'Ouattara', 'anglo-saxons', 'immobilier-logement', '56', '342', '2,8', 'Appâtées', 'Camargue', '12,1', 'Duperier', '52', 'Uhrenbetrieb', 'Framatome', 'pudiquement', 'Jetbus', 'Sélectour', 'Evidemment', 'eurobag', 'Duménil', 'Development', '213', 'Médiamétrie', '4700', 'sicomi', 'Singularisons', 'Goethe', 'Vevey', 'Anvers', 'A1', '4,9', 'Saxe-Anhalt', 'livre-fleuve', 'VVER', 'Lamotte', 'Midland', 'Alduy', 'Bloch-lainé', 'Alcatel-Alsthom', \"Ma'Pub\", 'Perier', '112', 'Teochew', 'ARD', 'correctement', 'Maine-et-Loire', '301', 'pétillé', 'différemment', 'etc', 'hautement', 'etc.', 'Koehler', 'GPT', 'Kursaal', 'ARRCO', 'allocation-chômage', 'Pioneer', 'monde-campus', 'sociaux-démocrates', 'Duramé', '1962', 'Evalué', 'Italcementi', 'F.', 'Portillo', 'irrémédiablement', 'expovente', '55,4', 'vice-', '1902', '2,95', 'Fondiaria', 'formellement', '200.000', 'officieusement', 'Toledo', 'Goldwyn', 'Pétriat', 'Fère', 'Five', 'arithmétiquement', '1970', 'franchissez', 'Argentina', 'façonniers', '11ème', '1,8', 'A2-FR3', \"Automobil'\", 'Marionnaux', 'Mellick', 'Quarante-huit', 'Desmure', 'Sportage', 'Cushionning', 'Prado', 'Girard', '193,6', 'Puy-de-dôme', '5,7', 'souscrira', 'Farrad', 'Bergougnoux', 'Vendôme', 'Saint-Nazaire', 'Liquefaction', 'Radio-Nostalgie', 'Hislaire', 'AVS', 'absolument', 'calendaire', 'franco-suédoise', 'Victoire-Colonia', 'rupiahs', 'co-entreprise', 'Saint-Gobain', '27.000', 'Weekly', 'world', 'Myers', 'Biafra', 'hypocritement', '7719', 'Disneyland', 'Lotharingie', 'défiscalise', '6,8', 'explicitement', 'motor', '144', 'Saint-Clément', 'Legrand', 'TIPP', 'anglo-néerlandaise', 'Sanson', '7.400', 'Soleuvre', 'UBS', 'actions-maison', '230', '26,8', 'Przemyslu', 'Dautry', 'be-bop', 'GmbH', '28,8', '19,1', 'TD', 'jaspinent', 'Euronews', 'récemment', 'Perez', 'septicisme', 'Copperman', 'Bonbel', '93', 'Sirven', '3,2972', '16,33', 'Orlyval', '180,6', '6,3', 'Allensbach', 'Jeancourt-Galignani', '1.225', 'IFTIM', 'plurifonctionnalité', 'Newspaper', 'Clinvest', 'Nordez', 'monospaces', '2234052785', 'Colonia', '1800', 'mots-clefs', 'arrière-grand-mère', '4,2', 'incomplètement', 'Lesieur', 'rapidement', 'Auto-Journal', '1,5180', 'Angeles', '1700', 'FN', 'Baur', '29,519', 'cambistes', '36.420', 'Croix-rouge', 'Diberder', '31,5', '4600', 'pence', 'année-record', 'Luchon', 'GILLIAM', 'Gorbatchev', 'Vaclav', 'ressources-richesses', '42,3', 'Tywersus', '[', 'Gosselin', '209', 'Sodero', '732', 'passe-passe', 'adds', 'Nayirah', '675.000', '26,6', 'décapitalisation', '95,8', 'Kyubin', 'Juventus', '36-32', 'Sibille', 'géo-politico-stratégiques', '552.000', 'Marchianni', 'NV', '92.1269', 'reclasssement', '13,7', 'pull-over', 'R', 'Laffont', 'Shakespeare', 'provisionnant', 'moribondes', 'Dewavrin', 'aucunement', 'Elkabbach', 'Deverloy', 'Baïkonour', 'Lobry', 'haridelles', 'majoritairement', 'week-ends', 'Street', 'laissés-pour-compte', 'VVD', 'Sogénor', '5.200', '5.720', 'dizaïgn', '578', 'là-bas', 'Chivasso', 'post-', 'spontanément', '422', 'Evian', 'suréquipées', 'Solana', 'Dusseldorf', 'Joumblatt', 'néocapitalisme', 'Fievet', '820', '3e', '46e', 'Maly', 'stockeurs', 'Tête-Défense', 'clopidogrel', 'Cie', 'Pache', '7,63', 'Dumont', 'Figaro-Economie', 'branle-bas', 'dix-mille', 'Born', 'stylique', '1.900', '0,60', 'Japy', 'Salta', '111,12', 'mutuellement', '210.000', 'Dracula', 'European', 'Fellous', 'Scitex', 'votos', 'guigner', 'SIMSET', 'AERIENS', 'Rizzoli', 'Alfonsin', '3,65', 'Soditik', 'Yazid', 'Wagner', '997', 'TEPCO', '91-92', 'contre-attaque', 'croisance', 'Equipment', '288', 'Newsweek', 'Marboré', '329', '8,77', 'CIFUS', 'Dutoya', 'Benedetti', 'chrétiens-démocrates', 'camions-citernes', 'LMG', 'Meiji', '31.000', '1958', 'QPL', 'Latrell', '70e', '2006-08-08', 'Evoquant', 'déqualification-surqualification', 'AGIRC', 'Québecor', 'toujours-plus', 'semi-remorque', 'Teissier', 'Bening', 'petits-enfants', 'Nihon', 'impunément', 'Picade', 'Smoke', 'régulièrement', 'Milwaukee', 'Oliveira', 'Schuller', '85.000', 'Yonnne', '29.910', '32,10', 'Pariente', 'Reveglio', 'Duboucher', 'CCSDN', 'Barrat', 'Wever', '36,1', 'Kothari', '285', 'week-end', 'euro-américain', 'Marie-Agnès', '2971000', 'Muller', 'and', '1.416', 'Gian-franco', '81,8', 'Chicago', 'Allemange', '615', 'outre-Rhin', 'Trente-huit', '11,4', 'top-down', 'TSH', '420', 'Barrau', 'F-70', 'Bundestag', 'parallèlement', 'TVS', 'mea', 'Montbéliard', 'durillons', '90', 'sacro-saintes', 'Telecommunications', 'Kondratieff', 'immolons', 'Wirschaft', 'unanimiste', '108,20', 'Miyazawa', 'Vôge', '23/12/2006', 'résiduaire', 'Chaumet', 'Spirou', '1,96', 'distribuions', '1894', 'Guison', 'totalement', '2000', 'Tchernobyl', 'Mineau', 'cogitent', 'Quillery', 'APS', 'consacrerons', 'Wojcik', 'méfiée', '146000', 'qualité-prix', 'Cupiaga', '7500', 'Becerril', 'fondamentalement', 'temporis', '750000', 'âprement', '11,5', 'Cement', 'QG', 'Jean-Daniel', 'Darralde', 'Théobald', '202.348', '1.441', \"dizaï'neur\", '240.000', 'Comau', 'lourdement', 'Meurthe-et-Moselle', 'questions-clés', '1978', 'Argos', 'Compu', 'Mérieux', 'Denfert-Rochereau', 'Ouest-allemands', '9,3', 'AELE', '38', 'australo-argentin', 'Sportfernsehen', 'Thyssac', 'Shimbun', 'sous-sols', '1988-1990', 'Montbrial', '1,54', 'assurance-maladie', 'Watanabe', '10,5', 'Par-delà', '38.503', 'Flandre', 'soi-disant', 'Fachinetti', 'concernante', 'curieusement', 'remarquablement', 'Psychologiquement', 'GTE', 'France3', '89', 'diabolicum', 'simplement', 'Tarcy', 'Tourmon', 'Andrevon', 'cléricanaille', 'Maniacs', 'Peugeot-Citroën', 'rauco', '1652', 'dissuation', '40,95', 'Kleinwort', 'Tuffier', 'Elysée', 'Touraine', 'Wall', 'mystérieusement', '118', 'désindustrialise', 'Maurice-Henry', 'Jacomet', 'Cryns', '4,6', 'TGV-Est', '1979-1981', 'vingt-huit', '1,20', 'singulièrement', 'Neddy', 'Langlois', 'Westcountry', 'Vascons', '5,70', 'Tim.', 'Professionnellement', 'Chekland', '557,6', '56,4', 'Citizens', 'Grenet', 'Airlines', 'Saint-Honoré', 'IFOP', 'Granada', '45', 'Szpiner', '66.000', 'époumonés', 'reformatage', 'Cooke', '695', 'précoloniale', '8,71', '80.000', '1.360', '7', '1952', 'FIEHP', '5,19', 'Saint-Dizier', 'Hosneld', 'casus', 'Cointreau', 'outre-mer', 'désorganiserait', 'Vatelot', '14,4', 'itinéraire', 'PLANTU', '2,59', '59,9', 'rapportai', '1979-1986', 'Lafayette', 'Rougier', '1840', '1634', 'contre-partie', 'Jankowski', 'Guitton', 'Corriere', 'Onché', '450', 'Houston', '1982-1983', 'Sachs', 'junk', 'Atlantique-2', 'Pechiney', 'VEV', '22.964', 'CAF', 'CGA', '10,1', 'PEA', 'publiquement', '1992-1995', 'Elysées', 'Clearstream', 'Emmaüs', '90.000', 'Schlüter', 'Chapat', 'Pasqua', '70000', 'Aktyubinsk', 'Fillioud', 'Bertez', 'guingois', 'Ecco', '16,1', 'Denoël', 'Hammoutène', '847', 'Caillaud', 'corresp.', 'RMC', 'Sandomierz', 'Bérégovoy', 'Promifi', 'sous-employés', 'omnipuissante', 'Miot', 'Simplement', '354300', '16,75', 'frauduleusement', 'incontestablement', 'Matra-Hachette', 'Daubeny', 'arabis', '5.630', '350', 'fraudent', 'Pizzetti', 'recelé', 'Flégny', '40,2', '317', 'Jean-Moulin', 'SCOA', 'Zentih', 'Centrest', 'UCPA', 'inéluctablement', 'sud-africain', 'Cans', '840', 'SNA', 'refroidissaient', 'Paris-Nantes', 'ONG', '7-8', 'Canaan', '6', '2812500', 'Kogaz', 'St-Sauveur', 'Géorgique', 'Favier', 'FAO', 'OPCVM', 'congratulaient', '44.485', 'Mazeaud', 'grognes', 'pronostiquait', 'grosso', 'obligatoirement', 'successivement', '10,6', 'monte-matériaux', 'SMH', 'SCOR', 'Castaneda', 'Baudéan', '1990', 'Sorcy-Saint-Martin', 'J.-P.', '8,3', 'gagne-pain', 'Hauts-de-Seine', '926', 'Unzen', 'Fressoz', '605', 'RSF', 'Chen-Feng', 'LGN', 'chairman', '64,2', 'Jonson', 'My', 'républicanisation', 'stockeur', '1673', 'Kissinger', '14,2', '3,43', '1983', 'sociétés-écrans', '119.000', '11.870', 'Borel', '233', 'Gourcuff', 'monétiques', 'SNJ', '1,79', 'Stoltenberg', 'réassureurs', '15', '37,2', 'Fère-Champenoise', 'Setubal', '116,5', 'Mermet', 'Varaut', 'étalon-or', 'CNAM', 'CAC', 'quatre-vingt-quatre', 'micro-', 'Certeau', 'Goldsmith', 'Louxor', 'Angoulême', 'Desmukh', 'PJ', '3.000', 'Summers', 'Lebaube', 'chèrement', '2.014', 'Alsace-Lorraine', 'JEF', '18855', 'Heureusement', 'dénervé', '5,50', 'CME', 'quatre-vingts', '1,77', 'Wild', 'fortiori', 'UUP', 'franc-mark', 'trente-et-une', '588', '1952-1953', 'Secondat', 'Fitz-Pegado', 'Douyère', 'R.', '3,42', 'Khmelnitski', 'Furniture', 'Grandin', 'tristement', 'entreprises-phares', '92,96', 'Breguet-Atlantic', 'Halbwax', 'commutant-type', 'deçà', 'Catia', '1987', 'Médiamat', '1750', 'Co', 'Castelo', 'worsred', 'maurassiens', 'inquilinaje', 'Norilsk', 'of', 'Superphénix', 'ESO', 'Viard', 'ministre-président', 'Olszewski', 'Kozo', '29', 'ombreuse', 'Bouygues', '32.000', 'semi-Conductor', 'outre-Manche', 'pare-soleil', 'Chin-Feun', 'nouveau-né', '3,3570', 'spa', 'Ecosystème', 'Champs-Elysées', '3.753', 'Croissez', 'CESI', 'Motors', 'quartzeux', 'Thiollet', 'après-cohabitation', 'TCE', 'Vénasque', 'véritablement', '787', '237', '1926', 'Ominco', 'Taiwan', 'Debré', '25,3', 'Souain', 'Camdessus', 'soupesait', '40', 'Careil', 'VVF', 'Dumon', 'excursionne', 'amplifieront', 'Eiffage', 'MATIF', 'Puy-en-Velay', 'babyboomers', '31,8', 'Valeo', 'Chines', '571-597', 'invariablement', 'Oural', 'mama', 'Rocard', '1448', 'Mauroy-Delors', 'movie', 'Bourdieu', 'VMS', 'inlassablement', 'Sting', 'Prévert', 'Saint-Pétersbourg', 'succursalistes', '5,45', 'Quatre-vingt-quatorze', 'popiwek', '458', 'Abedi', '1870', '4,5', 'UDF-PR', '8,9', 'amnistiable', '20,2', 'rééchelonnements', 'Dodd-Frank', '25', 'RTP', 'chèques-vacances', 'est-allemande', 'irrésistiblement', '565', 'Ivrea', 'Deverra', 'para-politiques', 'joint-ventures', 'Carbofrance', 'CFCA', 'Italtel', 'Paris-Dakar', '540', '15,5', 'Valente', 'UNICEF', '45.000', '160.000', 'Bourin', '0,25', 'extrêmement', 'formation-reclassement', 'Bayerischen', 'crûment', 'Educâble', 'Bacquet', '5,49', 'XVIII', 'ferrata', '8,8', 'Volskwagen', 'écourtent', '62', 'Jean-Manuel', 'tâtaient', 'Andreuzza', 'Rollat', 'FNCA', 'CGV', 'nord-américain', 'All', 'Nouvel', '43,25', 'Fleury-Michon', 'vingts', 'San-Diego', 'légalement', 'CNP', '1.100', 'Seiters', 'french', 'Lefur', 'sommerso', 'titres-chocs', 'XIIIe', 'devis-type', '217', '6,2', '4,75', 'VLT', '120000', 'eurosceptiques', 'Monribot', 'Grand-Dallas', 'spectaculairement', 'CEPII', 'Duhot', 'corrélativement', '1938', 'Johannès', 'Auto-Éco', 'Bundes', 'essaient', 'cimentière', 'fluctuaient', '1945-1953', 'contructions', 'Ponthieux', 'soixante-deux', 'ex-', 'jump', '0,35', '42,5', 'Balkany', 'Viet', 'zapperont', 'visiblement', \"chef-d'oeuvre\", 'donnant-donnant', 'condensats', 'Airways', '970', 'Prate', 'fiscalement', 'Notat', 'au-dehors', 'Mitsui', 'Garett', 'EBM', 'base-ball', 'mono-industrie', 'trente-deux', 'UNAT', 'TACIS', 'Dimitrievitch', 'EDI', 'Pichincha', 'téléphobes', 'Bolatto', 'réellement', 'tamisés', 'vice-président', 'Cruise', 'SPQR', 'Lantra', 'Fares', 'Calédonie', '95', 'Designe', 'Lagha', '!', 'tête-à-tête', '44', 'Sanyo', '5.759', 'Gutenberg', 'Zahidi', 'Ventilo', 'pluri-', 'Bokassa', 'rondelettes', 'ISBN', 'Bonny', '20,4', 'Qatargas', 'wave', '40,1', 'group', '6,40', 'Julliard', 'tragi-comique', '35e', 'porte-clés', '77,4', 'Brecht', 'Mireille-Bénédicte', 'Burnham', 'Neto', 'Kanemaru', '11.526', 'pétrochimique', '48.000', '1788', '18,6', 'Apollonies', 'Préalablement', 'Usine-Publications', '509', '1950', 'louvoyé', 'aides-éducateurs', 'amplement', 'Bousquet', 'Lyon-Turin', 'Hans-Joachim', 'Ranque', '68,9', '1,60', 'Arthuis', '544.412', 'psychologiquement', 'Mitsubishi', 'ejideros', 'loans', 'CO2', '3.090', '400.000', '9', 'sine', 'CHMP', 'trente-neuf', 'Caixa', 'Kippour', 'Mexico-City', 'AFRIQUE-EUROPE', 'Problem', 'agro-alimentaires', 'Lefoulon', 'Gimvindus', 'Soler', '37,9', 'AUD', 'animez', '3.200', 'Ironiquement', '4000', '149', 'Descours', '2840985314', 'Delort', 'Coudari', 'Ruhr', 'abordions', 'assemblée-marathon', '370', 'Yoplait-Candia', 'Committee', '2.231', 'Dumay', 'dello', 'quasiment', '620', 'Schweitzer', 'Roissy', 'contre-pouvoir', \"'m\", 'Thabourin', 'Mathussière', 'assurances-vie', '54,5', 'NEDC', 'Bréhal', 'Malev', 'Bolzon', 'nouvel', 'Saint-Jean', 'A2', 'Liu', 'Cauteretz', '18.000', 'Méry', 'matériellement', '72.000', 'Rosès', 'Sumitomo', '1,12', 'cm³', 'Demange', 'Louts', 'bar-tabac', 'Angoulème', '23000', 'Prince-de-Galles', 'Di', 'Celles-ci', '73,8', 'Ssangyong', 'Banexi', 'technico-économique', 'Bodard', '280.000', '100,62', 'running', 'Lang', 'FIJ', '73', 'Deschanel', 'Natwest', 'sacro-saint', '13,45', '1,47', 'CSO', 'Rmistes', 'chauffeurs-routiers', '2002', \"Boissy-d'Anglas\", 'Fennebresque', 'garden', 'Janeiro', 'Paggio', 'Sorepark', '0,62', '7,7', '5000', '17,494', 'Deutsche', 'inégalement', '660', 'rééquilibraient', 'Domange', 'Rubber', 'Blainville', 'Audi', 'Alkirch', 'CIB', 'Syntec', '19', 'Balcerowicz', 'discrètement', 'procès-verbal', 'Leiris', 'fabricant-grossiste', 'Contrairement', '9,6', 'Kvant', 'Shui-bian', 'déja', 'Propria', '4,35', 'réceptionnistes', '23,2', 'sous-estimation', '915.000', 'après-vente', '92.494', '11,1', 'Potentiellement', 'bitoniaux', 'Sharry', 'SOLA', 'Sabouret', '2,23', '240', 'Pilkington', 'Lopez', 'Ras-Laffan', 'Giffaumont-Champaubert', 'habituellement', '203,38', '92-1268', 'stylicien', 'Coué', 'Equilbey', '41,017', '525', 'Deviers-Joncour', 'Montigny-les-Metz', 'GAME', 'accord-cadre', 'Servan-Schreiber', 'Trichet', 'anticoncurrentielle', 'Kirch', 'DFF', '33,3', 'PSU', 'Là-dessus', 'perpendiculairement', 'USGS', 'I', 'Jean-Christophe', 'Bianco', '770.000', 'Mirror', 'Château-Margaux', 'Ile-de-France', '550000', '740', 'Patten', 'credit', '4,65', 'Vlajko', 'en-tête', 'Saint-Vincent-de-Paul', 'Ambérieu', 'Lenoir', 'funk', '114', '442,6', '130', 'Alcara', '15,68', 'Besnier', 'quatre-vingt-dix-neuf', 'autoparodie', '5,97', 'Metall', 'Etat-nation', 'néantisés', '41,19', 'Vnesheconombank', 'Navarro', '54', 'Dachaud', 'USDA', 'Tallandier', 'après-guerre', 'évidemment', 'conspirationnistes', '107', 'exceptionnellement', 'Istat', 'Yahoo!', 'Georgescu-Roegen', 'Mme', '693.200', 'CHOMAGE', 'commercialement', 'plein-air', 'Mahfouz', 'Point-clé', 'Seveso', '7,66', 'énormément', 'Gildas', 'surpiquer', ')', 'beaux-arts', 'Whitehall', '374', 'Worms', '2.807,7', 'Morgen', '1.702', 'Ecollection', 'Largement', '316.900', '15.520', 'délitait', 'Formellement', 'économiquement', 'Kéloglanian', '2.014,05', 'Contassot', '1110', 'passe-partout', 'bi-', 'Toyota-City', 'm2', 'Sia', '531', 'zappeurs', 'Bade-Wurtemberg', 'Rarement', 'Corbusier', 'gracieusement', 'IRI', 'Béghin-Say', 'VAZQUEZ', 'vis-à-vis', 'Castro', 'Hanin', 'préféreriez', 'Nietzsche', 'Flying', 'SR-71', 'NEC', 'Mafouz', 'épaississaient', 'Ford', 'vingt-neuf', 'bruyamment', 'hâterait', 'détériorerait', 'Ford-Volkswagen', 'Robelin', 'CFA', 'Ninja', 'acheminâmes', '750', 'quatre-vingt-dix', 'Villin', 'Béarn', 'J.-L.', 'directement', '150', 'Charlie-hebdo', 'Boutet', '600000', 'quotidiennement', 'Game', 'Eurostat', 'Concrètement', 'Jean-René', 'Limited', 'modo', 'librement', '35000', 'PCV', 'Roseman', '53', 'après-midi', 'démocrate-chrétien', 'Finarte', 'Arjil', 'Saint-Amans', 'Orkem', 'Parayre', '1990-1991', '51,3', 'courûmes', 'déferait', '1.164', '104,8', 'Chimboraço', 'refranchir', 'FNE', 'Akers', '380.000', 'Vigot', '2009', '2,9', 'Sierra-Léone', 'CDF', 'commenterait', 'Portelli', 'réalignements', 'SED', 'NNPC', 'Budapest-Ferihegy', 'crève-misère', 'Imax', '1972', '7,09', '28.000', 'Ecurey', 'Tiananmen', 'VINGT-SIX', 'Saouma', 'Carthage', 'Lagouche', 'PKK', 'Mercedes-Benz', 'Quesnot', '3.809', 'Tuquoi', 'paneuropéennes', 'Mont-Cenis', 'Viannet', 'Sussex', 'rétro-', '3.146', 'Logiquement', '16.757', '500.000', 'sous-évalués', '56,9', 'Gaddum', 'SNAV', '3,3675', \"Val-d'Oise\", 'Villacoublay', 'Tobacco', 'Charente-Maritime', 'Fortune-France', 'M.', 'Bozzo', 'Walck-Pfaffenhoffen', '9.200', '900', 'Jaroslava', 'trente-cinq', 'Paris-Bordeaux', '225,16', '64,5', 'Emile', 'Giaccobi', 'Financial', 'Ursul', 'trente-et-un', 'Artzner', '36.100', 'Boulogne-sur-mer', 'Anguilcourt-le-Sart', 'Haut-Rhin', 'abondamment', 'Planck', '640', '5,10', 'Asset', '5,95', '25-34', 'sous-sections', 'Venezuela', 'CEI', '92.866', 'ACOSS', 'catégoriquement', '6,1', 'beau-père', '69.900', 'Vannes-le-Châtel', 'noblement', 'URL', 'Jäggi', 'contrits', 'Willow-Run', '1959-1962', '7,5', 'Méridian', 'Vigouroux', '100.000', 'durablement', 'Stempel', 'Brandenburg', 'Dupuit', 'Austerlitz', '300000', '6.829', 'contre-enquête', 'Ciolina', 'barbouzarde', 'Palazzi', 'Securities', 'Regioliner', 'Aker', 'bâtiment-génie', 'Nappi', 'Jean-Paul', 'cruellement', '1890', 'Economica', 'Monti', 'Neiertz', 'Izvestia', 'extraordinairement', 'Micra', 'Alenia', 'sous-consommation', 'SNEP', 'Beauty', '124,80', 'Punta-del-Este', 'Calgary', 'CLT', '1997', 'peut-être', 'Jack-Yves', 'Actuellement', 'Worth', 'Daily', '1.400', '3,41', '2018', 'Esterel', 'Solery', 'Asturies', 'Ganett', 'Leser', 'industrial', 'Vieux-Colombier', 'pyrénéisme', 'Mitterrand', 'progressivement', 'Radonet', '109', 'Z', 'celles-ci', 'Thomson-CSF', 'euro-obligations', 'Schreiner', 'écotaxe', '7,45', 'ADNOC', \"Côtes-d'Armor\", '1975', '268', 'km2', 'Ier', 'Reydel', 'facilement', 'Djalloud', 'Gatt', 'APEC', 'Bénasque', 'escarpeur', 'Fayette', 'quarante-sept', 'CCR', '2.610.000', 'Meissonnier', 'Gutman', '7,2', '1991', 'lui-même', '378', '%', 'Mecklenbourg-Poméranie', 'Laroze', 'apprentis-acteurs', 'nationalement', '62,8', 'afnium', 'Apparemment', 'soixante-cinq', 'outsourcing', 'MDR', 'Puel', '1934', 'Gillant', 'chauffe-eau', 'INFINT', '98', '24,7', 'PC', 'séraphisme', 'Franc-maçonnerie', '93,5', 'apurés', 'Skandia', '1956-1957', 'Poperen', 'forcément', '7535', 'A4', 'Branly', 'Lloris', 'CDS', 'George-', 'Soubiran', '1963-1971', '1947', 'Conflans', 'Laske', '2226116214', 'LIFFE', 'Newco', '1953', 'avant-guerre', 'FDP', 'Belfortain', 'Anne-Marie', 'républicaniser', 'GUF', 'Kojak', '131.000', 'Skoda', 'Dunkel', 'Colonna', 'GAN-CIC', 'Guintoli', 'ejidales', '2,7', '415000', 'graphosphère', '100000', 'RD192', 'Syveton', 'spaghetti', 'Twain', 'Chevènement', 'Lévy', 'Alma-Ata', 'favorablement', 'ponctuellement', 'Eisswein', 'Merrien', 'Nokia', 'FGAAC', 'libéralement', 'OACI', 'SFS', 'Northern', 'raisonnablement', '174', 'Knicks', 'Eurogen', 'Deuxièmement', 'Montrémy', 'Vrillière', 'Sole-24', 'Candia', 'notoirement', 'non-', 'hirudine', '43', '1960', 'Garrec', 'en-dessous', 'Arab', 'Senhao', 'Galbrun', 'Foucauld', 'Pétain', '78', '1842', 'Kaske', 'Évènement', 'Stasi', 'prêt-à-porter', 'Saint-Martin-Rivoli', '767', '25,7', '391', 'Milutinovic', 'Pagis', 'Nétanyahou', 'EAI', 'Zelnick', 'hoc', 'sagement', 'Blum', 'Taittinger', '1817', 'Sephi', '494', '5,11', 'champs-Elysées', 'entre-temps', '155.000', 'DeLa', 'Dusaulx', 'Souk-al-Sabt', 'Brazil', 'Dow', 'BID', 'Lalitte', '16,2', 'Feyel', 'saint-Gobain', '81', 'dûment', 'Phu', '500', 'dénationalisations', '495.000', '41', 'contre-propositions', 'Lepicard', 'E.', '18,2', 'franco-française', 'm²', 'Academy', 'COSEC', 'LTV', 'plombier-zingueur', 'gèreront', 'Dynaction', '0,95', 'Louvie', 'Ocalan', '6ème', 'Niedermowwe', 'Christophersen', '17e', '130.000', 'zig-Zag', 'avant-dernière', '1979-1982', 'Etat-providence', 'Detroit', 'Warner-Seven', 'Boccon-Gidod', 'SOFIL', '270', 'Bank', 'Val-Fréjus', 'Relativement', 'contre-choc', '3,4130', 'sous-réseau', 'demi-', 'Zemin', 'Soleil-Levant', 'Berryer', 'arrière-petit-fils', 'Staedelin', 'Cléon', 'charpentent', 'GTEI', 'Eiffel', 'Ferrari', 'culpa', 'syndicalisés', 'Leyrey', 'Darnal', 'Isoré', 'Alahaji', 'Confindustria', 'Tampering', '?', '216', 'tonnait', '250000', '22,4', '1906', 'ressaisie', 'Stépachine', 'HEC', 'Poullain', 'Arianespace', 'Nallet', 'Blas', '3,73', 'Jawaharlal', 'Craxi', 'RDA', '583', 'lasserait', 'Frantzen', '50,6', '3,5', 'prêteuses', '2004', 'Evgueni', 'drastiquement', 'Telecommunication', '103,9', '272', 'BTAN', 'XXe', 'Peake', 'Rawlings', 'efforceraient', 'sous-préfet', 'écumeux', 'Olida', 'Andrézieux-Bouthéon', 'honoreraient', 'Churchill', 'Gestapo', 'bonnement', 'Galien', 'Bourquin', 'Cahin-caha', 'F-16', '1965', '152', 'SCOPD', 'contre-rapport', 'sous-japonaise', 'Raytheon', '9,85', 'Cardenas', 'Leclerc', 'UNEDIC', 'Jean-Marie', '13', 'Nehru', 'unilatéralement', 'names', 'CFAO', 'justement', 'laisser-faire', 'PAC', 'Soum', 'Gafner', 'Seasons', '112,44', 'Despuech', 'Saône', 'Fennal', '12,9', 'Rodrigues', '1000', 'Bruneau', 'euromarché', '37,7', 'G20', 'bund', 'Pacifique-Ouest', 'CCFA', 'GIE', 'Iribarne', '1', '28', 'Ossau', 'Grundig', '1250', 'Gesellschaft', 'County', 'Saint-Louis', 'Delpey', 'lavanges', 'Haute-Corse', 'baby-sitter', 'Ciba', 'France-Inter', 'Lodz', 'Haroon', 'Evobus', 'Escoubous', 'Week', 'étalon-blé', 'CD.F-chimie', '25,2', 'revascularisations', 'Gherardi', 'CHEVILLOT', 'lottizzazione', 'grand-mère', 'machine-outil', '158', 'hors-cote', 'Samuel-Lajeunesse', 'Sainte-Catherine', '37.570', '7.000', '223', 'Haye', 'refroidissante', '6.300', '8,25', 'Hedge', '178', 'PSP', '75007', 'Accor', '265', 'Bettencourt', '1951', '0,16', '730', 'MS-DOS', 'Aegon', '4,867', '34', '96,69', 'Rubin', '7,31', 'éternellement', '154', '17,9', 'Néthou', 'campaign', 'Colombier-Fontaine', 'CNB', 'Cluzel', 'Lardjane', 'savoir-faire', 'Frankfurter', 'Bazire', 'Muzelle', 'BRGM', '446', '2.251', 'accessed', 'AOC', 'ratatinée', 'IML', '6,20', '13,1', 'Dupont', 'Joyot', 'chauves-souris', 'remue-ménage', 'médiocrement', 'contrario', 'Klein', '7700', '45,3', '1,3', 'exactement', 'socio-professionnelles', '1986-1988', 'Rovno', 'contre-visite', '31000', '18.400', '380', 'Eska', 'Léotard', 'MBB', 'Cosmetica', 'Terriblement', 'Bohême-Moravie', 'Ecofin', '53,9', 'Moreau', 'Buy', 'Andrésy', 'Lacharrière', 'Premièrement', '880', 'M6', '2.000', 'évènement', 'contrairement', 'nippent', 'Quant', 'Safa', '768.000', 'Nederland', 'Spotkania', 'Nobil', '118.385', 'CNDP', 'Gaudin', 'radicalement', 'Roosel', 'Metpart', '1,2', 'IRDI', 'SGF', '1.703', 'Dreyfus', 'PTT', 'Décidément', 'Troisièmement', 'deficiency', 'vitae', 'Rose-Croix', 'Mirrors', 'Senneville', 'Dray', 'delà', 'Levaux', 'Rioumaou', '55', 'Monod', 'Sysorex', 'potentiellement', '358', 'Poehlman', 'CEFTA', 'Cragnotti', '172', 'Atalla', 'Vergez', 'ronchonnaient', 'Ex-', '5.543,30', '1,858', 'Hongkong', 'UJJEF', '471.000', 'Fualdès', 'Bourget', 'Chéreau', 'long-courriers', 'San-Gennaro', 'dégraisse', 'Sèze', 'V6', '1992', 'reposâmes', '92.535', 'Pays-Bas', '290', '1939-1945', '3,4305', 'préconisons', 'ATT', 'Bolloré', 'tromba', 'Angestellter', 'Zimmermann', '53.000', 'Goldorak', 'OPEP', '1784', '1,13', 'Baedecker', 'HomeFed', 'Andros', '168,4', 'Cementos', 'Drabu', 'durement', 'carabineros', 'ESSEC', 'infiniment', 'kids', 'PRET', 'Schönhuber', 'Broad', 'Cadarache', 'IG-Metall', 'contre-sanctions', 'Ribaud', 'Footsie', '74.000', 'Maistre', '331', 'Gilson', 'COURCELLES-LES-MONTBELIARD', 'Rausch', 'Monark', 'OPAC', '3.092', 'Jean-Bedel', 'Nuevo-Leon', 'Hitler', 'ETATS-UNIS', 'Morisson', 'Lassalle', 'Olmex', 'Daher', 'rousseauiste', 'Delessert', 'Caf', 'Buhler', 'Saint-Germain', 'Möllemann', '15-25', 'Bertelsmann', '88,97', 'Nam', 'Grosser', '8,4', 'interpellez', 'Bohbot', 'Montmorency-Laval', 'barguigner', '39,5', '21e', 'Plon', 'révisaient', 'Rundfunk', 'vidéo-communication', 'EPAR', 'Haberer', 'seulement', 'Lancôme', 'Précédemment', 'turn', '12,2', 'arrière-garde', 'IFP', 'proportionnellement', '360', 'continuellement', 'étonnamment', 'UDF-RPR', 'Edimbourg', '^', 'phytosanitaire', 'UFB', 'éco-bilan', 'AT', '6,6', 'Reuters', 'Fitzpatrick', 'Barnier', 'Bjoernskov', '2.350', 'Berthiaume', 'Mather', 'logosphère', 'CED', '3,14', '245', 'FMP', 'Haute-Garonne', 'Jean-Yves', 'Moureaux', 'Kampf', 'ni-ni', 'Kubrick', 'Goldfinger', 'Institutional', 'proprement', '1968', '42000', 'Calcio', '1,93', 'Saussure', 'Falcon-2000', 'sino-britannique', 'Armenia', \"presqu'\", 'réacclimater', 'contre-feux', 'pots-de-vin', 'hélons', 'Vauthier', 'Delpeyrat', 'éventuellement', 'Serang', 'LRM', 'Britannnique', 'Sogenal', 'PLA', 'Burlington', 'Sarkozy', 'Vuillaume', 'Carli', 'Cegelec', 'Moverman', 'Ieoh', '8,75', 'Petroleum', 'bien-être', '791.667', '1622', 'CFM-56', 'contraventionnelles', 'Thomson', 'check-list', 'monts-de-piété', 'B-2', 'révât', '1935', 'Grimblat', 'Planchon', 'Pasumot', 'Staline', '33,4', 'austro-hongrois', '900.000', 'Grove', 'trade', 'Sassou-Nguesso', '8,48', '1981', 'DSIN', 'Sega', 'autoport', 'Gaboriau', 'Alègre', '8,24', '67,9', 'différens', 'franco-allemande', 'annuellement', 'Satory', 'Loral', 'nettement', '2A', '10500', '*', 'Dassault', 'Evaluée', 'Rothschild', 'homeless', 'Buchsbaum', 'vingt-deux', 'raffermira', 'Mandag', 'QGPC', 'Cesano', '1869', 'passionneront', 'bio-carburants', '1611,4', '600', 'Microsoft', '17.600', '254', '1821', 'indo-pakistanaises', '8,35', 'Defossez', 'J.-C.', '12.000', 'Gorby', 'Volvo', 'Boishebert', 'Mitteldeutscher', 'Aufbruch', '2,5', '0,89', 'Wilmans', '891', 'Findus', 'hedge', 'Dutchman', 'hispaniser', '10,9', 'Fininvest', 'Genève', 'Godard', 'sous-estimé', 'surendettement', 'Chérèque', 'Delors', '312', 'FFA', 'Delos', 'Djebel-Ali', '33', 'peton', 'EMEA', 'relecteurs', 'Joly', 'DAF', '101', 'Spie-Batignolles', 'Keizai', 'surqualification', 'PSA-Renault', 'tiralleurs', '8.544.988', 'découvrimes', 'Lingesler', '550.000', '30,3', 'Benetton', 'Sri-Lanka', 'terre-à-terre', 'Yoplait', 'abciximab', '37,8', 'San-Francisco', 'Lyonnaise-Dumez', 'No.', '7.150', 'Handelsblatt', 'UNOSTRA', 'Haberler', '118,7', 'Gamesa', 'lentement', '19,09', 'Compu.', 'UE', 'allègrement', '43,3', 'money', '75.000', 'Berger-Levrault', 'Toshiba', 'boucs-émissaires', '34000', 'to', 'Charles-Emile', '3.066.400', 'valse-hésitation', 'Charvet', 'Celui-ci', 'Kok', 'Ségolène', 'B', 'sensiblement', 'Kabuto-cho', 'tellement', 'Zeid', 'hors-cadre', 'Hector-Malot', '84', 'Void-Vacon', '1907', 'Culet', 'Chrysler', '1990-1992', 'nord-américaines', '111', '2,87', 'fourre-tout', 'Lockerbie', 'A3', '348', 'Dauphiné', 'laïcisant', 'RH', '15.000', 'Inexorablement', 'passivement', 'procroate', '632', 'Casetta', '57,5', 'Caire', 'Bourgois', '2,74', 'désengagements', '1,57', '30,45', 'scénarisation', 'décidément', 'quatre-vingt-deux', '1932', '2400', 'Ledru', 'Corse-du-Sud', 'financièrement', 'Modling', 'Peybernès', 'Abessalam', 'Haemmerlin', 'Plan-des-Etangs', 'amèrement', 'Graal', 'Lessons', 'ouest-allemands', '63', 'multimédiatiques', 'Pictures', '260.000', 'Delavenne', 'Curieusement', 'House', 'Mochis', '12', 'Loïk', '400000', 'Fed', '2.969', 'approvisionneront', 'Chen', '345', 'costards', 'Sov', '5,1', 'Montreux-Château', 'préalablement', 'stations-service', 'fesons', 'Bigre', '206', 'phytosanitaires', 'Funds', \"Jeand'Heurs\", 'di', 'Puy-de-Dôme', 'Montmartre', 'prudentiels', 'faux-semblant', '450.000', '30,5', 'Dassler', '3,10', 'Vivendi', 'Adidas', 'télé-', 'sextant-avionique', 'empiriquement', 'Syrian', 'Rapidement', '72.354', 'Telekom', 'au-dessus', 'désirai', 'PEKIN', 'EDF-GDF', 'Kuhn', 'Dumas', 'Neftegaz', 'funds', 'Credit', 'CGCT', '188,7', 'timidement', 'niortaise', 'Albessard', 'Pallywood', 'réindexer', 'Soulac', 'Almaz', 'Jean-Albert', 'avant-coureur', 'à-coups', '14', 'nous-mêmes', '97,47', 'Koninklijke', 'FGAS', 'ATR-72', 'Vidéopole', 'est-européen', 'Guffey', 'Taxil', '3.754', '1967', 'normalement', 'associates', 'Vogica', 'hasardait', 'Jose', '2008', '54.120', '7,4', '1911', 'Arvor', 'Lyon-Alemand', 'belli', 'chef-lieu', '3.193,65', '1.974,55', 'recherche-développement', '808', 'Afin', '650.000', '1,5940', 'Malraux', 'ressaisies', 'difficilement', 'Saint-Paul', '14.533', 'Rio-de-Janeiro', 'Seatib', 'Maceo', 'Bordeaux-Mérignac', 'Parretti', '23', 'Bertrand-Faure', 'IFREMER', 'leader-ship', 'microéconomie', '103', 'Dresdnerbank', 'Barking-Reach', '6,9', 'abandonnâmes', '3.800', 'Jeunet', 'militari', 'insensiblement', 'arrière-petits-enfants', 'outre-Atlantique', 'Sodiaal', 'Salam', 'Trotski', '596', '890', 'SAR', 'provisionnées', 'CFE-CGC', 'basse-cour', '1/4', '36.055', 'diamantées', '170', '1874', 'Finanziaria', 'elles-mêmes', 'barégine', 'ARTE', 'Farinet', 'EBF', '9,1', 'prudemment', 'access', 'LPG', '281', '1,689', 'Pulitzer', 'zapperaient', 'dynamic', 'Naftalski', 'celle-ci', 'Daimler-Benz', 'Pirani', 'Moggiopoli', '12,8', '5.926', '196', 'Adjaokuta', 'grand-père', '4,3', 'Corinthe', 'G.O.', 'Chalon-sur-Saône', 'Hafnia', 'sud-coréen', '49.000', 'franchement', 'Hocking', 'Vingt-cinq', '501', 'Madinier', '49,5', 'Hachette-Filipacchi', 'sweats', 'Citibank', 'Das-Island', '1,59', 'Bonn', 'AJEF', 'C02', 'auto-allumage', 'RMistes', 'impérativement', 'Demokratisches', 'ponctua', '2,569', 'Caffarelli', 'Savoye', 'CF', 'Fein', 'humanum', '1917', 'Levin', 'consciencieusement', 'vraiment', 'précisément', '102', 'Bin', '1915', 'homme-terminal', 'sous-évaluation', 'contre-pouvoirs', 'Council', '1,49', 'Schaer', '1941', 'bourrèrent', 'war', '1914', 'Montpellier-I', '5,18', 'empourpre', 'GE', 'Charasse', 'pc', 'P', '957', 'Ccas', '28,4', 'Bf.G.', '29,2', 'Vandoeuvre', '5,23', 'vraisemblablement', 'Family', 'Meciar', '3,25', 'avant-dernier', 'suon', '6,76', '1,785', 'Stern', 'AP', 'SFF', 'CEFRI', 'eurobanques', 'Courcoux', 'Delmas', 'Rétrocommissions', 'Nigerian', 'Rotillon', 'CBV', 'Andes', 'Bisounours', '55.000', 'CEL', 'DCN', 'Mast', '2,6', 'laborieusement', 'Curien', '9500', '54,8', '120.000', 'LS', 'GEC-Alsthom', 'Bébéar', '220725478X', 'TRW', 'quasi-', 'AIEJ', 'Daum', 'Sraffa', '4X4', 'Lunéville', 'Fokker', 'Chevignon', 'Republica', '51,1', '66', '68', '210', 'H.', 'ULN', 'Baltica', 'projet-clé', '18,10', 'Iacocca', 'SOFIREM', 'cinquante-deux', 'majestueusement', 'Ramoff', 'Etat-caisses', 'CEE', 'minima', 'çà', 'roupillait', 'soigneusement', 'articulerait', 'Drinking', 'Hourquette', 'Levallois-Perret', 'ESCA', 'Lechat', 'Dai-Ichi', 'drolatiques', '1918', 'soixante-sept', 'Brin-sur-Seille', 'Montacié', '192', 'Berghofer', '258', 'Dintilhac', 'SVT', 'Senderens', 'Beirut', '13,54', 'socio-économique', 'Eysymontt', 'beteiligungs', 'vingt-trois', '1,83', 'Ceux-ci', 'EFI', 'là-dedans', 'imprudemment', 'dix-neuvième', 'Marubeni', '46,7', 'extremis', 'Impetus', 'ITT', '3,75', 'cinquante-huit', 'Sagawa', 'désespérément', 'mensualisée', '36', '6,24', 'Fougerolle', '70-61', 'Bosson', 'Banqueting', 'Dammarie-sur-Saulx', 'tout-petits', 'Pyrénées-atlantiques', 'séparabilité', 'Metro', 'Panouse', 'porte-monnaie', '23,7', 'Durand', '9,36', 'Mtanious', 'Bayard', '91', '881900', 'isloise', '8,2', 'provisoirement', '4,1', 'Investment', '2.253', 'baisserons', '2.900', 'petit-fils', 'Zinsou', 'Franès', 'finement', 'Dix-sept', 'BRI', 'BNP', 'Nintendo', 'Balladur', 'Valréal', '115', 'rugissaient', 'Jacobs', 'Orangina', 'Cadel', '928', 'dessaisissant', '5,6', 'Cofreth', 'Seat', 'Tel-presse', 'subitement', 'Madine', 'dramatiquement', 'J+1', 'Sock', 'Hassep', 'Dantin', 'Apple', 'Tchuruk', '79,95', 'deflation', '696', 'Marnaval', 'Seulement', 'TKM', '13,80', 'solidement', 'Electricidad', 'Mont-Blanc', 'Carolco', 'replâtrages', 'Asko', 'Fourquet', 'Andreotti', 'Slaskie', 'Maastricht', '35', '506', 'CEPAL', 'Frigoscandia', 'Megastore', 'Burget', 'dix-huitième', 'Jean-Baptiste', 'sud-', 'Perot', 'länder', 'Davigel', 'illégalement', 'Comédie-Française', 'Kookaï', '669', 'alternativement', 'mini-', 'Signoles', 'judicieusement', 'Lenôtre', 'Covent', 'Fall', 'Bigorre', 'tour-opérateurs', 'Arguydal', 'superbement', 'faiblement', 'UHP', \"Demak'up\", '11.620', 'expert-comptable', 'CLF', '1974', 'ci-dessus', 'CNCC', '60,1', 'RFP', '71', 'Lambiotte', '1200', 'prédéfinis', ']', '8,50', 'Riboud', 'Deluchat', 'symboliquement', 'payiez', 'Bentsen', 'tee-shirt', 'ABOU-DHABI', '1884', 'ultra-', '2006', '1971', 'Jules-Romains', 'INSEAD', 'Fontenay-aux-roses', 'Bertinotti', '1.134', 'Lomet', '304,5', 'Prévost-Desprez', 'Disney', '75005', 'valses-hésitations', 'Pérouse', '575', 'microélectronique', 'UCSIP', 'TOLUCA', 'Schreiber', 'Sous-traités', 'Excelsior', '20.000', 'Baylet', 'Guerra', '52,3', 'veuillez', 'sous-jacente', 'incorrectement', 'Elifas', \"trompe-l'oeil\", 'Danané', 'Chefresne', '1.470', 'Products', 'Manet', '218.000', 'grèverait', 'multifonctionnel', '49,44', 'Aznar', 'Béart', 'Ferruzzi', 'RER', 'sérieusement', 'GUILBERT', '1930', 'tout-images', 'Jouve', 'Laffont-Leenhardt', '18ème', 'UET', 'Corradetti', 'facto', 'Maladetta', 'Deutschen', 'mouvemens', 'afro-asiatique', 'saisonnalité', 'Chauvinière', 'Pif-gadget', 'Urbina', '145', 'Buba', 'sustainable', 'lease-back', '125', 'Gomez', 'strictement', 'CGI', 'Loire-Atlantique', 'procédural', 'pots-de-vins', 'Kaufman', 'Lagayette', '1.601', '761', 'sous-jacentes', 'Errare', '14000', 'afin', 'Dhabi', '2001', '13,9', 'passablement', 'Zuchowicz', 'co-', '96,23', 'CEPME', 'Pedreno', 'Hannoun', '6-16', 'Clymène', '5.666', 'antipollution', 'MITI', 'sans-abri', '12,3', 'Ynetnews', 'Ouest-allemande', 'contre-performances', 'Christchurch', '9,4', 'shore', 'AFT', 'ralentissons', '100,2', 'préemballe', 'Malvy', 'Sochata', '164', 'F', 'Finanz', '650', 'Kinki', '200', 'Simoën', 'Celle-ci', 'Iberia', 'Limbourg', 'Karasz', 'Eramet', 'Mobalpa', 'Kodak', '1.875', 'Wilhem', 'reclassification', 'Giroud', 'Credito', 'Traditionnellement', 'onuso-américaine', 'Louise-Yvonne', 'Triel', 'hafnia', 'sans-emploi', 'Stone', 'Guerry', '50', '22,15', '2010', 'N2', 'Manglou', 'Stojiljkovic', 'NTT', '405', '11,67', '621', 'Expanso', '3.111', 'Moon', 'CM2', 'Odudu', 'Rajni', 'voiturin', 'marchandise-étalon', 'bank', '2,30', 'essaierons', 'IDC', 'Flichy', 'Egaz', 'pleinement', '390', 'ANPE', '30000', 'déprécia', 'zappent', 'néerlandophones', 'Kia', 'at', 'Régionalement', 'Godefroy', 'Mont-Saint-Martin', '310000', 'natation-triathlon-club', 'Saha', 'cash-flow', 'CE2', 'Dunhill', 'Arco', 'Briand', '20', 'trente-sept', 'Benyamin', 'George-V', 'Shackle', 'Wagons-Lits', '453', 'Guillaumat', 'Fourchette13', '1839', 'Rossinot', 'Khler', '136', 'autoritairement', 'Brauvilliers', 'Dussart', '2007', 'Leipzig', 'socio-économiques', 'Forget', '8,1', 'téléréservation', 'Schönberg', 'condottieri', 'Zaporozhe', 'Tholy', 'Mulroney', 'Autoliv', 'investment', '1789', 'para-', 'CSN', '0,5', 'SAHA', 'chènes', 'Saint-pères', 'potlach', 'Risc', 'Hervet', 'Izzadeen', 'Coudanne', '485.000', '6,68', 'Chubu', 'politico-', 'Bagnères', 'anciennement', 'Donzel', 'Fillon', 'BNCI', 'CEREQ', 'cognito', 'latino-américains', 'Flaubert', 'coactionnaires', 'Vale', 'denim', 'sous-titre', 'Walke', 'J.', '166', '2.309.130', 'Eco-Systeme', 'Sainte-Anne', 'Rhône-Alpes', 'Verot', 'crédit-relais', '2.971.000', 'Cassandre', 'Bercy', 'provided', 'sacré-coeur', '180.000', 'ceux-ci', 'Selenia', '92', 'struggle', 'Visos', 'Godart', 'Filofax', 'Edenor', 'patiemment', 'Vandingenen', '50000', 'Pareillement', 'Demoskopie', '98,9', '4', 'ternissait', 'Centrafrique', 'qui-vive', 'Vinci', '10,70', 'Zelentchuk', 'voleraient', 'Malouda', 'Hranice', 'Rohou', 'Iskandar', 'Marionnaud', 'minutieusement', 'Ukraine-Sud', '530000', '3,333', '_', 'fortuitement', 'insuffisamment', '17,5', 'Mandela', 'Rao', 'Vergès', 'quote-part', 'Botts', 'FEHAP', '0,02', 'Marriott-prince-de-Galles', 'Rabodeau', '467', '336.210,00', '7,1', 'Norlain', 'Roussel-Uclaf', 'Etats-Unis', 'float', 'foies-gras', 'centralement', 'UAP', 'isolément', '83.505', 'trouait', '8,72', 'free', 'Intermediazioni', 'SPP', 'Récemment', 'Pilviteuil', 'Perigot', '(', 'GPA', 'Euromarché', '18,27', 'Pirelli', 'Oréal', 'Hurand', 'Cazalet', 'Brasiliens', 'soixante-quatre', 'Alsthom', '36,7', 'Saône-et-Loire', 'Next', '3.130', 'socialement', 'Hervieu', 'exorcisons', 'Meech', 'Chaisaz', 'américano-hongrois', 'Rothmans', 'quarante-huit', '10,50', '523', 'Ornain', 'air-bag', 'Belfond', 'naturellement', 'Sogeti', '252', 'inexorablement', 'France-info', 'Cusiana', 'Schikedanz', '1999', 'évènemens', '-', '100', 'Sonata', 'Deferre', 'Nohant', 'Westdeutsche', 'désoeuvrement', '99', '1937', 'precise', 'Calvet', '9,9', '431', '1963', 'Courau', '80000', 'banquaires', 'Indosuez', 'Tardi', '60.000', 'Terraillon', 'U', 'HCR', '13,72', 'nord-américains', 'minifundio', 'ad', 'abasourdies', 'primitivement', 'SAE', 'Lanvin', 'courtisons', 'France-Antilles', 'Gaulle', 'ANC', 'cow-boys', \"Abbé-de-l'Epée\", '33,5', 'Tompkins', 'Tecni', '8000', '35-49', 'états-majors', 'roastbeef', 'Jaffra', '1983-1990', 'Havilland', 'cyclo', 'nerveusement', 'vaquait', 'Notre-Dame', 'descendions', 'Research', 'préqualification', '60e', 'Austrian', 'chatouillant', '11,25', 'Reuter', 'Iveco', 'RPR-UDF-UDC', 'Albright', 'volontairement', '5,2', 'Bertucci', 'HN', '1.500', '9,30', 'Lopoukhine', 'Néouvielle', 'Sud-est', 'J', 'demeurerons', 'Flins', 'FR', '3,38', '1.251', '685', \"Optim'hommes\", 'Berlin-Est', 'OFCE', 'Habituellement', 'paradoxalement', '1630', '500000', 'Haïfa', 'Giraud', 'Travlane', 'Bas-Rhin', 'emphythéotique', '1,4450', 'Grim', 'Rescue', 'France2', 'craquait', 'CV', '2,13', 'Raymondeau-Castanet', 'Screg', '45-36', 'joint-venture', '57.000', '1994', 'super-', 'F-117A', 'Guérard', \"Bailey's\", '1.525', 'SNEC', 'Initialement', 'incivique', 'Danske', 'au-dessous', 'Alomar', '83,5', 'Liaz', 'usine-symbole', 'libre-échangisme', 'Sassou', 'Grièvement', 'mal-aimée', 'finalement', 'CIATER', 'Levitt', 'Lamennais', 'RCS', \"Chavannes-sur-l'Etang\", 'Mans', 'Stéphan', 'traiterions', '503', 'actuellement', 'jazz-funk', '65,6', '300.000', 'hara-kiri', '1816', 'GM', 'Prisunic', 'Réole', 'Youppie', 'Europ-Assistance', 'VSD', 'Altech', '1.450', 'soixante-trois', '3.028.300', 'Ballard', 'pré-formateur', 'Verdunois', 'Nipoué', '42', 'élisabethains', 'Bizac', 'bien-fondé', 'Mainishi', 'Drexel', 'Boublil', 'Verdome', 'SAGEM', 'USPA', '0,67', 'pur-sang', '10,4', 'Poitou', 'maison-mère', 'Three', 'Quick', 'social-démocrate', 'personnellement', 'Rambo', 'XV', 'ANPLIE', 'presseraient', 'Wildlife', 'Espana', 'reforestation', 'MK2', '1955', 'Particulièrement', 'Bensimon', 'paneuropéenne', '1,50', 'compagnon-serrurier', 'Fitoussi', 'avant-garde', 'Deir-Ez-Zor', 'triomphalement', 'sous-gouverneur', 'PABX', '11.01.02.', 'Saint-Claude', '720', 'Littlehampton', 'Naf', 'Boudiaf', '63.000', \"presqu'île\", '19-20', 'franco-italiens', 'Gael', 'Industry', 'Immopar', 'FNTR', 'BFCE', ',', 'IME', '3.763', 'Compusearch', 'Iberduero', '10,81', 'Créteil', 'Vincenne', 'Vasconi', 'Boucault', 'enlisera', 'Rouméas', '29,9', 'pare-brise', '112,20', 'François-Xavier', 'sainte-Victoire', 'Mels', 'Sarrelouis', 'Hansen', '2.389', 'Secodip', 'Loira', 'Telegraph', '1.899.484', 'f', 'NORTHERN', 'historiquement', 'War', 'aseptisaient', 'Ouest-allemandes', '15ème', 'Marti', 'Stéphanos', '280', 'Carrère', 'Maucher', 'conjointement', 'OEA', '229,8', 'Alhuwalia', 'Trautmann', '1,90', 'Romorantin', 'grand-messes', 'Radio-France', '..', 'COMECON', 'Birt', 'LIBERTAD', 'KBG', 'Reliquet', 'Saint-Pierre-et-Miquelon', '68.470', 'Vingt-trois', 'Michelin', 'Lyon-Figaro', 'EEE', 'Baudis', 'admirablement', 'M-53', '129,5', '441000', 'Branco', 'Hang-Seng', 'Guéno', 'franche-comté', 'Darcet', 'embrassions', 'Haussmann', 'Mirecurtiens', 'Bosnie', '10.422', 'Sony', '455', 'Doumer', '84,5', 'Nèu', 'Mannesmann', 'Blondelle', 'Giga', 'EPT', '1.100.000', 'constamment', 'Sergueï', 'narcotrafiquants', '6,4', 'éco-fin', 'réunionais', 'spécifiquement', 'SDR', 'Maradona', 'UDF', '208', 'BESANÇON', 'Kier', '=', 'priori', 'Alcoa', 'Enders', '24,4', 'Gauthier', 'Sabatier', 'm3', 'Revellin-Falcoz', 'DRH', 'Vercken', 'Schengen', 'agro-alimentaire', '39,4', 'Lathière', 'Hopkins', '6,70', 'Utrecht', '287', '1,9', '236', 'ALENA', '173', '1,09', 'Etat-régions', '15,6', '2,28', 'admits', '1810', 'Thouvenel', 'Belot', 'Abubakar', 'ex-officio', 'Lazaro', 'Jones', 'américano-japonais', 'Sept-Laux', 'Nimes', 'anglo-américaine', '22,8', 'Saurer-Diederichs', 'Berlusconi', 'délocaliserons', '52000', 'Pas-de-Calais', 'PUF', 'psys', 'Lifson', 'contre-offensive', 'poétiquement', 'IFINT', \"Rubik's\", '177', '1989', 'Alahadji', '4,85', 'Historiquement', 'prochainement', 'sous-traitantes', 'Souchon', 'majorisme', 'Broendby', 'couramment', '918', 'île-de-France', '18,1', 'ADP', 'new-yorkaise', 'main-forte', 'Makeover', 'Gerpresse', 'dir-cab', 'Repa', 'Jospin', '4,7', '19,3', 'jalousement', '0,39', 'simplificateurs', '385', '4ème', 'BIE', 'II', 'bluff', '17', '472', 'localement', '215', '1957', 'savings', \"dizaï'n\", 'délibérément', 'momentanément', 'Falcon-10', '5,14', 'Bidegain', 'exagérément', 'fortement', 'Elf', 'Groep', 'medium-term', 'fluidités', '23,6', 'Ledoucin', 'Franchissez', 'VLM', 'Swiss', 'crapahuté', '107,36', 'indexait', 'Fermaut', '1977', 'Thiriot', 'miraculeusement', 'Confagricoltura', 'nord-est', '121.2', 'MODEF', 'Léotin', \"Alpe-d'Huez\", 'Baa1', 'Basse-Saxe', 'Disc', 'eurosceptique', '2ème', '4,95', 'Saint-Siège', '3.665', 'Elettrofinanziaria', 'BBV', 'Lucette', '117.000', 'vingt-cinquième', '2004-2005', 'inter-établissements', 'récessionnistes', 'TFI', '4.4', 'Torre', 'CSG', '275', 'Leenhardt', 'antidumping', '147', 'Paz', '3.045', '131.39', '65', 'Parallèlement', 'montâmes', 'Azoulay', '18,5', '1939', '11', 'recéder', 'Europerformance', '3.300', 'Broadcasting', 'Manos', 'Enel', 'Calciopoli', 'Rau', 'World', '107,92', 'efficacement', 'Brazzaville-Kinshasa', 'Costé', '4.890', 'SPPI', '45.701', 'gaîté-lyrique', '2028', 'Guillaud', 'Tchernomyrdine', 'Simone-Signoret', 'technologiquement', 'géographiquement', '111547', '2006-2007', 'assurerions', 'Winterthur', 'SANEF', 'nouvellement', 'Devillers', 'Pfauwadel', 'MPCS', 'INAO', 'Pintoux', 'Cognacq-Jay', 'MCI', 'brusquement', 'froissera', 'rez-de-chaussée', '150000', 'Villeneuve-lès-Avignon', 'FETESE', 'doctor', 'Poincaré', '46.000', 'unanimement', 'Jean-Mathieu', 'BCCI', 'XX', 'Forkum', 'Zajac', 'Chamard', '162.900', 'plus-values', 'Imaginarium', 'Arouy', 'appelations', 'ACESA', '790', 'CIO', 'roues-pelles', 'sous-groupe', 'memoria', 'Sarrebrück', 'étourdisse', 'Mr', 'GMF', '3.440', 'Benattia', 'subodorent', '337', 'Touzet', '42,13', 'Correa', '7.500', '165', '1,40', 'trop-tôt-considérés-comme-vieux', 'Speedo', 'dommages-intérêts', '170.000', '0,54', 'Becker', 'Saratov', 'Das', 'recomptent', 'crunch', 'Scandedition', 'Harvard', 'Johannesburg', 'automatiquement', 'Benghazi', '700', 'Smoby', 'Arud', '1,832', 'Alleau', 'Coirre', '3,63', '1867', 'RPR-UDF', '10.549.935', '1950-1970', '69,5', '304', 'entrefaites', 'Haute-Saône', 'Difficilement', 'MGM', 'Gatwick', 'Xantia', 'T', 'CCF', '129', 'Avant-Scène', 'Eugénie-les-Bains', 'American', 'partitocratie', 'Wroclaw', 'Aylwin', 'Péricard', 'Transcet', 'Dresdner', '1993', 'réputait', '530', 'Deng', 'Shanghaï', 'roche-sur-Yon', 'SGE', '21.032', 'Barèges', 'nord-ouest', 'States', 'Gare-du-Nord', '1642', 'sub-', 'SMCI', 'Grunelius', 'facteurs-clés', 'SCPI', 'Ciampi', 'Mega-store', 'Kissan', 'persona', 'sous-préfectures', '72', 'CDME', 'Doctoring', '20000', 'Finalement', 'success', 'Vaux-Racine', 'Drut', 'montable', 'Gaube', 'nihilo', '33,8', 'SGEN-CFDT', 'Tardivat', '83', 'bonasses', 'chiffrera', 'Plantade', 'integrazione', 'trente-quatre', '749,4', '1,4850', 'Photog', 'pâtissier-traiteur', 'Clinger', 'Olipar', 'plate-forme', '3,8', 'SIVP', \"man's\", '1982', 'Ingénico', 'SODERS', 'longuement', 'GUITTON', 'Haddad', 'gavages', 'Théoriquement', 'Sanofi', 'Dunod', 'NEW-YORK', 'Gandillot', 'Begemann', 'Madelin', 'Csesar', '1859', '600.000', 'cléricalisant', 'Plaza-Athénée', 'n', 'NHS', 'SCOPD-Manufrance', 'sensu', 'décret-loi', '50e', 'Egalisation', 'Warcholak', 'blocs-notes', 'Pepsi-Cola', 'Sauterne', 'für', 'Benco', 'Bernardini', 'Abbas', 'SELCUK', 'Diff.', 'trente-trois', 'aequo', 'politiquement', 'UNC', 'emboîtait', 'moins-values', 'Kourou', '40.000', '2.115', 'Eutelsat', 'SNB-CGC', 'Béré', 'Milewski', 'Bell', '1959', '4.000', 'Halphen', 'Salinas', '1,62', 'Galotte', '1900-1904', 'Kopchinsky', 'Gulf', '335', 'Sombart', 'périscolaire', 'S', 'rationnellement', 'ultrasophistiqué', 'plantais', '1.300', 'coll.', '1956', 'mitrons', '10.350', 'SFH', 'Proche-Orient', 'Kahloon', 'Dubaï', 'PROMATT', '302', 'PCF', 'nécessairement', 'Larkfield', '91,63', '2,53', 'Lupot', 'Novellino', 'Elf-Sanofi', '320', 'anglo-saxonnes', 'Guimaraes', 'Stéfann', 'GATT', \"Côte-d'ivoire\", 'Doubin', '1.235', 'GEC', '333', 'come-back', 'Murielle', 'Goetzfrid', 'Indiscutablement', 'mini-rencontres', 'Treuhandanstalt', 'Généreusement', 'ATOS', 'Norddeutscher', 'Stromboni', 'Leigh-Pemberton', '0,20', \"Monde-l'économie\", 'Guarda', 'rhodanien', '1928', 'Cambridge', 'Com', '127', 'Saint-Maurice', '89,9', '8.445', 'Locabail', '520', 'Electronic', 'Lurçat', 'Falconbridge', 'linéen', '3.700', 'Knowlton', 'Quarante-deux', 'Thiaucourt', '1734', 'extra-', 'Généval', 'embauchons', 'Semdim', 'Louis-Pascal', 'salarialement', 'Marland', 'Monopoly', 'Denormandie', 'Rousselot', 'précipitamment', 'exploration-production', '15,1', 'IFAV', '17,473', 'Bucher', '0,74', '1971-1979', 'Lofti', '1984', 'lave-vaisselle', 'pareillement', 'state', 'Deere', 'Mazowiecki', 'raffermissant', '938', 'Bootsy', '48', '97', 'Barberis', 'Karmitz', 'Nickelodeon', 'pipi-room', 'implicitement', 'Kirghiztan', 'SCA', 'Storebrand', 'tourisme-loisirs', 'phonographique', 'BSN', 'mainoue', 'Marne-la-Vallée', 'Arboust', '0,17', 'Medicines', 'laisser-aller', 'UNIRS', '3.336,16', 'SFO', '430', 'Bundespost', 'A36', 'Handels', 'lean', '3,36', '7.960', 'BCE', 'Constantinople', 'Bouw', 'indépendamment', 'Abondamment', 'Kadosh', '1981-1983', 'Benarnenses', 'URSSAF', '1619', 'draîner', 'Smithkline', '8,769', 'Eclerc', '19,9', 'Insta', '805', 'x', 'CNPA', 'clairement', 'Pentland', 'Greenpeace', '1914-1918', 'tam-tam', 'paiera', 'primo-accession', 'Dien', 'Scandinavie', '320.000', '262', 'Menem', '515', '43,610', 'Bosnie-Herzégovine', 'Khrunichev', 'Aerospace', 'STASI', 'disclaimer', 'DTB', 'Energy', 'Allgemeine', 'déchanteront', 'Stuyvesant', '8011', 'pétillent', '675', 'Buffalo', 'Guillen', 'Fimalac', '49,9', 'Northwest', 'sud-américains', '2290052418', 'Questiaux', 'Ikea', '12,5', 'Gascogne', 'systématiquement', 'Thérond', 'amoncelait', 'Laneuville', 'État-voyou', 'Duluth', 'CNPF', 'ejidos', 'Bordet', 'Freedom', 'israélo-', '1,85', 'état-major', 'Poto-Poto', 'OUA', '147,06', 'Filipacchi', '8,17', 'Estenssoro', 'Cracovie', 'PLC', 'repartager', 'social-démocrates', '1992-1993', 'fébrilement', '178.000', 'sous-estimées', 'Vaillant-miroir-sprint', 'Olivetti', 'Pump', 'MTV3', 'import-export', 'Ribéry', 'adjurée', 'CND', 'Gulliver', 'CBOT', 'dix-septième', 'Techniquement', 'DIA', '850.000', 'plein-emploi', '12.360', '8', 'expo-vente', 'Grancher', 'destructurantes', '6.700', 'échaudées', 'Fokker-100', 'United', 'Alassane', '1922', '17,8', 'Saint-Dominique', '65,4', 'Mendras', 'fidèlement', 'DDE', '1,27', 'Munzthal', 'Elf-Aquitaine', 'Decampse', 'Kimberly-Clark', 'entaillaient', 'COB', 'Dismisses', 'Korauto', 'TPI', '1945', 'Louvre', '1,04', '8,80', 'Ladreit', 'man', '289,80', 'follement', 'Salenques', 'rasséréner', 'EO', '566', 'combisme', 'UGAP', 'Virgile', 'Sanio', 'relevions', 'KIO', 'Edouard', 'formation-emploi', 'UMP', 'Naqvi', 'précédemment', 'VAG', 'Kuwait', 'Hygena', 'Pflimlin', 'Schimpf', 'société-holding', 'Fund', 'TVHD', 'MJC', 'Caim', 'Bayard-presse', 'deutschen', 'Réau', '78e', '140', 'est-allemand', 'Picture', 'Labinal', 'Nursultan', '625.000', 'Sedgwick', 'Lorentz', 'Salt', '558', '7,9', 'Giuily', 'sino-', 'débandades', '128', 'Accounting', 'OLP', '1,65', 'là-même', 'Sud-Ouest', 'Enterprise', 'ouvertement', 'Bruand', '16,10', 'Sous-traitant', 'Eurojob', 'Angiox', 'Telcom', 'Proprietary', '1.050', 'bêbête-show', 'GTM-entrepose', 'Talamon', 'Schwartz', 'Bull', '11.694', 'F.L.', 'disinformation', '30', '3,3538', '1971-1975', 'Hanson', 'gouvenement', 'Cliparim', 'Litani', '12,4', 'Kouchner', '70.000', 'EBZ', 'CFE', '86.000', 'Aircraft', 'Normandin', 'Saint-Cricq', 'Saint-Cyr', 'Belaïd', 'G.', 'Ramirez', 'Denvers', 'France-soir', 'downgrading', 'Paris-Lyon', 'intégralement', 'Obnubilées', '11,52', 'Lander', '827', 'Terminator', '4e', '11,2', 'Clean', 'NDR', 'fontion', 'sous-estimer', '12500', '16,5', 'Général-Beuret', 'Georgyi', 'Royère', 'OM-Belgrade', 'Primakov', '28,5', 'Girardin', 'Moretti', '17,6', 'Karenztag', 'Loir-et-Cher', 'ejido', 'Tricentrol', 'Inco', 'décrû', 'Adnan', 'GRS', 'Nexus', 'Saintignon', 'Deutscher', 'sous-traiter', 'altering', 'Vindé', '17605', 'Agip', 'rôdage', '92.1270', 'Zaklady', 'UIR', 'directeur-fondateur', 'PS', 'PDO', '106,6', \"Poor's\", '26', '0,42', '64,41', 'Bosch', '1966', 'Lynch', 'Moulard', 'Nkoum', 'Hills', 'René-Coty', 'all', 'Add', '1824', 'Rasmussen', '3,04', '870', 'Gillot', '305', 'Debonneuil', 'revendons', 'principalement', 'ATMA', '77,8', '4,76', 'Aubry', '88,4', '1633', 'SSII', 'vingt-six', 'feuillants', '132', '7000', '5,47', '16,53', 'essai-erreur', 'Moulinex', 'UIMM', '2,96', 'Gazzetta', 'Etourneaux', 'Bonnemire', 'Allégret', 'Mei-Kuei', 'payments', 'Boel', 'sous-préfecture', 'Séguillon', '121', 'Blue', 'anglo-allemande', '1366672', 'Mont-de-Marsan', 'Ringier', 'Biré', 'Newport', 'CIVC', '556', 'Octopussy', 'Sun', 'Rexrodt', '1988-1989', '75,4', 'redécouvreur', 'ETAM', 'Brantôme', 'CSL', 'pêle-mêle', 'AB', 'péniblement', 'Héas', 'réadmettre', 'Marie-Thérèse', 'Paranal', 'Mercuria', 'raffermis', 'Heilbronner', '2806', 'congés-conversion', 'Desfossés', 'Narasimha', 'bouche-à-oreille', 'administrativement', '123-85', 'pianoter', 'Tiberi', 'Goldman', 'coûts-bénéfices', '34,07', 'reconcourir', 'confraternellement', 'Malheureusement', '1815', 'Mesle', 'Tcheliabinsk', 'Sidjanski', 'TBWA', '85.300', '275000', 'Indochine', 'Boeing-727', '6,5', 'Lagardère', 'GPG', 'ébahies', '44.000', 'Midi-Pyrénées', 'Chanteloup-les-Vignes', '93.000', 'Vilaseca', '22,20', 'FC', '69', 'Kuo', 'Trade', 'IFIL', '8,14', 'Mabinvest', 'Exo-jeunes', '180', 'Lufthansa', 'Rotterdam', 'Haute-Marne'}\n",
      "7227\n",
      "23073\n"
     ]
    }
   ],
   "source": [
    "word_to_vec_map = {}\n",
    "unknowns = set()\n",
    "invoc = 0\n",
    "\n",
    "for w in vocabulary:\n",
    "    wn = normalize_word(w)\n",
    "    wr = remove_prefix(wn, \"-t-\")\n",
    "    wr = remove_prefix(wr, \"-\")\n",
    "    try:\n",
    "        vec = wv[wr]\n",
    "        invoc = invoc + 1\n",
    "    except:\n",
    "        unknowns.add(w)\n",
    "        vec = np.zeros(veclength)\n",
    "    word_to_vec_map[w] = vec\n",
    "\n",
    "print(unknowns)\n",
    "print(len(unknowns))\n",
    "print(invoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute `word_to_vec_map` for all words in the vocabulary using the `cwindow` embeddings, plus the suffix and\n",
    "custom feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_vec_map = {}\n",
    "unknowns = set()\n",
    "invoc = 0\n",
    "\n",
    "for w in vocabulary:\n",
    "    wn = normalize_word(w)\n",
    "    wr = remove_prefix(wn, \"-t-\")\n",
    "    wr = remove_prefix(wr, \"-\")\n",
    "    try:\n",
    "        emb = wv[wr]\n",
    "        invoc = invoc + 1\n",
    "        features = word_features(w)\n",
    "    except:\n",
    "        unknowns.add(w)\n",
    "        emb = np.zeros(veclength)\n",
    "        features = word_features(w, unknown=True)\n",
    "    suffix = suffix_vector(wn)\n",
    "    word_to_vec_map[w] = np.concatenate((emb,suffix,features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute `word_to_index` and `index_to_word` for the entire vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = indexify(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. The Embedding layer\n",
    "\n",
    "In Keras, the embedding matrix is represented as a \"layer\", and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, we create an [Embedding()](https://keras.io/layers/embeddings/) layer in Keras, and initialize it with the fastTeX vectors loaded earlier in the notebook. \n",
    "\n",
    "The `Embedding()` layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below.\n",
    "\n",
    "The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors).\n",
    "\n",
    "We first convert all our training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence. \n",
    "\n",
    "**TODO**: I'd like try if it makes any difference to add the </s> end tag to the end of each sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to check what `sentences_to_indices()` does, and check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7438.,   7476.,  29246., ...,      0.,      0.,      0.],\n",
       "       [ 18496.,  17309.,  19050., ...,      0.,      0.,      0.],\n",
       "       [  9431.,   4184.,  24638., ...,      0.,      0.,      0.],\n",
       "       ..., \n",
       "       [  1191.,    326.,  29246., ...,      0.,      0.,      0.],\n",
       "       [ 18496.,  13351.,  29539., ...,      0.,      0.,      0.],\n",
       "       [ 20878.,   4452.,   7752., ...,      0.,      0.,      0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_to_indices(X_train, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the `Embedding()` layer for use with Keras, using pre-trained word vectors. After this layer is built, we can pass the output of `sentences_to_indices()` to it as an input, and the `Embedding()` layer will return the word embeddings for a sentence. \n",
    "\n",
    "We use the following steps:\n",
    "1. Initialize the embedding matrix as a numpy array of zeroes with the correct shape.\n",
    "2. Fill in the embedding matrix with all the word embeddings extracted from `word_to_vec_map`.\n",
    "3. Define Keras embedding layer. Use [Embedding()](https://keras.io/layers/embeddings/). Be sure to make this layer non-trainable, by setting `trainable = False` when calling `Embedding()`. If you were to set `trainable = True`, then it will allow the optimization algorithm to modify the values of the word embeddings. \n",
    "4. Set the embedding weights to be equal to the embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained fastText vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 2                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"est\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=False,mask_zero=True)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][2][1] = 0.0\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][2][1] =\", embedding_layer.get_weights()[0][2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **weights[0][2][1] =**\n",
    "        </td>\n",
    "        <td>\n",
    "           0.19175\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Part-of-Speech tagger\n",
    "\n",
    "We now build the POS-tagger model using the previously built enbedding layer and feed its output to a bidirectional LSTM network with 128 states in each direction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POS_model\n",
    "\n",
    "def POS_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the graph for the part-of-speech tagger model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = BatchNormalization()(X)\n",
    "    Y = Dropout(0.5)(X)\n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    Y = TimeDistributed(Dense(numClasses, activation='softmax'))(Y)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=Y)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model and check its summary. Because all sentences in the dataset are less than 10 words, we chose `max_len = 10`.  You should see your architecture, it uses \"20,223,927\" parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001\\*50 = 20,000,050 non-trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = POS_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using `categorical_crossentropy` loss, `adam` optimizer and `['accuracy']` metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train your model. Your Emojifier-V2 `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = lists_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_indices = lists_to_indices(Y_train, pos2_to_index, maxLen)\n",
    "Y_train_oh = to_categorical(Y_train_indices, num_classes=numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Y_train_indices[1])\n",
    "print(Y_train_oh[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev_indices = lists_to_indices(X_dev, word_to_index, max_len = maxLen)\n",
    "Y_dev_indices = lists_to_indices(Y_dev, pos2_to_index, max_len = maxLen)\n",
    "Y_dev_oh = to_categorical(Y_dev_indices, num_classes = numClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_indices, Y_train_oh, epochs = 30, batch_size = 32, shuffle=True, validation_data=(X_dev_indices,Y_dev_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model should perform close to **100% accuracy** on the training set. The exact accuracy you get may be a little different. Run the following cell to evaluate your model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_dev_indices, Y_dev_oh)\n",
    "print()\n",
    "print(\"Dev accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a test accuracy of about 94.8% for a vanilla model using only aa1.txt.\n",
    "A vanilla POS model on the full training set gets a dev accuracy of 98.50%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev_indices = lists_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_dev_indices)\n",
    "\n",
    "# print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "# print(pd.crosstab(Y_dev, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "\n",
    "y_dev_oh = to_categorical(Y_dev_indices, num_classes = numClasses)\n",
    "X_dev_indices = lists_to_indices(X_dev, word_to_index, maxLen)\n",
    "pred = model.predict(X_dev_indices)\n",
    "\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "\n",
    "for i in range(len(X_dev)-1):\n",
    "    for j in range(len(X_dev[i])):\n",
    "        num = np.argmax(pred[i][j])\n",
    "        if(num != Y_dev_indices[i][j]):\n",
    "            wrong = wrong + 1\n",
    "            print('Expected POS tag: '+ X_dev[i][j] + '|' + Y_dev[i][j] + ' prediction: '+ X_dev[i][j] + '|' + index_to_pos2[num])\n",
    "        else:\n",
    "            correct = correct + 1\n",
    "total = wrong + correct\n",
    "print(\"Total  : \", total)\n",
    "print(\"Correct: \", correct)\n",
    "print(\"Wrong  : \", wrong)\n",
    "\n",
    "cpct = (100*correct)/total\n",
    "wpct = (100*wrong)/total\n",
    "print(\"Correct %: \", cpct)\n",
    "print(\"Wrong   %: \", wpct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POStagger results on development set\n",
    "\n",
    "| tagset | LTSM units | batchnorm | dropout | epochs | results |\n",
    "|:-----|---------:|:----------:|-------:|-----:|----------:|\n",
    "| tt | 128 | no | 0 |  50 | 98.50 |\n",
    "| tt | 128 | yes | 0.5 | 30 | 98.76 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('tt_pos.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Supertagger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training and development data\n",
    "\n",
    "We split the data as before, only using Z (supertags) instead of Y2 (treetagger POStag set) as the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (9449,)\n",
      "Test:   (3150,)\n",
      "Dev:    (3150,)\n"
     ]
    }
   ],
   "source": [
    "# split the training data into the standard 60% train, 20% dev, 20% test \n",
    "X_train, X_testdev, Y_super_train, Y_super_testdev = train_test_split(X, Z, test_size=0.4)\n",
    "X_test, X_dev, Y_super_test, Y_super_dev = train_test_split(X_testdev, Y_super_testdev, test_size=0.5)\n",
    "print(\"Train: \", X_train.shape)\n",
    "print(\"Test:  \", X_test.shape)\n",
    "print(\"Dev:   \", X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training data\n",
    "\n",
    "Transform the training data into the form most convenient for the supertag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = lists_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_super_train_indices = lists_to_indices(Y_super_train, super_to_index, maxLen)\n",
    "Y_super_train_oh = to_categorical(Y_super_train_indices, num_classes=numSuperClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare development data\n",
    "\n",
    "Do the same for the development data. The development data allows us to check for over/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev_indices = lists_to_indices(X_dev, word_to_index, max_len = maxLen)\n",
    "Y_super_dev_indices = lists_to_indices(Y_super_dev, super_to_index, max_len = maxLen)\n",
    "Y_super_dev_oh = to_categorical(Y_super_dev_indices, num_classes = numSuperClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We define the structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Super_model\n",
    "# this is a direct supertag model not using the part-of-speech tags\n",
    "\n",
    "def Super_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the direct supertagger model's graph\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(256, return_sequences=True)(embeddings) \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "#    merged = concatenate([embeddings,X])\n",
    "#    X = LSTM(128, return_sequences=True)(merged) \n",
    "#    X = BatchNormalization()(X)\n",
    "#    X = Dropout(0.5)(X)\n",
    "\n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    X = TimeDistributed(Dense(numSuperClasses, activation='softmax'))(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 266)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 266, 429)          12999558  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 266, 256)          702464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 266, 256)          1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 266, 256)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 266, 891)          228987    \n",
      "=================================================================\n",
      "Total params: 13,932,033\n",
      "Trainable params: 931,963\n",
      "Non-trainable params: 13,000,070\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "supermodel = Super_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "supermodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supermodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9449 samples, validate on 3150 samples\n",
      "Epoch 1/50\n",
      "9449/9449 [==============================] - 731s 77ms/step - loss: 1.9373 - acc: 0.6222 - val_loss: 0.9228 - val_acc: 0.7604\n",
      "Epoch 2/50\n",
      "9449/9449 [==============================] - 735s 78ms/step - loss: 0.9313 - acc: 0.7572 - val_loss: 0.9078 - val_acc: 0.7629\n",
      "Epoch 3/50\n",
      "9449/9449 [==============================] - 731s 77ms/step - loss: 0.8173 - acc: 0.7743 - val_loss: 0.7454 - val_acc: 0.7930\n",
      "Epoch 4/50\n",
      "9449/9449 [==============================] - 740s 78ms/step - loss: 0.7668 - acc: 0.7827 - val_loss: 0.7343 - val_acc: 0.7897\n",
      "Epoch 5/50\n",
      "9449/9449 [==============================] - 772s 82ms/step - loss: 0.7339 - acc: 0.7894 - val_loss: 0.6889 - val_acc: 0.8015\n",
      "Epoch 6/50\n",
      "9449/9449 [==============================] - 682s 72ms/step - loss: 0.7095 - acc: 0.7945 - val_loss: 0.6755 - val_acc: 0.8041\n",
      "Epoch 7/50\n",
      "9449/9449 [==============================] - 664s 70ms/step - loss: 0.6899 - acc: 0.7978 - val_loss: 0.6645 - val_acc: 0.8062\n",
      "Epoch 8/50\n",
      "9449/9449 [==============================] - 666s 70ms/step - loss: 0.6726 - acc: 0.8013 - val_loss: 0.6613 - val_acc: 0.8070\n",
      "Epoch 9/50\n",
      "9449/9449 [==============================] - 664s 70ms/step - loss: 0.6578 - acc: 0.8044 - val_loss: 0.6502 - val_acc: 0.8098\n",
      "Epoch 10/50\n",
      "9449/9449 [==============================] - 671s 71ms/step - loss: 0.6435 - acc: 0.8066 - val_loss: 0.6440 - val_acc: 0.8112\n",
      "Epoch 11/50\n",
      "9449/9449 [==============================] - 677s 72ms/step - loss: 0.6299 - acc: 0.8105 - val_loss: 0.6375 - val_acc: 0.8130\n",
      "Epoch 12/50\n",
      "9449/9449 [==============================] - 699s 74ms/step - loss: 0.6195 - acc: 0.8133 - val_loss: 0.6362 - val_acc: 0.8158\n",
      "Epoch 13/50\n",
      "9449/9449 [==============================] - 667s 71ms/step - loss: 0.6088 - acc: 0.8155 - val_loss: 0.6306 - val_acc: 0.8142\n",
      "Epoch 14/50\n",
      "9449/9449 [==============================] - 664s 70ms/step - loss: 0.5982 - acc: 0.8181 - val_loss: 0.6281 - val_acc: 0.8161\n",
      "Epoch 15/50\n",
      "9449/9449 [==============================] - 665s 70ms/step - loss: 0.5881 - acc: 0.8193 - val_loss: 0.6267 - val_acc: 0.8182\n",
      "Epoch 16/50\n",
      "9449/9449 [==============================] - 682s 72ms/step - loss: 0.5777 - acc: 0.8231 - val_loss: 0.6215 - val_acc: 0.8195\n",
      "Epoch 17/50\n",
      "9449/9449 [==============================] - 671s 71ms/step - loss: 0.5682 - acc: 0.8249 - val_loss: 0.6240 - val_acc: 0.8183\n",
      "Epoch 18/50\n",
      "9449/9449 [==============================] - 689s 73ms/step - loss: 0.5585 - acc: 0.8281 - val_loss: 0.6176 - val_acc: 0.8195\n",
      "Epoch 19/50\n",
      "9449/9449 [==============================] - 674s 71ms/step - loss: 0.5488 - acc: 0.8298 - val_loss: 0.6256 - val_acc: 0.8190\n",
      "Epoch 20/50\n",
      "9449/9449 [==============================] - 678s 72ms/step - loss: 0.5434 - acc: 0.8309 - val_loss: 0.6259 - val_acc: 0.8196\n",
      "Epoch 21/50\n",
      "9449/9449 [==============================] - 677s 72ms/step - loss: 0.5307 - acc: 0.8343 - val_loss: 0.6230 - val_acc: 0.8209\n",
      "Epoch 22/50\n",
      "9449/9449 [==============================] - 693s 73ms/step - loss: 0.5233 - acc: 0.8367 - val_loss: 0.6235 - val_acc: 0.8189\n",
      "Epoch 23/50\n",
      "9449/9449 [==============================] - 698s 74ms/step - loss: 0.5152 - acc: 0.8381 - val_loss: 0.6204 - val_acc: 0.8204\n",
      "Epoch 24/50\n",
      "9449/9449 [==============================] - 723s 76ms/step - loss: 0.5074 - acc: 0.8398 - val_loss: 0.6253 - val_acc: 0.8206\n",
      "Epoch 25/50\n",
      "9449/9449 [==============================] - 708s 75ms/step - loss: 0.4999 - acc: 0.8419 - val_loss: 0.6298 - val_acc: 0.8204\n",
      "Epoch 26/50\n",
      "9449/9449 [==============================] - 718s 76ms/step - loss: 0.4922 - acc: 0.8444 - val_loss: 0.6263 - val_acc: 0.8206\n",
      "Epoch 27/50\n",
      "9449/9449 [==============================] - 704s 74ms/step - loss: 0.4820 - acc: 0.8474 - val_loss: 0.6405 - val_acc: 0.8181\n",
      "Epoch 28/50\n",
      "9449/9449 [==============================] - 717s 76ms/step - loss: 0.4740 - acc: 0.8497 - val_loss: 0.6360 - val_acc: 0.8188\n",
      "Epoch 29/50\n",
      "9449/9449 [==============================] - 700s 74ms/step - loss: 0.4663 - acc: 0.8516 - val_loss: 0.6439 - val_acc: 0.8180\n",
      "Epoch 30/50\n",
      "9449/9449 [==============================] - 708s 75ms/step - loss: 0.4585 - acc: 0.8543 - val_loss: 0.6426 - val_acc: 0.8204\n",
      "Epoch 31/50\n",
      "9449/9449 [==============================] - 707s 75ms/step - loss: 0.4495 - acc: 0.8566 - val_loss: 0.6464 - val_acc: 0.8184\n",
      "Epoch 32/50\n",
      "9449/9449 [==============================] - 712s 75ms/step - loss: 0.4431 - acc: 0.8581 - val_loss: 0.6473 - val_acc: 0.8195\n",
      "Epoch 33/50\n",
      "9449/9449 [==============================] - 726s 77ms/step - loss: 0.4335 - acc: 0.8605 - val_loss: 0.6578 - val_acc: 0.8179\n",
      "Epoch 34/50\n",
      "9449/9449 [==============================] - 771s 82ms/step - loss: 0.4263 - acc: 0.8625 - val_loss: 0.6579 - val_acc: 0.8187\n",
      "Epoch 35/50\n",
      "1664/9449 [====>.........................] - ETA: 9:47 - loss: 0.4038 - acc: 0.8692"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b0f77ca9dc31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_super_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_super_dev_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = supermodel.fit(X_train_indices, Y_super_train_oh, epochs = 50, batch_size = 64, shuffle=True, validation_data=(X_dev_indices,Y_super_dev_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = supermodel.evaluate(X_dev_indices, Y_super_dev_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supermodel.save('supertagger.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "\n",
    "y_dev_oh = to_categorical(Y_super_dev_indices, num_classes = numSuperClasses)\n",
    "X_dev_indices = lists_to_indices(X_dev, word_to_index, maxLen)\n",
    "pred = supermodel.predict(X_dev_indices)\n",
    "\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "f = open('superlog_raw.txt', 'w')\n",
    "for i in range(len(X_dev)-1):\n",
    "    for j in range(len(X_dev[i])):\n",
    "        num = np.argmax(pred[i][j])\n",
    "        if(num != Y_super_dev_indices[i][j]):\n",
    "            wrong = wrong + 1\n",
    "            f.write(X_dev[i][j]+\"|\"+Y_super_dev[i][j]+\"|\"+index_to_super[num]+\"\\n\")\n",
    "            print('Expected supertag: '+ X_dev[i][j] + '|' + Y_super_dev[i][j] + ' prediction: '+ X_dev[i][j] + '|' + index_to_super[num])\n",
    "        else:\n",
    "            correct = correct + 1\n",
    "f.close()\n",
    "total = wrong + correct\n",
    "\n",
    "print(\"Total  : \", total)\n",
    "print(\"Correct: \", correct)\n",
    "print(\"Wrong  : \", wrong)\n",
    "\n",
    "cpct = (100*correct)/total\n",
    "wpct = (100*wrong)/total\n",
    "print(\"Correct %: \", cpct)\n",
    "print(\"Wrong   %: \", wpct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_dev_oh = to_categorical(Y_super_dev_indices, num_classes = numSuperClasses)\n",
    "X_dev_indices = lists_to_indices(X_dev, word_to_index, maxLen)\n",
    "pred = supermodel.predict(X_dev_indices)\n",
    "\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "fo = open('super_out.txt', 'w')\n",
    "fc = open('super_correct.txt', 'w')\n",
    "for i in range(len(X_dev)-1):\n",
    "    for j in range(len(X_dev[i])):\n",
    "        num = np.argmax(pred[i][j])\n",
    "        fc.write(X_dev[i][j]+\"|\"+Y_super_dev[i][j]+\"\\n\")\n",
    "        fo.write(X_dev[i][j]+\"|\"+index_to_super[num]+\"\\n\")\n",
    "fc.close()\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supertagger results on development set\n",
    "\n",
    "\n",
    "Vanilla LSTM model (new feature set 20180309)\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 256 | yes | .5 |  10 | 80.66 | 81.12 |\n",
    "| cwindow | 50 | 256 | yes | .5 |  30 | 85.43 | 82.04 |\n",
    "| cwindow | 50 | 256 | yes | .5 |  50 |  |  |\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 128 | yes | .5 |  10 | 79.57 | 80.53 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  30 | 82.44 | 81.93 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  50 | 84.16 | 82.01 |\n",
    "\n",
    "\n",
    "Vanilla LSTM model (new feature set 20180309)\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 128 | yes | .5 |  10 | 79.42 | 80.70 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  30 | 82.40 | 81.72 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  50 | 84.15 | 82.12 |\n",
    "\n",
    "\n",
    "Vanilla LSTM model (new feature set 20180308)\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 128 | yes | .5 |  10 | 79.18 | 80.01 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  30 | 82.13 | 81.35 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  50 | 83.78 | 81.72 |\n",
    "\n",
    "Vanilla LSTM model (old feature set)\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | results |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|----------:|\n",
    "| fastText | 200 | 128 | yes | .2 |  50 | 80.21 |\n",
    "| cwindow | 50 | 128 | yes | .2 | 50 | 75.82 |\n",
    "| cwindow | 50 | 128 | yes | .4 | 50 | 79.25  | \n",
    "| cwindow | 50 | 128 | yes | .5 | 50 | 79.59  |\n",
    "| cwindow | 50 | 128 | no | .5 |  10 | 73.60 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  10 | 78.27 |\n",
    "| cwindow | 50 | 128 | no | .5 |  30 | 77.89 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  30 | 79.64 |\n",
    "\n",
    "Second LSTM layer\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | results |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|----------:|\n",
    "| cwindow | 50 | 2 * 128 | yes | .5 | 10 | 78.32  |\n",
    "| cwindow | 50 | 2 * 128 | yes | .5 | 30 | 80.10  |\n",
    "| cwindow | 50 | 2 * 128 | yes | .5 | 50 |  80.55 |\n",
    "\n",
    "Second LSTM layer plus forward mapping of the word embeddings\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | results |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|----------:|\n",
    "| cwindow | 50 | 2 * 128 | yes | .5 | 10 |  78.50 |\n",
    "| cwindow | 50 | 2 * 128 | yes | .5 | 30 |  80.45 |\n",
    "| cwindow | 50 | 2 * 128 | yes | .5 | 50 |  80.34 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_sequence(\"yves acceptera le lait\", model, word_to_index, index_to_pos2, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_sequence(\"yves acceptera le lait\", superposmodel, word_to_index, index_to_super, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_tagged(X_dev, model, word_to_index, index_to_pos2, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_tagged(X_dev[1:5], supermodel, word_to_index, index_to_super, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_tagged_beta(X_dev[1:4], superposmodel, 0.1, word_to_index, index_to_super, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_beta(X_dev, Y_super_dev, supermodel, word_to_index, super_to_index, index_to_super, 0.01, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined part-of-speech and supertagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the training data into the standard 60% train, 20% dev, 20% test \n",
    "X_train, X_testdev, Y_super_train, Y_super_testdev = train_test_split(X, Z, test_size=0.4)\n",
    "X_test, X_dev, Y_super_test, Y_super_dev = train_test_split(X_testdev, Y_super_testdev, test_size=0.5)\n",
    "print(\"Train: \", X_train.shape)\n",
    "print(\"Test:  \", X_test.shape)\n",
    "print(\"Dev:   \", X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = lists_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_super_train_indices = lists_to_indices(Y_super_train, super_to_index, maxLen)\n",
    "Y_super_train_oh = to_categorical(Y_super_train_indices, num_classes=numSuperClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev_indices = lists_to_indices(X_dev, word_to_index, max_len = maxLen)\n",
    "Y_super_dev_indices = lists_to_indices(Y_super_dev, super_to_index, max_len = maxLen)\n",
    "Y_super_dev_oh = to_categorical(Y_super_dev_indices, num_classes = numSuperClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Super_model\n",
    "# this is a direct supertag model not using the part-of-speech tags\n",
    "\n",
    "def Super_pos_model(input_shape, pos_model, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the combined supertag/part-of-speech model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    pos_model -- the part-of-speech model to incorporate\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with fastText vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # get probability distribution over parts_of_speech from pos_model\n",
    "    parts_of_speech = pos_model(sentence_indices)\n",
    "    \n",
    "    # concatenate with the embeddings\n",
    "    merged = concatenate([parts_of_speech,embeddings])\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(merged) \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    X = TimeDistributed(Dense(numSuperClasses, activation='softmax'))(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "superposmodel = Super_pos_model((maxLen,), model, word_to_vec_map, word_to_index)\n",
    "superposmodel.summary()\n",
    "print(superposmodel.layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "superposmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = superposmodel.fit(X_train_indices, Y_super_train_oh, epochs = 20, batch_size = 32, shuffle=True, validation_data=(X_dev_indices,Y_super_dev_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "superposmodel.save('superpostagger.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-level supertag model\n",
    "\n",
    "Use the probabilities over the supertags produced by the first model as input to a bi-directional LSTM. We should probably add the probabilities over the part-of-speech tags and/or the word vector information as well. This means adding the outputs of, respectively, layers `model_1` (32), `embedding_3` (663) and `concatenate_1` (695)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load previously defined model and associated dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_obj('word_to_vec_map')\n",
    "load_obj('word_to_index')\n",
    "superposmodel = load_model('superpostagger.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Super_two_model(input_shape, super_pos_model, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the graph of a supertag model over the output probabilities of another supertag model.\n",
    "    This is the simplest way to do this, using only the output but none of the internal activations\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    pos_model -- the part-of-speech model to incorporate\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "     \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "\n",
    "    # get probability distribution over supertags from super_pos_model\n",
    "    supertags = super_pos_model(sentence_indices)\n",
    "#    posout = supertags.layers['model_1'].output\n",
    "#    embout = supertags.layers['embedding_3'].output\n",
    "\n",
    " \n",
    "    # concatenate with the embeddings\n",
    "#    merged = concatenate([posout,supertags])\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(supertags) \n",
    "#    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    X = TimeDistributed(Dense(numSuperClasses, activation='softmax'))(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supertwomodel = Super_two_model((maxLen,), superposmodel, word_to_vec_map, word_to_index)\n",
    "supertwomodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supertwomodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = supertwomodel.fit(X_train_indices, Y_super_train_oh, epochs = 30, batch_size = 32, shuffle=True, validation_data=(X_dev_indices,Y_super_dev_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supertwomodel.save('super2tagger.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del supertwomodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supertagger results on development set\n",
    "\n",
    "| LTSM units | batchnorm | dropout | epochs | results |\n",
    "|---------:|:----------:|-------:|----:|----------:|\n",
    "| 128 | yes | .5 |  20 | 80.98 |\n",
    "| 128 | no | .5 |  30 |  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine POS-tagger with supertagger output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Superpos_two_model(input_shape, super_pos_model, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the graph of a supertag model over the output probabilities of another supertag model.\n",
    "    This version combines the supertagger output with the output of the POS-tagger\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    pos_model -- the part-of-speech model to incorporate\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "\n",
    "    # get probability distribution over supertags from super_pos_model\n",
    "    supertags = super_pos_model(sentence_indices)\n",
    "    \n",
    "    get_1st_layer_output = K.function([supertags[0].input],\n",
    "                                  [supertags[1].output])\n",
    "    posout = get_1st_layer_output([sentence_indices])[0]\n",
    "#    embout = supertags.layers['embedding_3'].output\n",
    "\n",
    " \n",
    "    # concatenate with the embeddings\n",
    "    merged = concatenate([posout,supertags])\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(merged) \n",
    "#    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    X = TimeDistributed(Dense(numSuperClasses, activation='softmax'))(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "superposmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "superpos2model = Superpos_two_model((maxLen,), superposmodel, word_to_vec_map, word_to_index)\n",
    "superpos2model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine word embedding, POS-tagger and supertagger output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Superstack_model(input_shape, super_pos_model, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the graph of a supertag model over the output probabilities of another supertag model.\n",
    "    This version combines the supertagger output with the output of the POS-tagger\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    pos_model -- the part-of-speech model to incorporate\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "\n",
    "    # get probability distribution over supertags from super_pos_model\n",
    "    supertags = super_pos_model(sentence_indices)\n",
    "#    posout = supertags.layers['model_1'].output\n",
    "#    embout = supertags.layers['embedding_3'].output\n",
    "    pos_emb = supertags.layers['concatenate_1'].output\n",
    " \n",
    "    # concatenate with the embeddings\n",
    "    merged = concatenate([pos_emb,supertags])\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(merged) \n",
    "#    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    X = TimeDistributed(Dense(numSuperClasses, activation='softmax'))(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def show_mem_usage():\n",
    "    '''Displays memory usage from inspection\n",
    "    of global variables in this notebook'''\n",
    "    gl = sys._getframe(1).f_globals\n",
    "    vars= {}\n",
    "    for k,v in list(gl.items()):\n",
    "        # for pandas dataframes\n",
    "        if hasattr(v, 'memory_usage'):\n",
    "            mem = v.memory_usage(deep=True)\n",
    "            if not np.isscalar(mem):\n",
    "                mem = mem.sum()\n",
    "            vars.setdefault(id(v),[mem]).append(k)\n",
    "        # work around for a bug\n",
    "        elif isinstance(v,pd.Panel):\n",
    "            v = v.values\n",
    "        vars.setdefault(id(v),[sys.getsizeof(v)]).append(k)\n",
    "    total = 0\n",
    "    for k,(value,*names) in vars.items():\n",
    "        if value>1e6:\n",
    "            print(names,\"%.3fMB\"%(value/1e6))\n",
    "        total += value\n",
    "    print(\"%.3fMB\"%(total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cutoff = 2\n",
    "\n",
    "def trim_dict(d, min_count=Cutoff):\n",
    "    for k,v in list(d.items()):\n",
    "        if v < min_count:\n",
    "            del d[k]\n",
    "    d['*UNK*'] = 1\n",
    "    d['*OOR*'] = 1\n",
    "    return d\n",
    "\n",
    "suffixcount1={}\n",
    "suffixcount2={}\n",
    "suffixcount3={}\n",
    "suffixcount4={}\n",
    "suffixcount5={}\n",
    "suffixcount6={}\n",
    "suffixcount7={}\n",
    "prefixcount1={}\n",
    "prefixcount2={}\n",
    "prefixcount3={}\n",
    "prefixcount4={}\n",
    "\n",
    "for word in vocabulary:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[0-8]', '9', word)\n",
    "    suf1 = word[-1:]\n",
    "    suf2 = word[-2:]\n",
    "    suf3 = word[-3:]\n",
    "    suf4 = word[-4:]\n",
    "    suf5 = word[-5:]\n",
    "    suf6 = word[-6:]\n",
    "    suf7 = word[-7:]\n",
    "    pref1 = word [:1]\n",
    "    pref2 = word [:2]\n",
    "    pref3 = word [:3]\n",
    "    pref4 = word [:4]\n",
    "    \n",
    "    if len(suf1) > 0:\n",
    "        if suf1 not in suffixcount1:\n",
    "            suffixcount1[suf1] = 1\n",
    "        else:\n",
    "            suffixcount1[suf1] += 1\n",
    "\n",
    "    if len(suf2) > 1: \n",
    "        if suf2 not in suffixcount2:\n",
    "            suffixcount2[suf2] = 1\n",
    "        else:\n",
    "            suffixcount2[suf2] += 1\n",
    "\n",
    "    if len(suf3) > 2: \n",
    "        if suf3 not in suffixcount3:\n",
    "            suffixcount3[suf3] = 1\n",
    "        else:\n",
    "            suffixcount3[suf3] += 1\n",
    "\n",
    "    if len(suf4) > 3: \n",
    "        if suf4 not in suffixcount4:\n",
    "            suffixcount4[suf4] = 1\n",
    "        else:\n",
    "            suffixcount4[suf4] += 1\n",
    "\n",
    "    if len(suf5) > 4: \n",
    "        if suf5 not in suffixcount5:\n",
    "            suffixcount5[suf5] = 1\n",
    "        else:\n",
    "            suffixcount5[suf5] += 1\n",
    "    if len(suf6) > 5: \n",
    "        if suf6 not in suffixcount6:\n",
    "            suffixcount6[suf6] = 1\n",
    "        else:\n",
    "            suffixcount6[suf6] += 1\n",
    "    if len(suf7) > 6: \n",
    "        if suf7 not in suffixcount7:\n",
    "            suffixcount7[suf7] = 1\n",
    "        else:\n",
    "            suffixcount7[suf7] += 1\n",
    "    if len(pref1) > 0:\n",
    "        if pref1 not in prefixcount1:\n",
    "            prefixcount1[pref1] = 1\n",
    "        else:\n",
    "            prefixcount1[pref1] += 1\n",
    "\n",
    "    if len(pref2) > 1:\n",
    "        if pref2 not in prefixcount2:\n",
    "            prefixcount2[pref2] = 1\n",
    "        else:\n",
    "            prefixcount2[pref2] += 1\n",
    "\n",
    "    if len(pref3) > 2:\n",
    "        if pref3 not in prefixcount3:\n",
    "            prefixcount3[pref3] = 1\n",
    "        else:\n",
    "            prefixcount3[pref3] += 1\n",
    "    if len(pref4) > 3:\n",
    "        if pref4 not in prefixcount4:\n",
    "            prefixcount4[pref4] = 1\n",
    "        else:\n",
    "            prefixcount4[pref4] += 1\n",
    "\n",
    "\n",
    "suffixcount1 = trim_dict(suffixcount1)\n",
    "suffixcount2 = trim_dict(suffixcount2)\n",
    "suffixcount3 = trim_dict(suffixcount3)\n",
    "suffixcount4 = trim_dict(suffixcount4)\n",
    "suffixcount5 = trim_dict(suffixcount5)\n",
    "suffixcount6 = trim_dict(suffixcount6)\n",
    "suffixcount7 = trim_dict(suffixcount7)\n",
    "\n",
    "prefixcount1 = trim_dict(prefixcount1)\n",
    "prefixcount2 = trim_dict(prefixcount2)\n",
    "prefixcount3 = trim_dict(prefixcount3)\n",
    "prefixcount4 = trim_dict(prefixcount4)\n",
    "\n",
    "suffix1 = set(suffixcount1.keys())\n",
    "suffix2 = set(suffixcount2.keys())\n",
    "suffix3 = set(suffixcount3.keys())\n",
    "suffix4 = set(suffixcount4.keys())\n",
    "suffix5 = set(suffixcount5.keys())\n",
    "suffix6 = set(suffixcount6.keys())\n",
    "suffix7 = set(suffixcount7.keys())\n",
    "\n",
    "prefix1 = set(prefixcount1.keys())\n",
    "prefix2 = set(prefixcount2.keys())\n",
    "prefix3 = set(prefixcount3.keys())\n",
    "prefix4 = set(prefixcount4.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1_to_integer, integer_to_p1 = indexify(prefix1)\n",
    "p2_to_integer, integer_to_p2 = indexify(prefix2)\n",
    "p3_to_integer, integer_to_p3 = indexify(prefix3)\n",
    "p4_to_integer, integer_to_p4 = indexify(prefix4)\n",
    "\n",
    "s1_to_integer, integer_to_s1 = indexify(suffix1)\n",
    "s2_to_integer, integer_to_s2 = indexify(suffix2)\n",
    "s3_to_integer, integer_to_s3 = indexify(suffix3)\n",
    "s4_to_integer, integer_to_s4 = indexify(suffix4)\n",
    "s5_to_integer, integer_to_s5 = indexify(suffix5)\n",
    "s6_to_integer, integer_to_s6 = indexify(suffix6)\n",
    "s7_to_integer, integer_to_s7 = indexify(suffix7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_prefvec(word, alen, afset, af_to_int):\n",
    "    if len(word) >= alen:\n",
    "        pref = word[:alen]\n",
    "        if pref in afset:\n",
    "            int = af_to_int[pref]\n",
    "        else:\n",
    "            int = af_to_int['*UNK*']\n",
    "    else:\n",
    "        int = af_to_int['*OOR*']\n",
    "    return to_categorical(int, len(afset)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_sufvec(word, alen, afset, af_to_int):\n",
    "    if len(word) >= alen:\n",
    "        pref = word[-alen:]\n",
    "        if pref in afset:\n",
    "            int = af_to_int[pref]\n",
    "        else:\n",
    "            int = af_to_int['*UNK*']\n",
    "    else:\n",
    "        int = af_to_int['*OOR*']\n",
    "    return to_categorical(int, len(afset)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_prefix_vector(word):\n",
    "    p1 = word_to_prefvec(word, 1, prefix1, p1_to_integer)\n",
    "    p2 = word_to_prefvec(word, 2, prefix2, p2_to_integer)\n",
    "    p3 = word_to_prefvec(word, 3, prefix3, p3_to_integer)\n",
    "    p4 = word_to_prefvec(word, 4, prefix4, p4_to_integer)\n",
    "    return np.concatenate((p1,p2,p3,p4))\n",
    "\n",
    "def word_to_suffix_vector(word):\n",
    "    s1 = word_to_sufvec(word, 1, suffix1, s1_to_integer)\n",
    "    s2 = word_to_sufvec(word, 2, suffix2, s2_to_integer)\n",
    "    s3 = word_to_sufvec(word, 3, suffix3, s3_to_integer)\n",
    "    s4 = word_to_sufvec(word, 4, suffix4, s4_to_integer)\n",
    "    s5 = word_to_sufvec(word, 5, suffix5, s5_to_integer)\n",
    "    s6 = word_to_sufvec(word, 6, suffix6, s6_to_integer)\n",
    "    s7 = word_to_sufvec(word, 7, suffix7, s7_to_integer)\n",
    "    return np.concatenate((s1,s2,s3,s4,s5,s6,s7))\n",
    "\n",
    "def word_to_affix_vector(word):\n",
    "    p1 = word_to_prefvec(word, 1, prefix1, p1_to_integer)\n",
    "    p2 = word_to_prefvec(word, 2, prefix2, p2_to_integer)\n",
    "    p3 = word_to_prefvec(word, 3, prefix3, p3_to_integer)\n",
    "    p4 = word_to_prefvec(word, 4, prefix3, p4_to_integer)\n",
    "    s1 = word_to_sufvec(word, 1, suffix1, s1_to_integer)\n",
    "    s2 = word_to_sufvec(word, 2, suffix2, s2_to_integer)\n",
    "    s3 = word_to_sufvec(word, 3, suffix3, s3_to_integer)\n",
    "    s4 = word_to_sufvec(word, 4, suffix4, s4_to_integer)\n",
    "    s5 = word_to_sufvec(word, 5, suffix5, s5_to_integer)\n",
    "    s6 = word_to_sufvec(word, 6, suffix6, s6_to_integer)\n",
    "    s7 = word_to_sufvec(word, 7, suffix7, s7_to_integer)\n",
    "    return np.concatenate((p1,p2,p3,p4,s1,s2,s3,s4,s5,s6,s7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_affixes(vocab):\n",
    "    \n",
    "    word_to_suffix = {}\n",
    "    word_to_prefix = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        w = word.lower()\n",
    "        w = re.sub(r'[0-8]', '9', w)\n",
    "        pvec = word_to_prefix_vector(w)\n",
    "        svec = word_to_suffix_vector(w)\n",
    "        word_to_prefix[word] = pvec\n",
    "        word_to_suffix[word] = svec\n",
    "        \n",
    "    return word_to_prefix, word_to_suffix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_prefix, word_to_suffix = compute_affixes(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Super_model\n",
    "# this is a direct supertag model not using the part-of-speech tags\n",
    "\n",
    "def Super_affix_model(input_shape, word_to_vec_map, word_to_prefix, word_to_suffix, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the direct supertagger model's graph\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its fastText vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    prefix_emb = pretrained_embedding_layer(word_to_prefix, word_to_index)\n",
    "    suffix_emb = pretrained_embedding_layer(word_to_suffix, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    pref = prefix_emb(sentence_indices)\n",
    "    suff = suffix_emb(sentence_indices)\n",
    "    P = Dense(32)(pref)\n",
    "    S = Dense(32)(suff)\n",
    "    merged = concatenate([embeddings,P,S])\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # returning a batch of sequences.\n",
    "    X = LSTM(64, return_sequences=True)(merged) \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "#    merged = concatenate([embeddings,X])\n",
    "#    X = LSTM(128, return_sequences=True)(merged) \n",
    "#    X = BatchNormalization()(X)\n",
    "#    X = Dropout(0.5)(X)\n",
    "\n",
    "    # Add a (time distributed) Dense layer followed by a softmax activation\n",
    "    X = TimeDistributed(Dense(numSuperClasses, activation='softmax'))(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices,outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 266)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 266, 5788)    175387976   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 266, 14983)   454014866   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 266, 429)     12999558    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 266, 32)      185248      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 266, 32)      479488      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 266, 493)     0           embedding_16[0][0]               \n",
      "                                                                 dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 266, 64)      142848      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 266, 64)      256         lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 266, 64)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 266, 891)     57915       dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 643,268,155\n",
      "Trainable params: 865,627\n",
      "Non-trainable params: 642,402,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "supermodel = Super_affix_model((maxLen,), word_to_vec_map, word_to_prefix, word_to_suffix, word_to_index)\n",
    "supermodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supermodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9449 samples, validate on 3150 samples\n",
      "Epoch 1/50\n",
      "9449/9449 [==============================] - 720s 76ms/step - loss: 2.0544 - acc: 0.6119 - val_loss: 0.9320 - val_acc: 0.7649\n",
      "Epoch 2/50\n",
      " 864/9449 [=>............................] - ETA: 9:57 - loss: 1.0959 - acc: 0.7481 "
     ]
    }
   ],
   "source": [
    "history = supermodel.fit(X_train_indices, Y_super_train_oh, epochs = 50, batch_size = 32, shuffle=True, validation_data=(X_dev_indices,Y_super_dev_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Todo\n",
    "\n",
    "Try adding dropout layers for the two dense affix layers\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Retraining 64-unit model with dropout of .5\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 64 | yes | .5 |  10 |  |  |\n",
    "\n",
    "\n",
    "\n",
    "Since the 128-unit LSTM model showed significant overfitting, training with smaller 64-unit model but lower dropout (.2). Still overgenerates.\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 64 | yes | .2 |  10 | 85.66 | 80.85 |\n",
    "\n",
    "First attempt with affex features. Clear overfitting after 10 epochs.\n",
    "\n",
    "| embedding | dimension| LTSM units | batchnorm | dropout | epochs | train | devel |\n",
    "|:-----|-----:|---------:|:----------:|-------:|----:|-----:|----------:|\n",
    "| cwindow | 50 | 128 | yes | .5 |  10 | 84.84 | 81.17 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  30 | 91.56 | 79.65 |\n",
    "| cwindow | 50 | 128 | yes | .5 |  50 | 94.41 | 78.97 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = supermodel.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gradients = supermodel.optimizer.get_gradients(supermodel.total_loss, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_tensors = [\n",
    "        # input data\n",
    "        supermodel.inputs[0],\n",
    "        # how much to weight each sample by\n",
    "        supermodel.sample_weights[0],\n",
    "        # labels\n",
    "        supermodel.targets[0],\n",
    "        # train or test mode\n",
    "        K.learning_phase()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_9/kernel:0' shape=(5788, 32) dtype=float32_ref>\n",
      "0.776805\n",
      "-0.760271\n",
      "<tf.Variable 'dense_9/bias:0' shape=(32,) dtype=float32_ref>\n",
      "0.0926415\n",
      "-0.0809307\n",
      "<tf.Variable 'dense_10/kernel:0' shape=(14983, 32) dtype=float32_ref>\n",
      "0.732644\n",
      "-0.778347\n",
      "<tf.Variable 'dense_10/bias:0' shape=(32,) dtype=float32_ref>\n",
      "0.085453\n",
      "-0.0683008\n"
     ]
    }
   ],
   "source": [
    "print(weights[0])\n",
    "print(np.max(K.get_value(weights[0])))\n",
    "print(np.min(K.get_value(weights[0])))\n",
    "\n",
    "print(weights[1])\n",
    "print(np.max(K.get_value(weights[1])))\n",
    "print(np.min(K.get_value(weights[1])))\n",
    "\n",
    "print(weights[2])\n",
    "print(np.max(K.get_value(weights[2])))\n",
    "print(np.min(K.get_value(weights[2])))\n",
    "\n",
    "print(weights[3])\n",
    "print(np.max(K.get_value(weights[3])))\n",
    "print(np.min(K.get_value(weights[3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a18fc4d94515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# set sample weights to one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "    steps = 0\n",
    "    total_norm = 0\n",
    "    s_w = None\n",
    "    while steps < 32:\n",
    "        X, y = next(data)\n",
    "        # set sample weights to one\n",
    "        # for every input\n",
    "        if s_w is None:\n",
    "            s_w = np.ones(X.shape[0])\n",
    "\n",
    "        gradients = grad_fct([X, s_w, y, 0])\n",
    "        total_norm += np.sqrt(np.sum([np.sum(np.square(g)) for g in gradients]))\n",
    "        steps += 1\n",
    "\n",
    "    return total_norm / float(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
